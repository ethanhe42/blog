<!DOCTYPE html><html lang="en"><head><link rel="shortcut icon" href="/favicon.png"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/manifest.json"/><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="theme-color" content="#EB625A"/><meta property="og:type" content="website"/><script async="" src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4253216309174736" crossorigin="anonymous"></script><meta property="og:title" content="MLOps Infra &amp; tooling"/><meta property="og:site_name" content="Yihui He"/><meta name="twitter:title" content="MLOps Infra &amp; tooling"/><meta property="twitter:domain" content="yihui-he.github.io"/><meta name="twitter:creator" content="@he_yi_hui"/><meta name="description" content="Yihui He, AI research scientist / full stack engineer"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:card" content="summary"/><link rel="canonical" href="https://yihui-he.github.io/mlops-infra-tooling"/><meta property="og:url" content="https://yihui-he.github.io/mlops-infra-tooling"/><meta property="twitter:url" content="https://yihui-he.github.io/mlops-infra-tooling"/><title>MLOps Infra &amp; tooling</title><meta name="next-head-count" content="20"/><link rel="preload" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" data-n-g=""/><link rel="preload" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/blog/_next/static/chunks/webpack-ed2d627650adadaab635.js" defer=""></script><script src="/blog/_next/static/chunks/framework-c93ed74a065331c4bd75.js" defer=""></script><script src="/blog/_next/static/chunks/main-0989120ac94443065aa9.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-93d47364a9b4c8c53030.js" defer=""></script><script src="/blog/_next/static/chunks/1bfc9850-762e4e08544c8bec659c.js" defer=""></script><script src="/blog/_next/static/chunks/ae51ba48-7f31b5cf321fe3268476.js" defer=""></script><script src="/blog/_next/static/chunks/d7eeaac4-07e2c37279f27c6f41bc.js" defer=""></script><script src="/blog/_next/static/chunks/808-8e92cc1e8e3a13c0ecab.js" defer=""></script><script src="/blog/_next/static/chunks/270-25d82ee694bd9ee7ef5a.js" defer=""></script><script src="/blog/_next/static/chunks/pages/%5BpageId%5D-845d25c09c3d16569292.js" defer=""></script><script src="/blog/_next/static/fIyH02Gfo9GcHzi1tLPop/_buildManifest.js" defer=""></script><script src="/blog/_next/static/fIyH02Gfo9GcHzi1tLPop/_ssgManifest.js" defer=""></script></head><body><script src="noflash.js"></script><div id="__next"><div class="notion notion-app light-mode notion-block-e9d171d817bc4aa688f779391cf072d1"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/blog"><img class="icon notion-page-icon" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F63e89fe9-f5cd-4414-be0e-53d91aa54fc3%2Fme_small.jpg?table=block&amp;id=bd32b787-e471-49f3-8941-174a9c6846b6&amp;cache=v2" alt="Yihui He’s Blog" loading="lazy"/><span class="title">Yihui He’s Blog</span></a><span class="spacer">/</span><a class="breadcrumb" href="/blog/deep-learning-engineer-manual"><span class="title">deep learning engineer manual</span></a><span class="spacer">/</span><div class="breadcrumb active"><span class="title">MLOps Infra &amp; tooling</span></div></div><div class="rhs"></div></div></header><div class="notion-page-scroller"><main class="notion-page notion-page-no-cover notion-page-no-icon notion-page-has-text-icon notion-full-page"><h1 class="notion-title">MLOps Infra &amp; tooling</h1><div class="notion-page-content notion-page-content-has-aside notion-page-content-has-toc"><article class="notion-page-content-inner"><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-8e76d7445b544e8a97c2311ab506fa65" data-id="8e76d7445b544e8a97c2311ab506fa65"><span><div id="8e76d7445b544e8a97c2311ab506fa65" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8e76d7445b544e8a97c2311ab506fa65" title="1 - Dream vs. Reality for ML Practitioners"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">1 - Dream vs. Reality for ML Practitioners</span></span></h2><div class="notion-text notion-block-55addcdcf7ed49f0b4c7f35e981ba283">The <b>dream</b> of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system.</div><div class="notion-text notion-block-e9a06f7beea44247bf34ea5fc01952f8">The <b>reality</b> is that you will have to:</div><ul class="notion-list notion-list-disc notion-block-31903d73ff4f44a3a6b090a65e791004"><li>Aggregate, process, clean, label, and version the data</li></ul><ul class="notion-list notion-list-disc notion-block-c1dd873d24114ed396a37669d39d9b16"><li>Write and debug model code</li></ul><ul class="notion-list notion-list-disc notion-block-8ca61f12422949c99d2090b58e59b4ff"><li>Provision compute</li></ul><ul class="notion-list notion-list-disc notion-block-2d02886b7bb04b9daba4de9049e6b204"><li>Run many experiments and review the results</li></ul><ul class="notion-list notion-list-disc notion-block-e09e4b950909410baf3a0180d9bf5cbe"><li>Discover that you did something wrong or maybe try a different architecture -&gt; Write more code and provision more compute</li></ul><ul class="notion-list notion-list-disc notion-block-71be3474fc044f68b355b0845ca3958d"><li>Deploy the model when you are happy</li></ul><ul class="notion-list notion-list-disc notion-block-e0b64fba6863427bae0637fbb5be728b"><li>Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop</li></ul><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-5a87f2b783c246179003af64ffa7616f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling1.png?table=block&amp;id=5a87f2b7-83c2-4617-9003-af64ffa7616f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-184ecab1c4664eb5b344e69b7eb4da62">For example, the slide above is from <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.youtube.com/watch?v=oBklltKXtDE"><b>Andrej Karpathy’s talk</b></a> at PyTorch Devcon 2019 discussing Tesla’s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla’s ML engineers can all go on vacation :)</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-c8f16ecfdfc24c258461fb9376e7e909"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling2.png?table=block&amp;id=c8f16ecf-dfc2-4c25-8461-fb9376e7e909&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-b3699db952f74af8899253ef41de9c53">The picture above (from the famous Google paper “<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf"><b>Machine Learning: The High-Interest Credit Card of Technical Debt</b></a>”) shows that <b>the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support</b>. As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc.</div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-254d31ebf05b48e782e438a40348ac53" data-id="254d31ebf05b48e782e438a40348ac53"><span><div id="254d31ebf05b48e782e438a40348ac53" class="notion-header-anchor"></div><a class="notion-hash-link" href="#254d31ebf05b48e782e438a40348ac53" title="2 - Three Buckets of Tooling Landscape"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">2 - Three Buckets of Tooling Landscape</span></span></h2><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-24fb98a9a37842ba99309f898264c20f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling3.png?table=block&amp;id=24fb98a9-a378-42ba-9930-9f898264c20f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-4edad84cb021451a8b5d6c753c436100">We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment.</div><ul class="notion-list notion-list-disc notion-block-79921b568d414fcaab5b3f510ed8a6b4"><li>The <b>data</b> bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling.</li></ul><ul class="notion-list notion-list-disc notion-block-93e1e6f0b0b147f0a0a0538cb042f730"><li>The <b>training/evaluation</b> bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning.</li></ul><ul class="notion-list notion-list-disc notion-block-5d8ff8f7f80343a9aa0121ccc36bda54"><li>The <b>deployment</b> bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store.</li></ul><div class="notion-text notion-block-32303c040b734a86bcd0e6b2ab7d3eb6">There are also several vendors offering “all-in-one” MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket.</div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-3fedfd6eb96749a2b11eaa60271a459d" data-id="3fedfd6eb96749a2b11eaa60271a459d"><span><div id="3fedfd6eb96749a2b11eaa60271a459d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3fedfd6eb96749a2b11eaa60271a459d" title="3 - Software Engineering (vscode)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">3 - Software Engineering (vscode)</span></span></h2><div class="notion-text notion-block-c4498607471a41c6bdcbda10d3f82a84">When it comes to writing deep learning code, <b>Python is the clear programming language of choice</b>. As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper.</div><div class="notion-text notion-block-98a5f6ea19034b948bd3f6009c0ac99c"><b>When choosing your IDEs, there are many options out there</b> (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you’re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://code.visualstudio.com/"><b>Visual Studio Code</b></a> makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely.</div><div class="notion-text notion-block-d5421b4beb904f2788fbdf5e36c72d3a"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://jupyter.org/"><b>Jupyter Notebooks</b></a> have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://medium.com/netflix-techblog/notebook-innovation-591ee3221233"><b>Netflix based all of their machine learning workflows on them</b></a>, effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/fastai/nbdev"><b>nbdev</b></a> that shows people how to develop well-tested code in a notebook environment.</div><div class="notion-text notion-block-9ddf4f56d75b4a3898c824c4458bbf99">However, <b>there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products</b>. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://towardsdatascience.com/5-reasons-why-jupyter-notebooks-suck-4dc201e27086"><b>Alexander Mueller&#x27;s blog post</b></a> outlines the five reasons why they suck:</div><ul class="notion-list notion-list-disc notion-block-0ce0219640e644ed96d2f43548412ced"><li>It is challenging to enable <b>good code versioning</b> because notebooks are big JSON files that cannot be merged automatically.</li></ul><ul class="notion-list notion-list-disc notion-block-87fb71fbbc264987a7ae1918d980ca49"><li><b>Notebook “IDE” is primitive</b>, as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important.</li></ul><ul class="notion-list notion-list-disc notion-block-99a10c57a09944d1970005fad39adab2"><li>It is very hard to structure code reasonably, put code into functions, and <b>develop tests</b> while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently.</li></ul><ul class="notion-list notion-list-disc notion-block-ef7e964ce72e45bf89313c9eb70576a0"><li>Notebooks have <b>out-of-order execution artifacts</b>, meaning that you can easily destroy your current working state when jumping between cells of notebooks.</li></ul><ul class="notion-list notion-list-disc notion-block-d6e2c0034fb14423a463194e0d899b87"><li>It is also difficult to <b>run long or distributed tasks</b>. If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster.</li></ul><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-26e76ab19e59487c9f68b47df1552dbe"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling4.png?table=block&amp;id=26e76ab1-9e59-487c-9f68-b47df1552dbe&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-b4732f187c4d44709e796217b0c4d61d">Recently, a new application framework called <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://streamlit.io/"><b>Streamlit</b></a> was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace"><b>the launch blog post</b></a>, here are the core principles of Streamlit:</div><ul class="notion-list notion-list-disc notion-block-3eb7f9f12ee74228ac93961e74053e93"><li><b>Embrace Python scripting</b>: Streamlit apps are just scripts that run from top to bottom. There’s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps.</li></ul><ul class="notion-list notion-list-disc notion-block-8e54f621ad74436a8e54775ca7a9cd61"><li><b>Treat widgets as variables</b>: There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase.</li></ul><ul class="notion-list notion-list-disc notion-block-ca54c4176f9c4e6c9b28cde871c054c7"><li><b>Reuse data and computation</b>: Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information.</li></ul><div class="notion-text notion-block-4a15aec4b3ba49d3975b832b909e5814">Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku.</div><div class="notion-text notion-block-5632874ed9e544dd8ddef17d7b8e23b5">We recommend using <b>conda</b> to set up your Python and CUDA environments and <b>pip-tools</b> to separate mutually compatible versions of all requirements <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/full-stack-deep-learning/conda-piptools"><b>for our lab</b></a>.</div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-4be418c6c7694284ad33e25fe8925011" data-id="4be418c6c7694284ad33e25fe8925011"><span><div id="4be418c6c7694284ad33e25fe8925011" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4be418c6c7694284ad33e25fe8925011" title="4 - Compute Hardware"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">4 - Compute Hardware</span></span></h2><div class="notion-text notion-block-adb9aa60b5e24e77943c47f18c19e323">We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step.</div><ul class="notion-list notion-list-disc notion-block-53e3812beb244c15887b67fcc5d63b14"><li>During the <b>development</b> stage, we write code, debug models, and look at the results. It’d be nice to be able to compile and train models via an intuitive GUI quickly.</li></ul><ul class="notion-list notion-list-disc notion-block-f90b61a787af41aebf0543a4ac158bd9"><li>During the <b>training/evaluation</b> stage, we design model architecture, search for hyper-parameters, and train large models. It’d be nice to launch experiments and review results easily.</li></ul><div class="notion-text notion-block-7e7b5a9a8c234b838277a1c23729a6d5">Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://openai.com/blog/ai-and-compute/"><b>this 2018 report from OpenAI</b></a>). Looking at recent Transformer models, while <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/abs/2005.14165"><b>OpenAI’s GPT-3</b></a> has not been fully commercialized yet, Google already released the <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/pdf/2101.03961"><b>Switch Transformer</b></a> with orders of magnitude larger in the number of parameters.</div><div class="notion-text notion-block-a29f964fb8164282838eee92aa99975e"><em>So should you get your own hardware, go straight to the cloud, or use on-premise options?</em></div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-3a3eefc01ae04c3e891c8054a9480006" data-id="3a3eefc01ae04c3e891c8054a9480006"><span><div id="3a3eefc01ae04c3e891c8054a9480006" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3a3eefc01ae04c3e891c8054a9480006" title="GPU Basics"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>GPU Basics</b></span></span></h3><div class="notion-text notion-block-cc94a700c0524e17a0f31d25db9e4b26">This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google’s TPUs are the fastest, which is available only on GCP.</div><div class="notion-text notion-block-02dde77df7944a46a7af9682804eba7e">There is a new NVIDIA architecture every year: Kepler -&gt; Pascal -&gt; Volta -&gt; Turing -&gt; Ampere. NVIDIA often released the server version of the cards first, then the “enthusiast” version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version.</div><div class="notion-text notion-block-095fb5a4838a4cf58ece05fa671e47ad">GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. <b>The more data you can fit on the GPU, the larger your batches are, the faster your training goes</b>.</div><div class="notion-text notion-block-2904bb5f0bee48fcacd14cdcd1e081b3">For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed <b>tensor cores</b> that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-95473b57509d4c839d513c351f5f3cf4"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling5.png?table=block&amp;id=95473b57-509d-4c83-9d51-3c351f5f3cf4&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-40a256326e7f4cf386bb26ad6de0d63e">Let’s go through different GPU architectures:</div><ul class="notion-list notion-list-disc notion-block-f60ba5bd476e4c5e9803317b25d3d70b"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://en.wikipedia.org/wiki/Kepler_(microarchitecture)"><b>Kepler</b></a><b>/</b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://en.wikipedia.org/wiki/Maxwell_(microarchitecture)"><b>Maxwell</b></a>: They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80).</li></ul><ul class="notion-list notion-list-disc notion-block-93bde4028e67473480e83d4768acb9bc"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/"><b>Pascal</b></a><b>: They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering.</b></li></ul><ul class="notion-list notion-list-disc notion-block-6d8e694b343e48deaa48d14c0d8886a9"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/"><b>Volta</b></a><b>/</b><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.nvidia.com/en-us/geforce/turing/"><b>Turing</b></a><b>: These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100.</b></li></ul><ul class="notion-list notion-list-disc notion-block-961185357da44a4bb49613be43aa72a6"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.nvidia.com/en-us/data-center/nvidia-ampere-gpu-architecture/"><b>Ampere</b></a><b>: This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing.</b></li></ul><div class="notion-text notion-block-0a258a4f5bd643b5a718eb0809e2eabb">You can check out <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://lambdalabs.com/blog/nvidia-a100-vs-v100-benchmarks/"><b>this recent GPU benchmark</b></a> from Lambda Labs and consult Tim Dettmers’ advice on <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/"><b>which GPUs to get</b></a>.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-815dc2a2d96d4a17824e5218655f7899" data-id="815dc2a2d96d4a17824e5218655f7899"><span><div id="815dc2a2d96d4a17824e5218655f7899" class="notion-header-anchor"></div><a class="notion-hash-link" href="#815dc2a2d96d4a17824e5218655f7899" title="Cloud Options"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Cloud Options</b></span></span></h3><div class="notion-text notion-block-831dfd67d4f345718f2a004e939439d4">Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://lambdalabs.com/service/gpu-cloud"><b>Lambda Labs</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.coreweave.com/pricing"><b>Corewave</b></a> that provide cloud GPUs.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-fec36a7941c54f53b05a5924ebcff651" data-id="fec36a7941c54f53b05a5924ebcff651"><span><div id="fec36a7941c54f53b05a5924ebcff651" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fec36a7941c54f53b05a5924ebcff651" title="On-Prem Options"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>On-Prem Options</b></span></span></h3><div class="notion-text notion-block-28c4aee272e44a79aea10419d85d73b1">You can either <b>build your own</b> or <b>buy pre-built devices</b> from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-a8af587f1fce4c58908c1896c80296eb" data-id="a8af587f1fce4c58908c1896c80296eb"><span><div id="a8af587f1fce4c58908c1896c80296eb" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a8af587f1fce4c58908c1896c80296eb" title="Recommendations"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Recommendations</b></span></span></h3><div class="notion-text notion-block-3e180a30964046169553b0b33ac9dfa8">Even though the cloud is expensive, it’s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it.</div><div class="notion-text notion-block-d5c5b340fc374aa487272f701cb97091">Here are our recommendations for three profiles:</div><ul class="notion-list notion-list-disc notion-block-f7d8bd087121419488942504a5eaa3a6"><li><b>Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation.</b></li></ul><ul class="notion-list notion-list-disc notion-block-71ed635ac08d460b8ce2aaa67da2ae0c"><li><b>Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation.</b></li></ul><ul class="notion-list notion-list-disc notion-block-5efb26e921a44ab99680fe36cc8a5715"><li><b>Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation.</b></li></ul><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-7cebeea21cfd49ffa305f90994a01165" data-id="7cebeea21cfd49ffa305f90994a01165"><span><div id="7cebeea21cfd49ffa305f90994a01165" class="notion-header-anchor"></div><a class="notion-hash-link" href="#7cebeea21cfd49ffa305f90994a01165" title="5 - Resource Management (Kubeflow or Cloud AI)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">5 - Resource Management (Kubeflow or Cloud AI)</span></span></h2><div class="notion-text notion-block-28e8405b8334404e86d2cdf18fc54af9">With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product.</div><div class="notion-text notion-block-b7bf4f0fb8b442a98b06e73206675651">For this challenge of allocating resources to experimenting users, there are some common solutions:</div><ol start="1" class="notion-list notion-list-numbered notion-block-e9bb1ba44c914639b1a532fe87eb6b90"><li><b>Script a solution ourselves</b>: In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-4dafbbdccbdd4254a46f52f67feb4d96"><li><b>SLURM</b>: If we don&#x27;t want to write the script entirely ourselves, standard cluster job schedulers like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://slurm.schedmd.com/documentation.html"><b>SLURM</b></a> can help us. The workflow is as follows: First, a script defines a job’s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-0fb43ce6f71a47daadc3b20f0c6c81a0"><li><b>Docker/Kubernetes</b>: The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.docker.com/"><b>Docker</b></a> packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://kubernetes.io/"><b>Kubernetes</b></a> lets us run these Docker containers on a cluster. In particular, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kubeflow.org/"><b>Kubeflow</b></a> is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup.</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-a5773ca1afd747278a1b30cda1aa1b38"><li><b>Custom ML software</b>: There’s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://aws.amazon.com/sagemaker/"><b>AWS Sagemaker</b></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://gradient.paperspace.com/"><b>Paperspace Gradient</b></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://determined.ai/"><b>Determined AI</b></a> are advancing. Newer startups like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.anyscale.com/"><b>Anyscale</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.grid.ai/"><b>Grid.AI</b></a> (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands.</li></ol><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-7b8b57b4851f4ac9844938425e4eb807" data-id="7b8b57b4851f4ac9844938425e4eb807"><span><div id="7b8b57b4851f4ac9844938425e4eb807" class="notion-header-anchor"></div><a class="notion-hash-link" href="#7b8b57b4851f4ac9844938425e4eb807" title="6 - Frameworks and Distributed Training"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">6 - Frameworks and Distributed Training</span></span></h2><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-67d9886a1afb4692887c446477011cc6" data-id="67d9886a1afb4692887c446477011cc6"><span><div id="67d9886a1afb4692887c446477011cc6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#67d9886a1afb4692887c446477011cc6" title="Deep Learning Frameworks (pytorch + pytorch lighting)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Deep Learning Frameworks (pytorch + pytorch lighting)</b></span></span></h3><div class="notion-text notion-block-bc388637b6754714ad593d38c4c744d3">If you’ve built a deep learning model in the last few years, you’ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pypi.org/project/Theano/"><b>Theano</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://torch.ch/"><b>Torch</b></a> have been around for 10+ years. In contemporary use, there are three main frameworks we’ll focus on - <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/"><b>TensorFlow</b></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://keras.io/"><b>Keras</b></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pytorch.org/"><b>PyTorch</b></a>. We evaluate frameworks based on their utility for <b>production</b> and <b>development</b>.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-83dd171e4fca4123a0073b0d722a9647"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling6.png?table=block&amp;id=83dd171e-4fca-4123-a007-3b0d722a9647&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-562a7959673247e2a6c6df7d740f14c0">When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of “meta-development” posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale.</div><div class="notion-text notion-block-28db0d1ee29f4ee7aa2dac97c0b70c8f">Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.fast.ai/"><b>fast.ai</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.pytorchlightning.ai/"><b>PyTorch Lighting</b></a> add best practices and additional functionality to PyTorch, making it even more popular. According to <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/"><b>this 2018 article on The Gradient</b></a>, more than 80% of submissions are in PyTorch in academic projects.</div><div class="notion-text notion-block-331fc94825204517862683f6ce6cbd5a">All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. <em>Why do we even require such extensive frameworks?</em></div><div class="notion-text notion-block-ea1d7600ee0f40ea91854ca5b3517b85">It’s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://towardsdatascience.com/automatic-differentiation-explained-b4ba8e60c2ad"><b>auto-differentiation</b></a>, an efficient way of computing the gradients, and <b>software compatibility with GPUs</b>, specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model.</div><div class="notion-text notion-block-a9e7ac1337684a8cba96c9d93ef75c1e">New projects like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/google/jax"><b>JAX</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://huggingface.co/"><b>HuggingFace</b></a> offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-6c14622609bb4eb79e0852a46ce52a16" data-id="6c14622609bb4eb79e0852a46ce52a16"><span><div id="6c14622609bb4eb79e0852a46ce52a16" class="notion-header-anchor"></div><a class="notion-hash-link" href="#6c14622609bb4eb79e0852a46ce52a16" title="Distributed Training"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Distributed Training</b></span></span></h3><div class="notion-text notion-block-c0c711d8fd8a45db9f646521fae89617">Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It’s increasingly a must-do. The important thing to note is that <b>distributed training is a process to conduct a single model training process</b>; don’t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-b2c726b2216d4b09a89e52efa88980bf" data-id="b2c726b2216d4b09a89e52efa88980bf"><span><div id="b2c726b2216d4b09a89e52efa88980bf" class="notion-header-anchor"></div><a class="notion-hash-link" href="#b2c726b2216d4b09a89e52efa88980bf" title="DATA PARALLELISM"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>DATA PARALLELISM</b></span></span></h3><div class="notion-text notion-block-92a4f53e10db4ce995fc75f167100970">Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., <em>torch.nn.DataParallel</em>). Other tools like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/horovod/horovod"><b>Horovod</b></a> (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/ray-project/ray"><b>Ray</b></a>, the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-a2908126707d4c3abf959b77adc91373" data-id="a2908126707d4c3abf959b77adc91373"><span><div id="a2908126707d4c3abf959b77adc91373" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a2908126707d4c3abf959b77adc91373" title="MODEL PARALLELISM"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MODEL PARALLELISM</b></span></span></h3><div class="notion-text notion-block-b37d8eaed77b4a38b1da7d52fa7840c7">Model parallelism is a lot more complicated. If you can’t fit your entire model’s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates.  New work is coming out to make this easier (e.g., research and framework maturity).</div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-8a72e69200d644bcbeb1014fa192217c" data-id="8a72e69200d644bcbeb1014fa192217c"><span><div id="8a72e69200d644bcbeb1014fa192217c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8a72e69200d644bcbeb1014fa192217c" title="7 - Experiment Management"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">7 - Experiment Management</span></span></h2><div class="notion-text notion-block-ed6e3b4c473a4322bfc811c55873d9b6">As you run numerous experiments to refine your model, it’s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate’s impact on your model’s performance metric. <em>With multiple model runs, how will you monitor the impact of the hyperparameter?</em></div><div class="notion-text notion-block-579450e0f5154715880a9ebaf4e75d36">A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let’s cover a few of the most common ones:</div><ul class="notion-list notion-list-disc notion-block-b2180beb2488491486f8e45206221bc8"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/tensorboard"><b>TensorBoard</b></a>: This is the default experiment tracking platform that comes with TensorFlow. As a pro, it’s easy to get started with. On the flip side, it’s not very good for tracking and comparing multiple experiments. It’s also not the best solution to store past work.</li></ul><ul class="notion-list notion-list-disc notion-block-4329725fd0294389b0f0fd77be7ea7db"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://mlflow.org/"><b>MLFlow</b></a>: An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://keepsake.ai/"><b>Keepsake</b></a>, recently came out focused solely on experiment tracking.</li></ul><ul class="notion-list notion-list-disc notion-block-66e08e63f2184be2a84fc711e3b586c7"><li>Paid platforms (<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.comet.ml/"><b>Comet.ml</b></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://wandb.ai/"><b>Weights and Biases</b></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://neptune.ai/"><b>Neptune</b></a>): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases.</li></ul><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-1b7c1fead3754d9ea05dd745988a1176" data-id="1b7c1fead3754d9ea05dd745988a1176"><span><div id="1b7c1fead3754d9ea05dd745988a1176" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1b7c1fead3754d9ea05dd745988a1176" title="8 - Hyperparameter Tuning"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">8 - Hyperparameter Tuning</span></span></h2><div class="notion-text notion-block-7ad14291ac6748eeb65bc6b307545cac">To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it’s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there’s an increasing number of software providers that do precisely this:</div><ul class="notion-list notion-list-disc notion-block-fbd6586195e04e4d8d2b7c125351cd35"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://sigopt.com/"><b>SigOpt</b></a> offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt’s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you’ve found the best parameters for your model.</li></ul><ul class="notion-list notion-list-disc notion-block-dda0e30393474e2da191f49e1d884bd9"><li>Rather than an API, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://docs.ray.io/en/master/tune/index.html"><b>Ray Tune</b></a> offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed.</li></ul><ul class="notion-list notion-list-disc notion-block-a8b7df78da3f434cb1c964de2319fb32"><li>Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a “<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://wandb.ai/site/sweeps"><b>sweep</b></a>,” during which W&amp;B sends parameter settings to individual “agents” (our machines) and compares performance.</li></ul><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-e9b3edabf46748e583fdc4eb0a1aa29a" data-id="e9b3edabf46748e583fdc4eb0a1aa29a"><span><div id="e9b3edabf46748e583fdc4eb0a1aa29a" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e9b3edabf46748e583fdc4eb0a1aa29a" title="9 - “All-In-One” Solutions"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">9 - “All-In-One” Solutions</span></span></h2><div class="notion-text notion-block-033da8e3b4f64bb1ae2574e99dde19f2">Some platforms integrate all the aspects of the applied ML stack we’ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the “lifecycle,” these platforms typically include:</div><ul class="notion-list notion-list-disc notion-block-597e6866bd7f47e38af31cc6acad784d"><li>Labeling and data querying services</li></ul><ul class="notion-list notion-list-disc notion-block-73cd8e7d72694a77aec417dc9be9df59"><li>Model training, especially though job scaling and scheduling</li></ul><ul class="notion-list notion-list-disc notion-block-49af8ce58aa34b218b753ff58ba7daca"><li>Experiment tracking and model versioning</li></ul><ul class="notion-list notion-list-disc notion-block-f0b0cdd509584657ae73ba6b273887c4"><li>Development environments, typically through notebook-style interfaces</li></ul><ul class="notion-list notion-list-disc notion-block-eaebbec43d5e487ba72a056bb875d72e"><li>Model deployment (e.g., via REST APIs) and monitoring</li></ul><div class="notion-text notion-block-1436459684a24c198a8b2cbacfe0e470">One of the earliest examples of such a system is Facebook’s <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"><b>FBLearner</b></a> (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model’s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://cloud.google.com/ai-platform"><b>Google Cloud AI Platform</b></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://aws.amazon.com/sagemaker/"><b>AWS SageMaker</b></a>. Startups like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://gradient.paperspace.com/"><b>Paperspace Gradient</b></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://neptune.ai/"><b>Neptune</b></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.floydhub.com/"><b>FloydHub</b></a> also offer all-in-one platforms focused on deep learning. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://github.com/determined-ai/determined"><b>Determined AI</b></a>, which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.dominodatalab.com/"><b>Domino Data Lab</b></a> is a traditional ML-focused startup with an extensive feature set worth looking at. It’s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this.</div><div class="notion-text notion-block-59dfd15fe14848808870ba1907ae0781">In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-d922db2da59e45088058d2e95c09f301"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-6-notes-media%2FInfra-Tooling7.png?table=block&amp;id=d922db2d-a59e-4508-8058-d2e95c09f301&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure></article><aside class="notion-aside"><div class="notion-aside-table-of-contents"><div class="notion-aside-table-of-contents-header">Table of Contents</div><nav class="notion-table-of-contents notion-gray"><a href="#8e76d7445b544e8a97c2311ab506fa65" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">1 - Dream vs. Reality for ML Practitioners</span></a><a href="#254d31ebf05b48e782e438a40348ac53" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">2 - Three Buckets of Tooling Landscape</span></a><a href="#3fedfd6eb96749a2b11eaa60271a459d" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">3 - Software Engineering (vscode)</span></a><a href="#4be418c6c7694284ad33e25fe8925011" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">4 - Compute Hardware</span></a><a href="#3a3eefc01ae04c3e891c8054a9480006" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">GPU Basics</span></a><a href="#815dc2a2d96d4a17824e5218655f7899" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Cloud Options</span></a><a href="#fec36a7941c54f53b05a5924ebcff651" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">On-Prem Options</span></a><a href="#a8af587f1fce4c58908c1896c80296eb" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Recommendations</span></a><a href="#7cebeea21cfd49ffa305f90994a01165" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">5 - Resource Management (Kubeflow or Cloud AI)</span></a><a href="#7b8b57b4851f4ac9844938425e4eb807" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">6 - Frameworks and Distributed Training</span></a><a href="#67d9886a1afb4692887c446477011cc6" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Deep Learning Frameworks (pytorch + pytorch lighting)</span></a><a href="#6c14622609bb4eb79e0852a46ce52a16" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Distributed Training</span></a><a href="#b2c726b2216d4b09a89e52efa88980bf" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">DATA PARALLELISM</span></a><a href="#a2908126707d4c3abf959b77adc91373" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">MODEL PARALLELISM</span></a><a href="#8a72e69200d644bcbeb1014fa192217c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">7 - Experiment Management</span></a><a href="#1b7c1fead3754d9ea05dd745988a1176" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">8 - Hyperparameter Tuning</span></a><a href="#e9b3edabf46748e583fdc4eb0a1aa29a" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">9 - “All-In-One” Solutions</span></a></nav></div><div class="PageSocial_pageSocial__2WqHl"><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://yihui-he.github.io" title="personal website" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="#000000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_github__slQ0z" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_linkedin__nElHT" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"></path></svg></div></a></div></aside></div></main><footer class="styles_footer__1r_c6"><div class="styles_copyright__3kWHj">Copyright 2022 <!-- -->Yihui He</div><div class="styles_social__235gY"><a class="styles_twitter__WwfaA" href="https://yihui-he.github.io" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></a><a class="styles_twitter__WwfaA" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a class="styles_github__32xIr" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__1XGvB" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"yihui-he.github.io","name":"Yihui He","rootNotionPageId":"bd32b787e47149f38941174a9c6846b6","rootNotionSpaceId":null,"description":"Yihui He, AI research scientist / full stack engineer"},"recordMap":{"block":{"e9d171d8-17bc-4aa6-88f7-79391cf072d1":{"role":"reader","value":{"id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","version":171,"type":"page","properties":{"title":[["MLOps Infra \u0026 tooling"]]},"content":["8e76d744-5b54-4e8a-97c2-311ab506fa65","55addcdc-f7ed-49f0-b4c7-f35e981ba283","e9a06f7b-eea4-4247-bf34-ea5fc01952f8","31903d73-ff4f-44a3-a6b0-90a65e791004","c1dd873d-2411-4ed3-96a3-7669d39d9b16","8ca61f12-4229-49c9-9d20-90b58e59b4ff","2d02886b-7bb0-4b9d-aba4-de9049e6b204","e09e4b95-0909-410b-af3a-0180d9bf5cbe","71be3474-fc04-4f68-b355-b0845ca3958d","e0b64fba-6863-427b-ae06-37fbb5be728b","5a87f2b7-83c2-4617-9003-af64ffa7616f","184ecab1-c466-4eb5-b344-e69b7eb4da62","c8f16ecf-dfc2-4c25-8461-fb9376e7e909","b3699db9-52f7-4af8-8992-53ef41de9c53","254d31eb-f05b-48e7-82e4-38a40348ac53","24fb98a9-a378-42ba-9930-9f898264c20f","4edad84c-b021-451a-8b5d-6c753c436100","79921b56-8d41-4fca-ab5b-3f510ed8a6b4","93e1e6f0-b0b1-47f0-a0a0-538cb042f730","5d8ff8f7-f803-43a9-aa01-21ccc36bda54","32303c04-0b73-4a86-bcd0-e6b2ab7d3eb6","3fedfd6e-b967-49a2-b11e-aa60271a459d","c4498607-471a-41c6-bdcb-da10d3f82a84","98a5f6ea-1903-4b94-8bd3-f6009c0ac99c","d5421b4b-eb90-4f27-88fb-df5e36c72d3a","9ddf4f56-d75b-4a38-98c8-24c4458bbf99","0ce02196-40e6-44ed-96d2-f43548412ced","87fb71fb-bc26-4987-a7ae-1918d980ca49","99a10c57-a099-44d1-9700-05fad39adab2","ef7e964c-e72e-45bf-8931-3c9eb70576a0","d6e2c003-4fb1-4423-a463-194e0d899b87","26e76ab1-9e59-487c-9f68-b47df1552dbe","b4732f18-7c4d-4470-9e79-6217b0c4d61d","3eb7f9f1-2ee7-4228-ac93-961e74053e93","8e54f621-ad74-436a-8e54-775ca7a9cd61","ca54c417-6f9c-4e6c-9b28-cde871c054c7","4a15aec4-b3ba-49d3-975b-832b909e5814","5632874e-d9e5-44dd-8dde-f17d7b8e23b5","4be418c6-c769-4284-ad33-e25fe8925011","adb9aa60-b5e2-4e77-943c-47f18c19e323","53e3812b-eb24-4c15-887b-67fcc5d63b14","f90b61a7-87af-41ae-bf05-43a4ac158bd9","7e7b5a9a-8c23-4b83-8277-a1c23729a6d5","a29f964f-b816-4282-838e-ee92aa99975e","3a3eefc0-1ae0-4c3e-891c-8054a9480006","cc94a700-c052-4e17-a0f3-1d25db9e4b26","02dde77d-f794-4a46-a7af-9682804eba7e","095fb5a4-838a-4cf5-8ece-05fa671e47ad","2904bb5f-0bee-48fc-acd1-4cdcd1e081b3","95473b57-509d-4c83-9d51-3c351f5f3cf4","40a25632-6e7f-4cf3-86bb-26ad6de0d63e","f60ba5bd-476e-4c5e-9803-317b25d3d70b","93bde402-8e67-4734-80e8-3d4768acb9bc","6d8e694b-343e-48de-aa48-d14c0d8886a9","96118535-7da4-4a4b-b496-13be43aa72a6","0a258a4f-5bd6-43b5-a718-eb0809e2eabb","815dc2a2-d96d-4a17-824e-5218655f7899","831dfd67-d4f3-4571-8f2a-004e939439d4","fec36a79-41c5-4f53-b05a-5924ebcff651","28c4aee2-72e4-4a79-aea1-0419d85d73b1","a8af587f-1fce-4c58-908c-1896c80296eb","3e180a30-9640-4616-9553-b0b33ac9dfa8","d5c5b340-fc37-4aa4-8727-2f701cb97091","f7d8bd08-7121-4194-8894-2504a5eaa3a6","71ed635a-c08d-460b-8ce2-aaa67da2ae0c","5efb26e9-21a4-4ab9-9680-fe36cc8a5715","7cebeea2-1cfd-49ff-a305-f90994a01165","28e8405b-8334-404e-86d2-cdf18fc54af9","b7bf4f0f-b8b4-42a9-8b06-e73206675651","e9bb1ba4-4c91-4639-b1a5-32fe87eb6b90","4dafbbdc-cbdd-4254-a46f-52f67feb4d96","0fb43ce6-f71a-47da-adc3-b20f0c6c81a0","a5773ca1-afd7-4727-8a1b-30cda1aa1b38","7b8b57b4-851f-4ac9-8449-38425e4eb807","67d9886a-1afb-4692-887c-446477011cc6","bc388637-b675-4714-ad59-3d38c4c744d3","83dd171e-4fca-4123-a007-3b0d722a9647","562a7959-6732-47e2-a6c6-df7d740f14c0","28db0d1e-e29f-4ee7-aa2d-ac97c0b70c8f","331fc948-2520-4517-8626-83f6ce6cbd5a","ea1d7600-ee0f-40ea-9185-4ca5b3517b85","a9e7ac13-3768-4a8c-ba96-c9d93ef75c1e","6c146226-09bb-4eb7-9e08-52a46ce52a16","c0c711d8-fd8a-45db-9f64-6521fae89617","b2c726b2-216d-4b09-a89e-52efa88980bf","92a4f53e-10db-4ce9-95fc-75f167100970","a2908126-707d-4c3a-bf95-9b77adc91373","b37d8eae-d77b-4a38-b1da-7d52fa7840c7","8a72e692-00d6-44bc-beb1-014fa192217c","ed6e3b4c-473a-4322-bfc8-11c55873d9b6","579450e0-f515-4715-880a-9ebaf4e75d36","b2180beb-2488-4914-86f8-e45206221bc8","4329725f-d029-4389-b0f0-fd77be7ea7db","66e08e63-f218-4be2-a84f-c711e3b586c7","1b7c1fea-d375-4d9e-a05d-d745988a1176","7ad14291-ac67-48ee-b65b-c6b307545cac","fbd65861-95e0-4e4d-8d2b-7c125351cd35","dda0e303-9347-4e2d-a191-f49e1d884bd9","a8b7df78-da3f-434c-b1c9-64de2319fb32","e9b3edab-f467-48e5-83fd-c4eb0a1aa29a","033da8e3-b4f6-4bb1-ae25-74e99dde19f2","597e6866-bd7f-47e3-8af3-1cc6acad784d","73cd8e7d-7269-4a77-aec4-17dc9be9df59","49af8ce5-8aa3-4b21-8b75-3ff58ba7daca","f0b0cdd5-0958-4657-ae73-ba6b273887c4","eaebbec4-3d5e-487b-a72a-056bb875d72e","14364596-84a2-4c19-8a8b-2cbacfe0e470","59dfd15f-e148-4880-8870-ba1907ae0781","d922db2d-a59e-4508-8058-d2e95c09f301"],"created_time":1645635360000,"last_edited_time":1646024640000,"parent_id":"41249f3f-76a2-458a-b13d-2bc06310337b","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"41249f3f-76a2-458a-b13d-2bc06310337b":{"role":"reader","value":{"id":"41249f3f-76a2-458a-b13d-2bc06310337b","version":205,"type":"page","properties":{"title":[["deep learning engineer manual"]]},"content":["fcd4d688-6b0e-4ee7-8838-53a4d48a0af4","0b626c07-cbba-41a8-9d08-978d41ee2200","e9d171d8-17bc-4aa6-88f7-79391cf072d1","6dd6d37e-aa69-45af-8c37-4bb14f716fa2","94cea945-881b-4d6b-be5c-505834b715db","a1ce7ac8-2778-42ea-b8fb-f03493372730","4f6aae13-77aa-40ad-a5fb-7db5b90c3ae3","f9e18efc-827a-4f89-a8fc-e8975dcadc02","3476e08f-8137-4d53-a65f-9f0ea5c2ccf0","49123afb-c329-487d-afa0-3c5efe8cb545","45eddebe-91be-4903-b581-278254e0b525","d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","bd908e48-f0ff-4c31-b2c8-095924adfaf7","032e90f7-6a0b-43bd-ad11-65169e05639c","92d36485-cfb3-4100-9fa9-567395e4385c","4ffbb2cd-f634-421a-9e5a-9f2758193609"],"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"}],"created_time":1645559700000,"last_edited_time":1646026920000,"parent_id":"bd32b787-e471-49f3-8941-174a9c6846b6","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"bd32b787-e471-49f3-8941-174a9c6846b6":{"role":"reader","value":{"id":"bd32b787-e471-49f3-8941-174a9c6846b6","version":275,"type":"page","properties":{"title":[["Yihui He’s Blog"]]},"content":["cc62526f-e4eb-42e5-8e8f-a22c3abd7b46","eb534ff0-d0e6-4c83-b817-92c761ad9034","07cdf7d1-24c9-4fd7-abc0-e1f79f38aafd","41249f3f-76a2-458a-b13d-2bc06310337b","09ac442d-d3b5-4f43-9531-90cdae7640b7","76f263cc-0736-4657-91dd-551b1c541b4c","fad54c70-af5c-42a3-a00a-ff18bea9c382","9abe926f-8b36-4a60-a77e-e5436690a8c0","84c674a7-bbe3-4ac5-88c7-68e0c3909585","09517098-a843-437e-bb7a-4ac67866b29b","e7a12f56-b48c-43bf-803e-96d70518685f","932f5707-ab83-4542-af11-68fec7b5f28d","e6311219-0ef8-4142-8a3f-eeedc910dd3b","5f16dda4-316e-43af-a4ea-8fc6d6938404","23859d82-2201-4bb6-a86a-db693ea4992a","d5a94597-8cb4-43be-a6d0-0840b5518710","c54caf49-d28c-46ac-b72d-fc9bfa58bbdd","4751172a-1cb9-4633-84d6-21ba334ee3d7","8e23ed11-04f5-48ab-a95d-1f47a5cd22af","4b2fb8ba-1e4c-47ff-903b-0869360db746","010faf71-b9e5-4c47-832a-658dafc6e7ae","3db8e8fd-7383-4aea-8beb-608282db050e","60ff1c6d-fde5-426d-ac57-7bba2a5296a2","31008559-3d6f-4c33-bb50-4e463c9b19fb","201efca9-ba21-47c3-8b0d-35b41dd3f91f","04ced6eb-ca82-46db-8f8e-077f3bf6b29c","daf31581-086b-4fb1-9695-5a79117d4300","a0487f2c-eed0-4154-a6e8-2366ca1d7939","95728d67-dc1f-4086-881b-1cd1bf72f985","cba2b20a-aa13-466c-bcb6-a9f9c9f057da","5e0b979d-ef00-44a3-894f-38d28cfd7576","38a364d6-720a-4788-aeee-c2b224745860","6a838606-233d-4584-b2f1-3d198b1603e8","f93f1e88-d826-4cd3-ab2a-cd92ddd2ac53","e8156bf2-5303-4590-a44a-6db1404c8c7b","2411768d-af34-4f88-8772-c46ca4428256","52357a78-3241-4a0a-bcb7-46bfd15070a4","3f4c64fb-75dc-4ccd-9e22-ce5704d0dfdb","d1ef1d4d-bfa8-40c4-a78c-037da96f3681","e5ac54eb-e227-49f0-9bf8-31ef47dbd9f9"],"discussions":["a13be18e-5008-4f8c-b07a-0fadb99d7450"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63e89fe9-f5cd-4414-be0e-53d91aa54fc3/me_small.jpg"},"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"},{"role":"reader","type":"public_permission","added_timestamp":1645466303668}],"created_time":1645422720000,"last_edited_time":1646039760000,"parent_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1","parent_table":"space","alive":true,"file_ids":["e3d85759-49f2-40d2-ba30-d25ca401872e","cfc564cc-91ae-4a96-a912-fcc9a28ec5db","63e89fe9-f5cd-4414-be0e-53d91aa54fc3"],"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8e76d744-5b54-4e8a-97c2-311ab506fa65":{"role":"reader","value":{"id":"8e76d744-5b54-4e8a-97c2-311ab506fa65","version":7,"type":"header","properties":{"title":[["1 - Dream vs. Reality for ML Practitioners"]]},"created_time":1645635396487,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"55addcdc-f7ed-49f0-b4c7-f35e981ba283":{"role":"reader","value":{"id":"55addcdc-f7ed-49f0-b4c7-f35e981ba283","version":2,"type":"text","properties":{"title":[["The "],["dream",[["b"]]],[" of ML practitioners is that we are provided the data, and somehow we build an optimal machine learning prediction system available as a scalable API or an edge deployment. That deployment then generates more data for us, which can be used to improve our system."]]},"created_time":1645635396487,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e9a06f7b-eea4-4247-bf34-ea5fc01952f8":{"role":"reader","value":{"id":"e9a06f7b-eea4-4247-bf34-ea5fc01952f8","version":2,"type":"text","properties":{"title":[["The "],["reality",[["b"]]],[" is that you will have to:"]]},"created_time":1645635396487,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"31903d73-ff4f-44a3-a6b0-90a65e791004":{"role":"reader","value":{"id":"31903d73-ff4f-44a3-a6b0-90a65e791004","version":2,"type":"bulleted_list","properties":{"title":[["Aggregate, process, clean, label, and version the data"]]},"created_time":1645635396487,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c1dd873d-2411-4ed3-96a3-7669d39d9b16":{"role":"reader","value":{"id":"c1dd873d-2411-4ed3-96a3-7669d39d9b16","version":2,"type":"bulleted_list","properties":{"title":[["Write and debug model code"]]},"created_time":1645635396487,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8ca61f12-4229-49c9-9d20-90b58e59b4ff":{"role":"reader","value":{"id":"8ca61f12-4229-49c9-9d20-90b58e59b4ff","version":2,"type":"bulleted_list","properties":{"title":[["Provision compute"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2d02886b-7bb0-4b9d-aba4-de9049e6b204":{"role":"reader","value":{"id":"2d02886b-7bb0-4b9d-aba4-de9049e6b204","version":2,"type":"bulleted_list","properties":{"title":[["Run many experiments and review the results"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e09e4b95-0909-410b-af3a-0180d9bf5cbe":{"role":"reader","value":{"id":"e09e4b95-0909-410b-af3a-0180d9bf5cbe","version":2,"type":"bulleted_list","properties":{"title":[["Discover that you did something wrong or maybe try a different architecture -\u003e Write more code and provision more compute"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"71be3474-fc04-4f68-b355-b0845ca3958d":{"role":"reader","value":{"id":"71be3474-fc04-4f68-b355-b0845ca3958d","version":2,"type":"bulleted_list","properties":{"title":[["Deploy the model when you are happy"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e0b64fba-6863-427b-ae06-37fbb5be728b":{"role":"reader","value":{"id":"e0b64fba-6863-427b-ae06-37fbb5be728b","version":2,"type":"bulleted_list","properties":{"title":[["Monitor the predictions that the model makes on production data so that you can gather some good examples and feed them back to the initial data flywheel loop"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5a87f2b7-83c2-4617-9003-af64ffa7616f":{"role":"reader","value":{"id":"5a87f2b7-83c2-4617-9003-af64ffa7616f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling1.png"]]},"created_time":1645635396488,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"184ecab1-c466-4eb5-b344-e69b7eb4da62":{"role":"reader","value":{"id":"184ecab1-c466-4eb5-b344-e69b7eb4da62","version":2,"type":"text","properties":{"title":[["For example, the slide above is from "],["Andrej Karpathy’s talk",[["b"],["a","https://www.youtube.com/watch?v=oBklltKXtDE"]]],[" at PyTorch Devcon 2019 discussing Tesla’s self-driving system. Their dream is to build a system that goes from the data gathered through their training, evaluation, and inference processes and gets deployed on the cars. As people drive, more data will be collected and added back to the training set. As this process repeats, Tesla’s ML engineers can all go on vacation :)"]]},"created_time":1645635396501,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c8f16ecf-dfc2-4c25-8461-fb9376e7e909":{"role":"reader","value":{"id":"c8f16ecf-dfc2-4c25-8461-fb9376e7e909","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling2.png"]]},"created_time":1645635396501,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b3699db9-52f7-4af8-8992-53ef41de9c53":{"role":"reader","value":{"id":"b3699db9-52f7-4af8-8992-53ef41de9c53","version":2,"type":"text","properties":{"title":[["The picture above (from the famous Google paper “"],["Machine Learning: The High-Interest Credit Card of Technical Debt",[["b"],["a","https://papers.nips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf"]]],["”) shows that "],["the ML code portion in a real-world ML system is a lot smaller than the infrastructure needed for its support",[["b"]]],[". As ML projects move from small-scale research experiments to large-scale industry deployments, your organization most likely will require a massive amount of infrastructure to support large inferences, distributed training, data processing pipelines, reproducible experiments, model monitoring, etc."]]},"created_time":1645635396502,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"254d31eb-f05b-48e7-82e4-38a40348ac53":{"role":"reader","value":{"id":"254d31eb-f05b-48e7-82e4-38a40348ac53","version":7,"type":"header","properties":{"title":[["2 - Three Buckets of Tooling Landscape"]]},"created_time":1645635396502,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"24fb98a9-a378-42ba-9930-9f898264c20f":{"role":"reader","value":{"id":"24fb98a9-a378-42ba-9930-9f898264c20f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling3.png"]]},"created_time":1645635396502,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4edad84c-b021-451a-8b5d-6c753c436100":{"role":"reader","value":{"id":"4edad84c-b021-451a-8b5d-6c753c436100","version":2,"type":"text","properties":{"title":[["We can break down the landscape of all this necessary infrastructure into three buckets: data, training/evaluation, and deployment."]]},"created_time":1645635396502,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"79921b56-8d41-4fca-ab5b-3f510ed8a6b4":{"role":"reader","value":{"id":"79921b56-8d41-4fca-ab5b-3f510ed8a6b4","version":2,"type":"bulleted_list","properties":{"title":[["The "],["data",[["b"]]],[" bucket includes the data sources, data lakes/warehouses, data processing, data exploration, data versioning, and data labeling."]]},"created_time":1645635396502,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"93e1e6f0-b0b1-47f0-a0a0-538cb042f730":{"role":"reader","value":{"id":"93e1e6f0-b0b1-47f0-a0a0-538cb042f730","version":2,"type":"bulleted_list","properties":{"title":[["The "],["training/evaluation",[["b"]]],[" bucket includes compute sources, resource management, software engineering, frameworks and distributed training libraries, experiment management, and hyper-parameter tuning."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5d8ff8f7-f803-43a9-aa01-21ccc36bda54":{"role":"reader","value":{"id":"5d8ff8f7-f803-43a9-aa01-21ccc36bda54","version":2,"type":"bulleted_list","properties":{"title":[["The "],["deployment",[["b"]]],[" bucket includes continuous integration and testing, edge deployment, web deployment, monitoring, and feature store."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"32303c04-0b73-4a86-bcd0-e6b2ab7d3eb6":{"role":"reader","value":{"id":"32303c04-0b73-4a86-bcd0-e6b2ab7d3eb6","version":2,"type":"text","properties":{"title":[["There are also several vendors offering “all-in-one” MLOps solutions that cover all three buckets. This lecture focuses on the training/evaluation bucket."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3fedfd6e-b967-49a2-b11e-aa60271a459d":{"role":"reader","value":{"id":"3fedfd6e-b967-49a2-b11e-aa60271a459d","version":33,"type":"header","properties":{"title":[["3 - Software Engineering (vscode)"]]},"created_time":1645635396503,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c4498607-471a-41c6-bdcb-da10d3f82a84":{"role":"reader","value":{"id":"c4498607-471a-41c6-bdcb-da10d3f82a84","version":2,"type":"text","properties":{"title":[["When it comes to writing deep learning code, "],["Python is the clear programming language of choice",[["b"]]],[". As a general-purpose language, Python is easy to learn and easily accessible, enabling you to find skilled developers on a faster basis. It has various scientific libraries for data wrangling and machine learning (Pandas, NumPy, Scikit-Learn, etc.). Regardless of whether your engineering colleagues write code in a lower-level language like C, C++, or Java, it is generally neat to join different components with a Python wrapper."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"98a5f6ea-1903-4b94-8bd3-f6009c0ac99c":{"role":"reader","value":{"id":"98a5f6ea-1903-4b94-8bd3-f6009c0ac99c","version":2,"type":"text","properties":{"title":[["When choosing your IDEs, there are many options out there",[["b"]]],[" (Vim, Emacs, Sublime Text, Jupyter, VS Code, PyCharm, Atom, etc.). Each of these has its uses in any application, and you’re better to switch between them to remain agile without relying heavily on shortcuts and packages. It also helps teams work better if they can jump into different IDEs and comment/collaborate with other colleagues. In particular, "],["Visual Studio Code",[["b"],["a","https://code.visualstudio.com/"]]],[" makes for a very nice Python experience, where you have access to built-in git staging and diffing, peek at documentation, linter code as you write, and open projects remotely."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d5421b4b-eb90-4f27-88fb-df5e36c72d3a":{"role":"reader","value":{"id":"d5421b4b-eb90-4f27-88fb-df5e36c72d3a","version":2,"type":"text","properties":{"title":[["Jupyter Notebooks",[["b"],["a","https://jupyter.org/"]]],[" have rapidly grown in popularity among data scientists to become the standard for quick prototyping and exploratory analysis. For example, "],["Netflix based all of their machine learning workflows on them",[["b"],["a","https://medium.com/netflix-techblog/notebook-innovation-591ee3221233"]]],[", effectively building a whole notebook infrastructure to leverage them as a unifying layer for scheduling workflows. Jeremy Howard develops his fast.ai codebase entirely with notebooks and introduces a project called "],["nbdev",[["b"],["a","https://github.com/fastai/nbdev"]]],[" that shows people how to develop well-tested code in a notebook environment."]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9ddf4f56-d75b-4a38-98c8-24c4458bbf99":{"role":"reader","value":{"id":"9ddf4f56-d75b-4a38-98c8-24c4458bbf99","version":2,"type":"text","properties":{"title":[["However, "],["there are many problems with using notebooks as a last resort when working in teams that aim to build machine/deep learning products",[["b"]]],[". "],["Alexander Mueller's blog post",[["b"],["a","https://towardsdatascience.com/5-reasons-why-jupyter-notebooks-suck-4dc201e27086"]]],[" outlines the five reasons why they suck:"]]},"created_time":1645635396503,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0ce02196-40e6-44ed-96d2-f43548412ced":{"role":"reader","value":{"id":"0ce02196-40e6-44ed-96d2-f43548412ced","version":2,"type":"bulleted_list","properties":{"title":[["It is challenging to enable "],["good code versioning",[["b"]]],[" because notebooks are big JSON files that cannot be merged automatically."]]},"created_time":1645635396504,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"87fb71fb-bc26-4987-a7ae-1918d980ca49":{"role":"reader","value":{"id":"87fb71fb-bc26-4987-a7ae-1918d980ca49","version":2,"type":"bulleted_list","properties":{"title":[["Notebook “IDE” is primitive",[["b"]]],[", as they have no integration, no lifting, and no code-style correction. Data scientists are not software engineers, and thus, tools that govern their code quality and help improve it are very important."]]},"created_time":1645635396504,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"99a10c57-a099-44d1-9700-05fad39adab2":{"role":"reader","value":{"id":"99a10c57-a099-44d1-9700-05fad39adab2","version":2,"type":"bulleted_list","properties":{"title":[["It is very hard to structure code reasonably, put code into functions, and "],["develop tests",[["b"]]],[" while working in notebooks. You better develop Python scripts based on test-driven development principles as soon as you want to reproduce some experiments and run notebooks frequently."]]},"created_time":1645635396504,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ef7e964c-e72e-45bf-8931-3c9eb70576a0":{"role":"reader","value":{"id":"ef7e964c-e72e-45bf-8931-3c9eb70576a0","version":2,"type":"bulleted_list","properties":{"title":[["Notebooks have "],["out-of-order execution artifacts",[["b"]]],[", meaning that you can easily destroy your current working state when jumping between cells of notebooks."]]},"created_time":1645635396505,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d6e2c003-4fb1-4423-a463-194e0d899b87":{"role":"reader","value":{"id":"d6e2c003-4fb1-4423-a463-194e0d899b87","version":2,"type":"bulleted_list","properties":{"title":[["It is also difficult to "],["run long or distributed tasks",[["b"]]],[". If you want to handle big datasets, better pull your code out of notebooks, start a Python folder, create fixtures, write tests, and then deploy your application to a cluster."]]},"created_time":1645635396505,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"26e76ab1-9e59-487c-9f68-b47df1552dbe":{"role":"reader","value":{"id":"26e76ab1-9e59-487c-9f68-b47df1552dbe","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling4.png"]]},"created_time":1645635396505,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b4732f18-7c4d-4470-9e79-6217b0c4d61d":{"role":"reader","value":{"id":"b4732f18-7c4d-4470-9e79-6217b0c4d61d","version":2,"type":"text","properties":{"title":[["Recently, a new application framework called "],["Streamlit",[["b"],["a","https://streamlit.io/"]]],[" was introduced. The creators of the framework wanted machine learning engineers to be able to create beautiful apps without needing a tools team; in other words, these internal tools should arise as a natural byproduct of the machine learning workflow. According to "],["the launch blog post",[["b"],["a","https://towardsdatascience.com/coding-ml-tools-like-you-code-ml-models-ddba3357eace"]]],[", here are the core principles of Streamlit:"]]},"created_time":1645635396505,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3eb7f9f1-2ee7-4228-ac93-961e74053e93":{"role":"reader","value":{"id":"3eb7f9f1-2ee7-4228-ac93-961e74053e93","version":2,"type":"bulleted_list","properties":{"title":[["Embrace Python scripting",[["b"]]],[": Streamlit apps are just scripts that run from top to bottom. There’s no hidden state. You can factor your code with function calls. If you know how to write Python scripts, you can write Streamlit apps."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8e54f621-ad74-436a-8e54-775ca7a9cd61":{"role":"reader","value":{"id":"8e54f621-ad74-436a-8e54-775ca7a9cd61","version":2,"type":"bulleted_list","properties":{"title":[["Treat widgets as variables",[["b"]]],[": There are no callbacks in Streamlit. Every interaction simply reruns the script from top to bottom. This approach leads to a clean codebase."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ca54c417-6f9c-4e6c-9b28-cde871c054c7":{"role":"reader","value":{"id":"ca54c417-6f9c-4e6c-9b28-cde871c054c7","version":2,"type":"bulleted_list","properties":{"title":[["Reuse data and computation",[["b"]]],[": Streamlit introduces a cache primitive that behaves like a persistent, immutable-by-default data store that lets Streamlit apps safely and effortlessly reuse information."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4a15aec4-b3ba-49d3-975b-832b909e5814":{"role":"reader","value":{"id":"4a15aec4-b3ba-49d3-975b-832b909e5814","version":2,"type":"text","properties":{"title":[["Right now, Streamlit is building features that enable sharing machine learning projects to be as easy as pushing a web app to Heroku."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5632874e-d9e5-44dd-8dde-f17d7b8e23b5":{"role":"reader","value":{"id":"5632874e-d9e5-44dd-8dde-f17d7b8e23b5","version":2,"type":"text","properties":{"title":[["We recommend using "],["conda",[["b"]]],[" to set up your Python and CUDA environments and "],["pip-tools",[["b"]]],[" to separate mutually compatible versions of all requirements "],["for our lab",[["b"],["a","https://github.com/full-stack-deep-learning/conda-piptools"]]],["."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4be418c6-c769-4284-ad33-e25fe8925011":{"role":"reader","value":{"id":"4be418c6-c769-4284-ad33-e25fe8925011","version":7,"type":"header","properties":{"title":[["4 - Compute Hardware"]]},"created_time":1645635396506,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"adb9aa60-b5e2-4e77-943c-47f18c19e323":{"role":"reader","value":{"id":"adb9aa60-b5e2-4e77-943c-47f18c19e323","version":2,"type":"text","properties":{"title":[["We can break down the compute needs into an early-stage development step and a late-stage training/evaluation step."]]},"created_time":1645635396506,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"53e3812b-eb24-4c15-887b-67fcc5d63b14":{"role":"reader","value":{"id":"53e3812b-eb24-4c15-887b-67fcc5d63b14","version":2,"type":"bulleted_list","properties":{"title":[["During the "],["development",[["b"]]],[" stage, we write code, debug models, and look at the results. It’d be nice to be able to compile and train models via an intuitive GUI quickly."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f90b61a7-87af-41ae-bf05-43a4ac158bd9":{"role":"reader","value":{"id":"f90b61a7-87af-41ae-bf05-43a4ac158bd9","version":2,"type":"bulleted_list","properties":{"title":[["During the "],["training/evaluation",[["b"]]],[" stage, we design model architecture, search for hyper-parameters, and train large models. It’d be nice to launch experiments and review results easily."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7e7b5a9a-8c23-4b83-8277-a1c23729a6d5":{"role":"reader","value":{"id":"7e7b5a9a-8c23-4b83-8277-a1c23729a6d5","version":2,"type":"text","properties":{"title":[["Compute matters with each passing year due to the fact that the results came out of deep learning are using more and more compute (check out "],["this 2018 report from OpenAI",[["b"],["a","https://openai.com/blog/ai-and-compute/"]]],["). Looking at recent Transformer models, while "],["OpenAI’s GPT-3",[["b"],["a","https://arxiv.org/abs/2005.14165"]]],[" has not been fully commercialized yet, Google already released the "],["Switch Transformer",[["b"],["a","https://arxiv.org/pdf/2101.03961"]]],[" with orders of magnitude larger in the number of parameters."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a29f964f-b816-4282-838e-ee92aa99975e":{"role":"reader","value":{"id":"a29f964f-b816-4282-838e-ee92aa99975e","version":2,"type":"text","properties":{"title":[["So should you get your own hardware, go straight to the cloud, or use on-premise options?",[["i"]]]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3a3eefc0-1ae0-4c3e-891c-8054a9480006":{"role":"reader","value":{"id":"3a3eefc0-1ae0-4c3e-891c-8054a9480006","version":8,"type":"sub_header","properties":{"title":[["GPU Basics",[["b"]]]]},"created_time":1645635396507,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cc94a700-c052-4e17-a0f3-1d25db9e4b26":{"role":"reader","value":{"id":"cc94a700-c052-4e17-a0f3-1d25db9e4b26","version":2,"type":"text","properties":{"title":[["This is basically an NVIDIA game, as they are the only provider of good deep learning GPUs. However, Google’s TPUs are the fastest, which is available only on GCP."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"02dde77d-f794-4a46-a7af-9682804eba7e":{"role":"reader","value":{"id":"02dde77d-f794-4a46-a7af-9682804eba7e","version":2,"type":"text","properties":{"title":[["There is a new NVIDIA architecture every year: Kepler -\u003e Pascal -\u003e Volta -\u003e Turing -\u003e Ampere. NVIDIA often released the server version of the cards first, then the “enthusiast” version, and finally the consumer version. If you use these cards for business purposes, then you suppose to use the server version."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"095fb5a4-838a-4cf5-8ece-05fa671e47ad":{"role":"reader","value":{"id":"095fb5a4-838a-4cf5-8ece-05fa671e47ad","version":2,"type":"text","properties":{"title":[["GPUs have a different amount of RAM. You can only compute on the data that is on the GPU memory. "],["The more data you can fit on the GPU, the larger your batches are, the faster your training goes",[["b"]]],["."]]},"created_time":1645635396507,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2904bb5f-0bee-48fc-acd1-4cdcd1e081b3":{"role":"reader","value":{"id":"2904bb5f-0bee-48fc-acd1-4cdcd1e081b3","version":2,"type":"text","properties":{"title":[["For deep learning, you use 32-bit precision. In fact, starting with the Volta architecture, NVIDIA developed "],["tensor cores",[["b"]]],[" that are specifically designed for deep learning operations (mixed-precision between 32 and 16 bit). Tensor Cores reduce the used cycles needed for calculating multiply and addition operations and the reliance on repetitive shared memory access, thus saving additional cycles for memory access. This is very useful for the convolutional/Transformer models that are prevalent nowadays."]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"95473b57-509d-4c83-9d51-3c351f5f3cf4":{"role":"reader","value":{"id":"95473b57-509d-4c83-9d51-3c351f5f3cf4","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling5.png"]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"40a25632-6e7f-4cf3-86bb-26ad6de0d63e":{"role":"reader","value":{"id":"40a25632-6e7f-4cf3-86bb-26ad6de0d63e","version":2,"type":"text","properties":{"title":[["Let’s go through different GPU architectures:"]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f60ba5bd-476e-4c5e-9803-317b25d3d70b":{"role":"reader","value":{"id":"f60ba5bd-476e-4c5e-9803-317b25d3d70b","version":2,"type":"bulleted_list","properties":{"title":[["Kepler",[["b"],["a","https://en.wikipedia.org/wiki/Kepler_(microarchitecture)"]]],["/",[["b"]]],["Maxwell",[["b"],["a","https://en.wikipedia.org/wiki/Maxwell_(microarchitecture)"]]],[": They are 2-4x slower than the Pascal/Volta ones below. You should not buy these old guards (K80)."]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"93bde402-8e67-4734-80e8-3d4768acb9bc":{"role":"reader","value":{"id":"93bde402-8e67-4734-80e8-3d4768acb9bc","version":2,"type":"bulleted_list","properties":{"title":[["Pascal",[["b"],["a","https://www.nvidia.com/en-us/data-center/pascal-gpu-architecture/"]]],[": They are in the 1080 Ti cards from 2017, which are still useful if bought used (especially for recurrent neural networks). P100 is the equivalent cloud offering.",[["b"]]]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6d8e694b-343e-48de-aa48-d14c0d8886a9":{"role":"reader","value":{"id":"6d8e694b-343e-48de-aa48-d14c0d8886a9","version":2,"type":"bulleted_list","properties":{"title":[["Volta",[["b"],["a","https://www.nvidia.com/en-us/data-center/volta-gpu-architecture/"]]],["/",[["b"]]],["Turing",[["b"],["a","https://www.nvidia.com/en-us/geforce/turing/"]]],[": These are the preferred choices over the Kepler and Pascal because of their support for 16-bit mixed-precision via tensor cores. Hardware options are 2080 Ti and Titan RTX, while the cloud option is V100.",[["b"]]]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"96118535-7da4-4a4b-b496-13be43aa72a6":{"role":"reader","value":{"id":"96118535-7da4-4a4b-b496-13be43aa72a6","version":2,"type":"bulleted_list","properties":{"title":[["Ampere",[["b"],["a","https://www.nvidia.com/en-us/data-center/nvidia-ampere-gpu-architecture/"]]],[": This architecture is available in the latest hardware (3090) and cloud (A100) offerings. They have the most tensor cores, leading to at least 30% speedup over Turing.",[["b"]]]]},"created_time":1645635396508,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0a258a4f-5bd6-43b5-a718-eb0809e2eabb":{"role":"reader","value":{"id":"0a258a4f-5bd6-43b5-a718-eb0809e2eabb","version":2,"type":"text","properties":{"title":[["You can check out "],["this recent GPU benchmark",[["b"],["a","https://lambdalabs.com/blog/nvidia-a100-vs-v100-benchmarks/"]]],[" from Lambda Labs and consult Tim Dettmers’ advice on "],["which GPUs to get",[["b"],["a","https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/"]]],["."]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"815dc2a2-d96d-4a17-824e-5218655f7899":{"role":"reader","value":{"id":"815dc2a2-d96d-4a17-824e-5218655f7899","version":8,"type":"sub_header","properties":{"title":[["Cloud Options",[["b"]]]]},"created_time":1645635396509,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"831dfd67-d4f3-4571-8f2a-004e939439d4":{"role":"reader","value":{"id":"831dfd67-d4f3-4571-8f2a-004e939439d4","version":2,"type":"text","properties":{"title":[["Amazon Web Services, Google Cloud Platform, and Microsoft Azure are the cloud heavyweights with largely similar functions and prices. There are also startups like "],["Lambda Labs",[["b"],["a","https://lambdalabs.com/service/gpu-cloud"]]],[" and "],["Corewave",[["b"],["a","https://www.coreweave.com/pricing"]]],[" that provide cloud GPUs."]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fec36a79-41c5-4f53-b05a-5924ebcff651":{"role":"reader","value":{"id":"fec36a79-41c5-4f53-b05a-5924ebcff651","version":8,"type":"sub_header","properties":{"title":[["On-Prem Options",[["b"]]]]},"created_time":1645635396509,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"28c4aee2-72e4-4a79-aea1-0419d85d73b1":{"role":"reader","value":{"id":"28c4aee2-72e4-4a79-aea1-0419d85d73b1","version":2,"type":"text","properties":{"title":[["You can either "],["build your own",[["b"]]],[" or "],["buy pre-built devices",[["b"]]],[" from vendors like Lambda Labs, NVIDIA, Supermicro, Cirrascale, etc."]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a8af587f-1fce-4c58-908c-1896c80296eb":{"role":"reader","value":{"id":"a8af587f-1fce-4c58-908c-1896c80296eb","version":8,"type":"sub_header","properties":{"title":[["Recommendations",[["b"]]]]},"created_time":1645635396509,"last_edited_time":1645635540000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3e180a30-9640-4616-9553-b0b33ac9dfa8":{"role":"reader","value":{"id":"3e180a30-9640-4616-9553-b0b33ac9dfa8","version":2,"type":"text","properties":{"title":[["Even though the cloud is expensive, it’s hard to make on-prem devices scale past a certain point. Furthermore, dev-ops things are easier to be done in the cloud than to be set up by yourself. And if your machine dies or requires maintenance, that will be a constant headache if you are responsible for managing it."]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d5c5b340-fc37-4aa4-8727-2f701cb97091":{"role":"reader","value":{"id":"d5c5b340-fc37-4aa4-8727-2f701cb97091","version":2,"type":"text","properties":{"title":[["Here are our recommendations for three profiles:"]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f7d8bd08-7121-4194-8894-2504a5eaa3a6":{"role":"reader","value":{"id":"f7d8bd08-7121-4194-8894-2504a5eaa3a6","version":2,"type":"bulleted_list","properties":{"title":[["Hobbyists: Build your own machine (maybe a 4x Turing or a 2x Ampere PC) during development. Either use the same PC or use cloud instances during training/evaluation.",[["b"]]]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"71ed635a-c08d-460b-8ce2-aaa67da2ae0c":{"role":"reader","value":{"id":"71ed635a-c08d-460b-8ce2-aaa67da2ae0c","version":2,"type":"bulleted_list","properties":{"title":[["Startups: Buy a sizeable Lambda Labs machine for every ML scientist during development. Buy more shared server machines or use cloud instances during training/evaluation.",[["b"]]]]},"created_time":1645635396509,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5efb26e9-21a4-4ab9-9680-fe36cc8a5715":{"role":"reader","value":{"id":"5efb26e9-21a4-4ab9-9680-fe36cc8a5715","version":2,"type":"bulleted_list","properties":{"title":[["Larger companies: Buy an even more powerful machine for every ML scientist during development. Use cloud with fast instances with proper provisioning and handling of failures during training/evaluation.",[["b"]]]]},"created_time":1645635396510,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7cebeea2-1cfd-49ff-a305-f90994a01165":{"role":"reader","value":{"id":"7cebeea2-1cfd-49ff-a305-f90994a01165","version":31,"type":"header","properties":{"title":[["5 - Resource Management (Kubeflow or Cloud AI)"]]},"created_time":1645635396510,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"28e8405b-8334-404e-86d2-cdf18fc54af9":{"role":"reader","value":{"id":"28e8405b-8334-404e-86d2-cdf18fc54af9","version":2,"type":"text","properties":{"title":[["With all the resources we have discussed (compute, dependencies, etc.), our challenge turns to manage them across the specific use cases we may have. Across all the resources, our goal is always to be able to easily experiment with the necessary resources to achieve the desired application of ML for our product."]]},"created_time":1645635396510,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b7bf4f0f-b8b4-42a9-8b06-e73206675651":{"role":"reader","value":{"id":"b7bf4f0f-b8b4-42a9-8b06-e73206675651","version":2,"type":"text","properties":{"title":[["For this challenge of allocating resources to experimenting users, there are some common solutions:"]]},"created_time":1645635396510,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e9bb1ba4-4c91-4639-b1a5-32fe87eb6b90":{"role":"reader","value":{"id":"e9bb1ba4-4c91-4639-b1a5-32fe87eb6b90","version":2,"type":"numbered_list","properties":{"title":[["Script a solution ourselves",[["b"]]],[": In theory, this is the simplest solution. We can check if a resource is free and then lock it if a particular user is using it or wants to."]]},"created_time":1645635396510,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4dafbbdc-cbdd-4254-a46f-52f67feb4d96":{"role":"reader","value":{"id":"4dafbbdc-cbdd-4254-a46f-52f67feb4d96","version":2,"type":"numbered_list","properties":{"title":[["SLURM",[["b"]]],[": If we don't want to write the script entirely ourselves, standard cluster job schedulers like "],["SLURM",[["b"],["a","https://slurm.schedmd.com/documentation.html"]]],[" can help us. The workflow is as follows: First, a script defines a job’s requirements. Then, the SLURM queue runner analyzes this and then executes the jobs on the correct resource."]]},"created_time":1645635396510,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0fb43ce6-f71a-47da-adc3-b20f0c6c81a0":{"role":"reader","value":{"id":"0fb43ce6-f71a-47da-adc3-b20f0c6c81a0","version":2,"type":"numbered_list","properties":{"title":[["Docker/Kubernetes",[["b"]]],[": The above approach might still be too manual for your needs, in which case you can turn to Docker/Kubernetes. "],["Docker",[["b"],["a","https://www.docker.com/"]]],[" packages the dependency stack into a lighter-than-VM package called a container (that excludes the OS). "],["Kubernetes",[["b"],["a","https://kubernetes.io/"]]],[" lets us run these Docker containers on a cluster. In particular, "],["Kubeflow",[["b"],["a","https://www.kubeflow.org/"]]],[" is an OSS project started by Google that allows you to spawn/manage Jupyter notebooks and manage multi-step workflows. It also has lots of plug-ins for extra processes like hyperparameter tuning and model deployment. However, Kubeflow can be a challenge to setup."]]},"created_time":1645635396511,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a5773ca1-afd7-4727-8a1b-30cda1aa1b38":{"role":"reader","value":{"id":"a5773ca1-afd7-4727-8a1b-30cda1aa1b38","version":2,"type":"numbered_list","properties":{"title":[["Custom ML software",[["b"]]],[": There’s a lot of novel work and all-in-one solutions being developed to provision compute resources for ML development efficiently. Platforms like "],["AWS Sagemaker",[["b"],["a","https://aws.amazon.com/sagemaker/"]]],[", "],["Paperspace Gradient",[["b"],["a","https://gradient.paperspace.com/"]]],[", and "],["Determined AI",[["b"],["a","https://determined.ai/"]]],[" are advancing. Newer startups like "],["Anyscale",[["b"],["a","https://www.anyscale.com/"]]],[" and "],["Grid.AI",[["b"],["a","https://www.grid.ai/"]]],[" (creators of PyTorch Lightning) are also tackling this. Their vision is around allowing you to seamlessly go from training models on your computer to running lots of training jobs in the cloud with a simple set of SDK commands."]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7b8b57b4-851f-4ac9-8449-38425e4eb807":{"role":"reader","value":{"id":"7b8b57b4-851f-4ac9-8449-38425e4eb807","version":13,"type":"header","properties":{"title":[["6 - Frameworks and Distributed Training"]]},"created_time":1645635396513,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"67d9886a-1afb-4692-887c-446477011cc6":{"role":"reader","value":{"id":"67d9886a-1afb-4692-887c-446477011cc6","version":45,"type":"sub_header","properties":{"title":[["Deep Learning Frameworks (pytorch + pytorch lighting)",[["b"]]]]},"created_time":1645635396513,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"bc388637-b675-4714-ad59-3d38c4c744d3":{"role":"reader","value":{"id":"bc388637-b675-4714-ad59-3d38c4c744d3","version":2,"type":"text","properties":{"title":[["If you’ve built a deep learning model in the last few years, you’ve probably used a deep learning framework. Frameworks like TensorFlow have crucially shaped the development of the deep learning revolution. The reality is that deep learning frameworks have existed for a while. Projects like "],["Theano",[["b"],["a","https://pypi.org/project/Theano/"]]],[" and "],["Torch",[["b"],["a","http://torch.ch/"]]],[" have been around for 10+ years. In contemporary use, there are three main frameworks we’ll focus on - "],["TensorFlow",[["b"],["a","https://www.tensorflow.org/"]]],[", "],["Keras",[["b"],["a","https://keras.io/"]]],[", and "],["PyTorch",[["b"],["a","https://pytorch.org/"]]],[". We evaluate frameworks based on their utility for "],["production",[["b"]]],[" and "],["development",[["b"]]],["."]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"83dd171e-4fca-4123-a007-3b0d722a9647":{"role":"reader","value":{"id":"83dd171e-4fca-4123-a007-3b0d722a9647","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling6.png"]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"562a7959-6732-47e2-a6c6-df7d740f14c0":{"role":"reader","value":{"id":"562a7959-6732-47e2-a6c6-df7d740f14c0","version":2,"type":"text","properties":{"title":[["When TensorFlow came out in 2015, it was billed heavily as a production-optimized DL framework with an underlying static optimized graph that could be deployed across compute environments. However, TF 1.0 had a pretty unpleasant development experience; in addition to developing your models, you had to consider the underlying execution graph you were describing. This kind of “meta-development” posed a challenge for newcomers. The Keras project solved many of these issues by offering a simpler way to define models, and eventually became a part of TensorFlow. PyTorch, when it was introduced in 2017, offered a polar opposite to TensorFlow. It made development super easy by consisting almost exclusively of simple Python commands, but was not designed to be fast at scale."]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"28db0d1e-e29f-4ee7-aa2d-ac97c0b70c8f":{"role":"reader","value":{"id":"28db0d1e-e29f-4ee7-aa2d-ac97c0b70c8f","version":2,"type":"text","properties":{"title":[["Using TF/Keras or PyTorch is the current recommended way to build deep learning models unless you have a powerful reason not to. Essentially, both have converged to pretty similar points that balance development and production. TensorFlow adopted eager execution by default and became a lot easier to develop quickly in. PyTorch subsumed Caffe2 and became much faster as a result, specifically by adding the ability to compile speedier model artifacts. Nowadays, PyTorch has a lot of momentum, likely due to its ease of development. Newer projects like "],["fast.ai",[["b"],["a","https://www.fast.ai/"]]],[" and "],["PyTorch Lighting",[["b"],["a","https://www.pytorchlightning.ai/"]]],[" add best practices and additional functionality to PyTorch, making it even more popular. According to "],["this 2018 article on The Gradient",[["b"],["a","https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/"]]],[", more than 80% of submissions are in PyTorch in academic projects."]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"331fc948-2520-4517-8626-83f6ce6cbd5a":{"role":"reader","value":{"id":"331fc948-2520-4517-8626-83f6ce6cbd5a","version":2,"type":"text","properties":{"title":[["All these frameworks may seem like excessive quibbling, especially since PyTorch and TensorFlow have converged in important ways. "],["Why do we even require such extensive frameworks?",[["i"]]]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ea1d7600-ee0f-40ea-9185-4ca5b3517b85":{"role":"reader","value":{"id":"ea1d7600-ee0f-40ea-9185-4ca5b3517b85","version":2,"type":"text","properties":{"title":[["It’s theoretically possible to define entire models and their required matrix math (e.g., a CNN) in NumPy, the classic Python numerical computing library. However, we quickly run into two challenges: back-propagating errors through our model and running the code on GPUs, which are powerful computation accelerators. For these issues to be addressed, we need frameworks to help us with "],["auto-differentiation",[["b"],["a","https://towardsdatascience.com/automatic-differentiation-explained-b4ba8e60c2ad"]]],[", an efficient way of computing the gradients, and "],["software compatibility with GPUs",[["b"]]],[", specifically interfacing with CUDA. Frameworks allow us to abstract the work required to achieve both features, while also layering in valuable abstractions for all the latest layer designs, optimizers, losses, and much more. As you can imagine, the abstractions offered by frameworks save us valuable time on getting our model to run and allow us to focus on optimizing our model."]]},"created_time":1645635396513,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a9e7ac13-3768-4a8c-ba96-c9d93ef75c1e":{"role":"reader","value":{"id":"a9e7ac13-3768-4a8c-ba96-c9d93ef75c1e","version":2,"type":"text","properties":{"title":[["New projects like "],["JAX",[["b"],["a","https://github.com/google/jax"]]],[" and "],["HuggingFace",[["b"],["a","https://huggingface.co/"]]],[" offer different or simpler abstractions. JAX focuses primarily on fast numerical computation with autodiff and GPUs across machine learning use cases (not just deep learning). HuggingFace abstracts entire model architectures in the NLP realm. Instead of loading individual layers, HuggingFace lets you load the entirety of a contemporary mode (along with weights)l like BERT, tremendously speeding up development time. HuggingFace works on both PyTorch and TensorFlow."]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6c146226-09bb-4eb7-9e08-52a46ce52a16":{"role":"reader","value":{"id":"6c146226-09bb-4eb7-9e08-52a46ce52a16","version":8,"type":"sub_header","properties":{"title":[["Distributed Training",[["b"]]]]},"created_time":1645635396514,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c0c711d8-fd8a-45db-9f64-6521fae89617":{"role":"reader","value":{"id":"c0c711d8-fd8a-45db-9f64-6521fae89617","version":2,"type":"text","properties":{"title":[["Distributed training is a hot topic as the datasets and the models we train become too large to work on a single GPU. It’s increasingly a must-do. The important thing to note is that "],["distributed training is a process to conduct a single model training process",[["b"]]],["; don’t confuse it with training multiple models on different GPUs. There are two approaches to distributed training: data parallelism and model parallelism."]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b2c726b2-216d-4b09-a89e-52efa88980bf":{"role":"reader","value":{"id":"b2c726b2-216d-4b09-a89e-52efa88980bf","version":8,"type":"sub_header","properties":{"title":[["DATA PARALLELISM",[["b"]]]]},"created_time":1645635396514,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"92a4f53e-10db-4ce9-95fc-75f167100970":{"role":"reader","value":{"id":"92a4f53e-10db-4ce9-95fc-75f167100970","version":2,"type":"text","properties":{"title":[["Data parallelism is quite simple but powerful. If we have a batch size of X samples, which is too large for one GPU, we can split the X samples evenly across N GPUs. Each GPU calculates the gradients and passes them to a central node (either a GPU or a CPU), where the gradients are averaged and backpropagated through the distributed GPUs. This paradigm generally results in a linear speed-up time (e.g., two distributed GPUs results in a ~2X speed-up in training time). In modern frameworks like PyTorch, PyTorch Lightning, and even in schedulers like SLURM, data-parallel training can be achieved simply by specifying the number of GPUs or calling a data parallelism-enabling object (e.g., "],["torch.nn.DataParallel",[["i"]]],["). Other tools like "],["Horovod",[["b"],["a","https://github.com/horovod/horovod"]]],[" (from Uber) use non-framework-specific ways of enabling data parallelism (e.g., MPI, a standard multiprocessing framework). "],["Ray",[["b"],["a","https://github.com/ray-project/ray"]]],[", the original open-source project from the Anyscale team, was designed to enable general distributed computing applications in Python and can be similarly applied to data-parallel distributed training."]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a2908126-707d-4c3a-bf95-9b77adc91373":{"role":"reader","value":{"id":"a2908126-707d-4c3a-bf95-9b77adc91373","version":8,"type":"sub_header","properties":{"title":[["MODEL PARALLELISM",[["b"]]]]},"created_time":1645635396514,"last_edited_time":1645635600000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b37d8eae-d77b-4a38-b1da-7d52fa7840c7":{"role":"reader","value":{"id":"b37d8eae-d77b-4a38-b1da-7d52fa7840c7","version":2,"type":"text","properties":{"title":[["Model parallelism is a lot more complicated. If you can’t fit your entire model’s weights on a single GPU, you can split the weights across GPUs and pass data through each to train the weights. This usually adds a lot of complexity and should be avoided unless absolutely necessary. A better solution is to pony up for the best GPU available, either locally or in the cloud. You can also use gradient checkpointing, a clever trick wherein you write some gradients to disk as you compute them and load them only as you need them for updates.  New work is coming out to make this easier (e.g., research and framework maturity)."]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8a72e692-00d6-44bc-beb1-014fa192217c":{"role":"reader","value":{"id":"8a72e692-00d6-44bc-beb1-014fa192217c","version":7,"type":"header","properties":{"title":[["7 - Experiment Management"]]},"created_time":1645635396514,"last_edited_time":1645635660000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ed6e3b4c-473a-4322-bfc8-11c55873d9b6":{"role":"reader","value":{"id":"ed6e3b4c-473a-4322-bfc8-11c55873d9b6","version":2,"type":"text","properties":{"title":[["As you run numerous experiments to refine your model, it’s easy to lose track of code, hyperparameters, and artifacts. Model iteration can lead to lots of complexity and messiness. For example, you could be monitoring the learning rate’s impact on your model’s performance metric. "],["With multiple model runs, how will you monitor the impact of the hyperparameter?",[["i"]]]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"579450e0-f515-4715-880a-9ebaf4e75d36":{"role":"reader","value":{"id":"579450e0-f515-4715-880a-9ebaf4e75d36","version":2,"type":"text","properties":{"title":[["A low-tech way would be to manually track the results of all model runs in a spreadsheet. Without great attention to detail, this can quickly spiral into a messy or incomplete artifact. Dedicated experiment management platforms are a remedy to this issue. Let’s cover a few of the most common ones:"]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b2180beb-2488-4914-86f8-e45206221bc8":{"role":"reader","value":{"id":"b2180beb-2488-4914-86f8-e45206221bc8","version":2,"type":"bulleted_list","properties":{"title":[["TensorBoard",[["b"],["a","https://www.tensorflow.org/tensorboard"]]],[": This is the default experiment tracking platform that comes with TensorFlow. As a pro, it’s easy to get started with. On the flip side, it’s not very good for tracking and comparing multiple experiments. It’s also not the best solution to store past work."]]},"created_time":1645635396514,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4329725f-d029-4389-b0f0-fd77be7ea7db":{"role":"reader","value":{"id":"4329725f-d029-4389-b0f0-fd77be7ea7db","version":2,"type":"bulleted_list","properties":{"title":[["MLFlow",[["b"],["a","https://mlflow.org/"]]],[": An OSS project from Databricks, MLFlow is a complete platform for the ML lifecycle. They have great experiment and model run management at the core of their platform. Another open-source project, "],["Keepsake",[["b"],["a","https://keepsake.ai/"]]],[", recently came out focused solely on experiment tracking."]]},"created_time":1645635396515,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"66e08e63-f218-4be2-a84f-c711e3b586c7":{"role":"reader","value":{"id":"66e08e63-f218-4be2-a84f-c711e3b586c7","version":2,"type":"bulleted_list","properties":{"title":[["Paid platforms ("],["Comet.ml",[["b"],["a","https://www.comet.ml/"]]],[", "],["Weights and Biases",[["b"],["a","https://wandb.ai/"]]],[", "],["Neptune",[["b"],["a","https://neptune.ai/"]]],["): Finally, outside vendors offer deep, thorough experiment management platforms, with tools like code diffs, report writing, data visualization, and model registering features. In our labs, we will use Weights and Biases."]]},"created_time":1645635396516,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1b7c1fea-d375-4d9e-a05d-d745988a1176":{"role":"reader","value":{"id":"1b7c1fea-d375-4d9e-a05d-d745988a1176","version":7,"type":"header","properties":{"title":[["8 - Hyperparameter Tuning"]]},"created_time":1645635396516,"last_edited_time":1645635660000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7ad14291-ac67-48ee-b65b-c6b307545cac":{"role":"reader","value":{"id":"7ad14291-ac67-48ee-b65b-c6b307545cac","version":2,"type":"text","properties":{"title":[["To finalize models, we need to ensure that we have the optimal hyperparameters. Since hyperparameter optimization (as this process is called) can be a particularly compute-intensive process, it’s useful to have software that can help. Using specific software can help us kill underperforming model runs with bad hyperparameters early (to save on cost) or help us intelligently sweep ranges of hyperparameter values. Luckily, there’s an increasing number of software providers that do precisely this:"]]},"created_time":1645635396516,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fbd65861-95e0-4e4d-8d2b-7c125351cd35":{"role":"reader","value":{"id":"fbd65861-95e0-4e4d-8d2b-7c125351cd35","version":2,"type":"bulleted_list","properties":{"title":[["SigOpt",[["b"],["a","https://sigopt.com/"]]],[" offers an API focused exclusively on efficient, iterative hyperparameter optimization. Specify a range of values, get SigOpt’s recommended hyperparameter settings, run the model and return the results to SigOpt, and repeat the process until you’ve found the best parameters for your model."]]},"created_time":1645635396516,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"dda0e303-9347-4e2d-a191-f49e1d884bd9":{"role":"reader","value":{"id":"dda0e303-9347-4e2d-a191-f49e1d884bd9","version":2,"type":"bulleted_list","properties":{"title":[["Rather than an API, "],["Ray Tune",[["b"],["a","https://docs.ray.io/en/master/tune/index.html"]]],[" offers a local software (part of the broader Ray ecosystem) that integrates hyperparameter optimization with compute resource allocation. Jobs are scheduled with specific hyperparameters according to state-of-the-art methods, and underperforming jobs are automatically killed."]]},"created_time":1645635396516,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a8b7df78-da3f-434c-b1c9-64de2319fb32":{"role":"reader","value":{"id":"a8b7df78-da3f-434c-b1c9-64de2319fb32","version":2,"type":"bulleted_list","properties":{"title":[["Weights and Biases also has this feature! With a YAML file specification, we can specify a hyperparameter optimization job and perform a “"],["sweep",[["b"],["a","https://wandb.ai/site/sweeps"]]],[",” during which W\u0026B sends parameter settings to individual “agents” (our machines) and compares performance."]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e9b3edab-f467-48e5-83fd-c4eb0a1aa29a":{"role":"reader","value":{"id":"e9b3edab-f467-48e5-83fd-c4eb0a1aa29a","version":9,"type":"header","properties":{"title":[["9 - “All-In-One” Solutions"]]},"created_time":1645635396517,"last_edited_time":1645635660000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"033da8e3-b4f6-4bb1-ae25-74e99dde19f2":{"role":"reader","value":{"id":"033da8e3-b4f6-4bb1-ae25-74e99dde19f2","version":2,"type":"text","properties":{"title":[["Some platforms integrate all the aspects of the applied ML stack we’ve discussed (experiment tracking, optimization, training, etc.) and wrap them into a single experience. To support the “lifecycle,” these platforms typically include:"]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"597e6866-bd7f-47e3-8af3-1cc6acad784d":{"role":"reader","value":{"id":"597e6866-bd7f-47e3-8af3-1cc6acad784d","version":2,"type":"bulleted_list","properties":{"title":[["Labeling and data querying services"]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"73cd8e7d-7269-4a77-aec4-17dc9be9df59":{"role":"reader","value":{"id":"73cd8e7d-7269-4a77-aec4-17dc9be9df59","version":2,"type":"bulleted_list","properties":{"title":[["Model training, especially though job scaling and scheduling"]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"49af8ce5-8aa3-4b21-8b75-3ff58ba7daca":{"role":"reader","value":{"id":"49af8ce5-8aa3-4b21-8b75-3ff58ba7daca","version":2,"type":"bulleted_list","properties":{"title":[["Experiment tracking and model versioning"]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f0b0cdd5-0958-4657-ae73-ba6b273887c4":{"role":"reader","value":{"id":"f0b0cdd5-0958-4657-ae73-ba6b273887c4","version":2,"type":"bulleted_list","properties":{"title":[["Development environments, typically through notebook-style interfaces"]]},"created_time":1645635396517,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"eaebbec4-3d5e-487b-a72a-056bb875d72e":{"role":"reader","value":{"id":"eaebbec4-3d5e-487b-a72a-056bb875d72e","version":2,"type":"bulleted_list","properties":{"title":[["Model deployment (e.g., via REST APIs) and monitoring"]]},"created_time":1645635396518,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"14364596-84a2-4c19-8a8b-2cbacfe0e470":{"role":"reader","value":{"id":"14364596-84a2-4c19-8a8b-2cbacfe0e470","version":2,"type":"text","properties":{"title":[["One of the earliest examples of such a system is Facebook’s "],["FBLearner",[["b"],["a","https://engineering.fb.com/2016/05/09/core-data/introducing-fblearner-flow-facebook-s-ai-backbone/"]]],[" (2016), which encompassed data and feature storage, training, inference, and continuous learning based on user interactions with the model’s outputs. You can imagine how powerful having one hub for all this activity can be for ML application and development speed. As a result, cloud vendors (Google, AWS, Azure) have developed similar all-in-one platforms, like "],["Google Cloud AI Platform",[["b"],["a","https://cloud.google.com/ai-platform"]]],[" and "],["AWS SageMaker",[["b"],["a","https://aws.amazon.com/sagemaker/"]]],[". Startups like "],["Paperspace Gradient",[["b"],["a","https://gradient.paperspace.com/"]]],[", "],["Neptune",[["b"],["a","https://neptune.ai/"]]],[", and "],["FloydHub",[["b"],["a","https://www.floydhub.com/"]]],[" also offer all-in-one platforms focused on deep learning. "],["Determined AI",[["b"],["a","https://github.com/determined-ai/determined"]]],[", which focuses exclusively on the model development and training part of the lifecycle, is the rare open-source platform in this space. "],["Domino Data Lab",[["b"],["a","https://www.dominodatalab.com/"]]],[" is a traditional ML-focused startup with an extensive feature set worth looking at. It’s natural to expect more MLOps (as this kind of tooling and infra is referred to) companies and vendors to build out their feature set and become platform-oriented; Weights and Biases is a good example of this."]]},"created_time":1645635396518,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"59dfd15f-e148-4880-8870-ba1907ae0781":{"role":"reader","value":{"id":"59dfd15f-e148-4880-8870-ba1907ae0781","version":2,"type":"text","properties":{"title":[["In conclusion, take a look at the below table to compare a select number of MLOps platform vendors. Pricing is quite variable."]]},"created_time":1645635396518,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d922db2d-a59e-4508-8058-d2e95c09f301":{"role":"reader","value":{"id":"d922db2d-a59e-4508-8058-d2e95c09f301","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-6-notes-media/Infra-Tooling7.png"]]},"created_time":1645635396518,"last_edited_time":1645635360000,"parent_id":"e9d171d8-17bc-4aa6-88f7-79391cf072d1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}}},"space":{},"discussion":{},"comment":{},"collection":{},"collection_view":{},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{}},"pageId":"e9d171d8-17bc-4aa6-88f7-79391cf072d1"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"mlops-infra-tooling"},"buildId":"fIyH02Gfo9GcHzi1tLPop","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>