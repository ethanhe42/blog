<!DOCTYPE html><html lang="en"><head><link rel="shortcut icon" href="/favicon.png"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/manifest.json"/><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="theme-color" content="#EB625A"/><meta property="og:type" content="website"/><script async="" src="https://www.googletagmanager.com/gtag/js?id=G-Y4TMBBH2RZ"></script><script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());
              gtag('config', 'G-Y4TMBBH2RZ', {
                page_path: window.location.pathname,
              });
          </script><meta property="og:title" content="Ultrasound Nerve Segmentation with TensorFlow"/><meta property="og:site_name" content="Yihui He"/><meta name="twitter:title" content="Ultrasound Nerve Segmentation with TensorFlow"/><meta property="twitter:domain" content="yihui-he.github.io"/><meta name="twitter:creator" content="@he_yi_hui"/><meta name="description" content="Yihui He, AI research scientist / full stack engineer"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:card" content="summary"/><link rel="canonical" href="https://yihui-he.github.io/ultrasound-nerve-segmentation-with-tensorflow"/><meta property="og:url" content="https://yihui-he.github.io/ultrasound-nerve-segmentation-with-tensorflow"/><meta property="twitter:url" content="https://yihui-he.github.io/ultrasound-nerve-segmentation-with-tensorflow"/><title>Ultrasound Nerve Segmentation with TensorFlow</title><meta name="next-head-count" content="21"/><link rel="preload" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" data-n-g=""/><link rel="preload" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/blog/_next/static/chunks/webpack-ed2d627650adadaab635.js" defer=""></script><script src="/blog/_next/static/chunks/framework-c93ed74a065331c4bd75.js" defer=""></script><script src="/blog/_next/static/chunks/main-0989120ac94443065aa9.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-e24568358f9b2ca9ee76.js" defer=""></script><script src="/blog/_next/static/chunks/1bfc9850-762e4e08544c8bec659c.js" defer=""></script><script src="/blog/_next/static/chunks/ae51ba48-7f31b5cf321fe3268476.js" defer=""></script><script src="/blog/_next/static/chunks/d7eeaac4-07e2c37279f27c6f41bc.js" defer=""></script><script src="/blog/_next/static/chunks/808-8e92cc1e8e3a13c0ecab.js" defer=""></script><script src="/blog/_next/static/chunks/270-4b8341b0373202f07d27.js" defer=""></script><script src="/blog/_next/static/chunks/pages/%5BpageId%5D-845d25c09c3d16569292.js" defer=""></script><script src="/blog/_next/static/sJmC7l3paKmXPUP3FZfmF/_buildManifest.js" defer=""></script><script src="/blog/_next/static/sJmC7l3paKmXPUP3FZfmF/_ssgManifest.js" defer=""></script></head><body><script src="noflash.js"></script><div id="__next"><div class="notion notion-app light-mode notion-block-e7a12f56b48c43bf803e96d70518685f"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/blog"><img class="icon notion-page-icon" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F63e89fe9-f5cd-4414-be0e-53d91aa54fc3%2Fme_small.jpg?table=block&amp;id=bd32b787-e471-49f3-8941-174a9c6846b6&amp;cache=v2" alt="Yihui He’s Blog" loading="lazy"/><span class="title">Yihui He’s Blog</span></a><span class="spacer">/</span><div class="breadcrumb active"><span class="title">Ultrasound Nerve Segmentation with TensorFlow</span></div></div><div class="rhs"></div></div></header><div class="notion-page-scroller"><main class="notion-page notion-page-no-cover notion-page-no-icon notion-page-has-text-icon notion-full-page"><h1 class="notion-title">Ultrasound Nerve Segmentation with TensorFlow</h1><div class="notion-page-content notion-page-content-has-aside notion-page-content-has-toc"><article class="notion-page-content-inner"><div class="notion-row"><a target="_blank" rel="noopener noreferrer" class="notion-bookmark notion-block-e1b9cf1aa06b4aa5b5df4162ef02cf44" href="https://github.com/yihui-he/u-net"><div><div class="notion-bookmark-title">GitHub - yihui-he/u-net: U-Net: Convolutional Networks for Biomedical Image Segmentation</div><div class="notion-bookmark-description">You can&#x27;t perform that action at this time. You signed in with another tab or window. You signed out in another tab or window. Reload to refresh your session. Reload to refresh your session.</div><div class="notion-bookmark-link"><img src="https://github.com/favicon.ico" alt="GitHub - yihui-he/u-net: U-Net: Convolutional Networks for Biomedical Image Segmentation" loading="lazy"/><div>https://github.com/yihui-he/u-net</div></div></div><div class="notion-bookmark-image"><img src="https://opengraph.githubassets.com/f6cf8b6e46f55fd0e2e077c91f82e78ecc7e2f3735e29d38263b695d8505977d/yihui-he/u-net" alt="GitHub - yihui-he/u-net: U-Net: Convolutional Networks for Biomedical Image Segmentation" loading="lazy"/></div></a></div><div class="notion-text notion-block-215376a772c6416490773bd7b10c9045"><b>Deep Learning Tutorial for Kaggle Ultrasound Nerve Segmentation competition, using Keras</b></div><div class="notion-text notion-block-44e153544aa349c2a5d3881611437f4a">This tutorial shows how to use <a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://keras.io/">Keras library</a> to build deep neural network for ultrasound image nerve segmentation. More info on this Kaggle competition can be found on <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kaggle.com/c/ultrasound-nerve-segmentation">https://www.kaggle.com/c/ultrasound-nerve-segmentation</a>.</div><div class="notion-text notion-block-2311aa37a8f94237956a2e8335bf519c">This deep neural network achieves <b>~0.57 score on the leaderboard</b> based on test images, and can be a good staring point for further, more serious approaches.</div><div class="notion-text notion-block-0b713d06db914cdea115e4193148f397">The architecture was inspired by <a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>.</div><hr class="notion-hr notion-block-74b7f93bda9f4980b27523a119ce850a"/><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-b8f27689262447d5a95d7ab5bf469d29" data-id="b8f27689262447d5a95d7ab5bf469d29"><span><div id="b8f27689262447d5a95d7ab5bf469d29" class="notion-header-anchor"></div><a class="notion-hash-link" href="#b8f27689262447d5a95d7ab5bf469d29" title="Overview"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Overview</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-2d6504b8030c47ea89595f564d27f5c7" data-id="2d6504b8030c47ea89595f564d27f5c7"><span><div id="2d6504b8030c47ea89595f564d27f5c7" class="notion-header-anchor"></div><a class="notion-hash-link" href="#2d6504b8030c47ea89595f564d27f5c7" title="Data"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Data</span></span></h4><div class="notion-text notion-block-d0dd1d233d7042f0b453223ceb775a35"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/data">Provided data</a> is processed by <code class="notion-inline-code">data.py</code> script. This script just loads the images and saves them into NumPy binary format files <b>.npy</b> for faster loading later.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-037d51eab6314c3daab4fe4650189864" data-id="037d51eab6314c3daab4fe4650189864"><span><div id="037d51eab6314c3daab4fe4650189864" class="notion-header-anchor"></div><a class="notion-hash-link" href="#037d51eab6314c3daab4fe4650189864" title="Pre-processing"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Pre-processing</span></span></h4><div class="notion-text notion-block-5a799cc0f27f4708b493923f02acd6c8">The images are not pre-processed in any way, except resizing to 64 x 80. Since the images are pretty noisy, I expect that some thoughtful pre-processing could yield better performance of the model.</div><div class="notion-text notion-block-9a87a66a4b984cf3855a33653978854b">Output images (masks) are scaled to [0, 1] interval.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-e801072984e44fed827623a1dafe047f" data-id="e801072984e44fed827623a1dafe047f"><span><div id="e801072984e44fed827623a1dafe047f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e801072984e44fed827623a1dafe047f" title="Model"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Model</span></span></h4><div class="notion-text notion-block-9ee76d8cf661420491e3087774858fc7">The provided model is basically a convolutional auto-encoder, but with a twist - it has skip connections from encoder layers to decoder layers that are on the same “level”. See picture below (note that image size and numbers of convolutional filters in this tutorial differs from the original U-Net architecture).</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-b3262fcf0fb74795b791560cff6e5290"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:624px;max-width:100%;flex-direction:column"><img style="object-fit:contain" src="https://www.notion.so/image/https%3A%2F%2Fraw.githubusercontent.com%2Fyihui-he%2Fu-net%2Fmaster%2Fimg%2Fu-net-architecture.png?table=block&amp;id=b3262fcf-0fb7-4795-b791-560cff6e5290&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-3025088b858c4cf3a500edb2d23d6129">This deep neural network is implemented with Keras functional API, which makes it extremely easy to experiment with different interesting architectures.</div><div class="notion-text notion-block-2f03046dcf8e4fa7817dd8a271dd6b5d">Output from the network is a 64 x 80 which represents mask that should be learned. Sigmoid activation function makes sure that mask pixels are in [0, 1] range.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-8733a735bc6e4d6c958b3defa8849512" data-id="8733a735bc6e4d6c958b3defa8849512"><span><div id="8733a735bc6e4d6c958b3defa8849512" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8733a735bc6e4d6c958b3defa8849512" title="Training"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Training</span></span></h4><div class="notion-text notion-block-7a2d7b2afe6e4763adb2bb0d9587c3a5">The model is trained for 20 epochs, where each epoch took ~30 seconds on Titan X. Memory footprint of the model is ~800MB.</div><div class="notion-text notion-block-522eb28df78f40afa68c93d2c7590774">After 20 epochs, calculated Dice coefficient is ~0.68, which yielded ~0.57 score on leaderboard, so obviously this model overfits (cross-validation pull requests anyone? ;)).</div><div class="notion-text notion-block-4e5fd49a6ebc47f6a3f969a83787a6f0">Loss function for the training is basically just a <b>negative of Dice coefficient</b> (which is used as <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/details/evaluation">evaluation metric on the competition</a>), and this is implemented as custom loss function using Keras backend - check <code class="notion-inline-code">dice_coef()</code> and <code class="notion-inline-code">dice_coef_loss()</code> functions in <code class="notion-inline-code">train.py</code> for more detail. Also, for making the loss function smooth, a factor <code class="notion-inline-code">smooth = 1</code> factor is added.</div><div class="notion-text notion-block-262e3a9ac0db4fd49882f40ea4b57b15">The weights are updated by Adam optimizer, with a 1e-5 learning rate. During training, model’s weights are saved in HDF5 format.</div><hr class="notion-hr notion-block-f05055165b09425c937df40feb06d0de"/><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-e9d860e8b8104cb2afdcb8c5e5e5c7be" data-id="e9d860e8b8104cb2afdcb8c5e5e5c7be"><span><div id="e9d860e8b8104cb2afdcb8c5e5e5c7be" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e9d860e8b8104cb2afdcb8c5e5e5c7be" title="How to use"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">How to use</span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-8a26f2e6d7cf43e5bdbd25e64e8bb6de" data-id="8a26f2e6d7cf43e5bdbd25e64e8bb6de"><span><div id="8a26f2e6d7cf43e5bdbd25e64e8bb6de" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8a26f2e6d7cf43e5bdbd25e64e8bb6de" title="Dependencies"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Dependencies</span></span></h4><div class="notion-text notion-block-942ceb589d0f458a94fbd5b90312c6c1">This tutorial depends on the following libraries:</div><ul class="notion-list notion-list-disc notion-block-11aa751dc3b141ab8e07d5ccbbed0e9b"><li>cv2 (OpenCV)</li></ul><ul class="notion-list notion-list-disc notion-block-c7e46795147f485b815f0908f0e2b52c"><li>Theano and/or Tensorflow</li></ul><ul class="notion-list notion-list-disc notion-block-b56a02241ad342ea9a055eb9156a3e47"><li>Keras &gt;= 1.0</li></ul><div class="notion-text notion-block-29ed9f6b95444beaacd9a40e1f38d669">Also, this code should be compatible with Python versions 2.7-3.5.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-0a475b3e5d454d0eb3ed846f4ffdbdca" data-id="0a475b3e5d454d0eb3ed846f4ffdbdca"><span><div id="0a475b3e5d454d0eb3ed846f4ffdbdca" class="notion-header-anchor"></div><a class="notion-hash-link" href="#0a475b3e5d454d0eb3ed846f4ffdbdca" title="Prepare the data"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Prepare the data</span></span></h4><div class="notion-text notion-block-d108dc19e0654541884cdf1a1970cbe8">In order to extract raw images and save them to <em>.npy</em> files, you should first prepare its structure. Make sure that <code class="notion-inline-code">raw</code> dir is located in the root of this project. Also, the tree of <code class="notion-inline-code">raw</code> dir must be like:</div><pre class="notion-code language-plain text"><code class="language-plain text"><span class="token operator">-</span>raw
 <span class="token operator">|</span>
 <span class="token operator">--</span><span class="token operator">--</span> train
 <span class="token operator">|</span>    <span class="token operator">|</span>
 <span class="token operator">|</span>    <span class="token operator">--</span><span class="token operator">--</span> <span class="token number">1_1.</span>tif
 <span class="token operator">|</span>    <span class="token operator">|</span>
 <span class="token operator">|</span>    <span class="token operator">--</span><span class="token operator">--</span> …
 <span class="token operator">|</span>
 <span class="token operator">--</span><span class="token operator">--</span> test
      <span class="token operator">|</span>
      <span class="token operator">--</span><span class="token operator">--</span> <span class="token number">1.</span>tif
      <span class="token operator">|</span>
      <span class="token operator">--</span><span class="token operator">--</span> …</code></pre><ul class="notion-list notion-list-disc notion-block-7352554b6c364c8696a3b210f348aeaf"><li>Now run <code class="notion-inline-code">python data.py</code>.</li></ul><div class="notion-text notion-block-3df16b5ff3c446d7a189703196080070">Running this script will create train and test images and save them to <b>.npy</b> files.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-33d95177f5224a3f92590dfbe06df2a3" data-id="33d95177f5224a3f92590dfbe06df2a3"><span><div id="33d95177f5224a3f92590dfbe06df2a3" class="notion-header-anchor"></div><a class="notion-hash-link" href="#33d95177f5224a3f92590dfbe06df2a3" title="Define the model"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Define the model</span></span></h4><ul class="notion-list notion-list-disc notion-block-6bc5c7f5ca2a4ba9b91a8561156036b5"><li>Check out <code class="notion-inline-code">get_unet()</code> in <code class="notion-inline-code">train.py</code> to modify the model, optimizer and loss function.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-84c3cad62e0b4bc9a86764b5cabe4ace" data-id="84c3cad62e0b4bc9a86764b5cabe4ace"><span><div id="84c3cad62e0b4bc9a86764b5cabe4ace" class="notion-header-anchor"></div><a class="notion-hash-link" href="#84c3cad62e0b4bc9a86764b5cabe4ace" title="Train the model and generate masks for test images"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Train the model and generate masks for test images</span></span></h4><ul class="notion-list notion-list-disc notion-block-99bc4547468c497e9593e6b2670167b5"><li>Run <code class="notion-inline-code">python train.py</code> to train the model.</li></ul><div class="notion-text notion-block-1f5053936a9b469eb750a3e557fc7dc3">Check out <code class="notion-inline-code">train_predict()</code> to modify the number of iterations (epochs), batch size, etc.</div><div class="notion-text notion-block-e4ebbf435fc84b919706ecf6e8342621">After this script finishes, in <code class="notion-inline-code">imgs_mask_test.npy</code> masks for corresponding images in <code class="notion-inline-code">imgs_test.npy</code> should be generated. I suggest you examine these masks for getting further insight of your model’s performance.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-be9457fc03c0495198075ae133a00ceb" data-id="be9457fc03c0495198075ae133a00ceb"><span><div id="be9457fc03c0495198075ae133a00ceb" class="notion-header-anchor"></div><a class="notion-hash-link" href="#be9457fc03c0495198075ae133a00ceb" title="Generate submission"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">Generate submission</span></span></h4><ul class="notion-list notion-list-disc notion-block-be7b34677edb4c87a4c3a5f0867a11f8"><li>Run <code class="notion-inline-code">python submission.py</code> to generate the submission file <code class="notion-inline-code">submission.csv</code> for the generated masks.</li></ul><div class="notion-text notion-block-cc8c7eefef2741b08cab34f9dd9e642b">Check out function <code class="notion-inline-code">submission()</code> and <code class="notion-inline-code">run_length_enc()</code> (thanks woshialex) for details.</div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-5f9e6a02cec142f090760b8563338850" data-id="5f9e6a02cec142f090760b8563338850"><span><div id="5f9e6a02cec142f090760b8563338850" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5f9e6a02cec142f090760b8563338850" title="About Keras"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">About Keras</span></span></h3><div class="notion-text notion-block-2ce77179be6c4434b1fd31bd365cda71">Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.</div><div class="notion-text notion-block-91af229409844f768ac7dfe621a03ed4">Use Keras if you need a deep learning library that:</div><div class="notion-text notion-block-b445ca86fee444cfbe061bdc5faaee74">allows for easy and fast prototyping (through total modularity, minimalism, and extensibility). supports both convolutional networks and recurrent networks, as well as combinations of the two. supports arbitrary connectivity schemes (including multi-input and multi-output training). runs seamlessly on CPU and GPU. Read the documentation <a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://keras.io/">Keras.io</a></div><div class="notion-text notion-block-687759b583d844b6bad764137e560f91">Keras is compatible with: Python 2.7-3.5.</div></article><aside class="notion-aside"><div class="notion-aside-table-of-contents"><div class="notion-aside-table-of-contents-header">Table of Contents</div><nav class="notion-table-of-contents notion-gray"><a href="#b8f27689262447d5a95d7ab5bf469d29" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">Overview</span></a><a href="#2d6504b8030c47ea89595f564d27f5c7" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Data</span></a><a href="#037d51eab6314c3daab4fe4650189864" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Pre-processing</span></a><a href="#e801072984e44fed827623a1dafe047f" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Model</span></a><a href="#8733a735bc6e4d6c958b3defa8849512" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Training</span></a><a href="#e9d860e8b8104cb2afdcb8c5e5e5c7be" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">How to use</span></a><a href="#8a26f2e6d7cf43e5bdbd25e64e8bb6de" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Dependencies</span></a><a href="#0a475b3e5d454d0eb3ed846f4ffdbdca" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Prepare the data</span></a><a href="#33d95177f5224a3f92590dfbe06df2a3" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Define the model</span></a><a href="#84c3cad62e0b4bc9a86764b5cabe4ace" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Train the model and generate masks for test images</span></a><a href="#be9457fc03c0495198075ae133a00ceb" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">Generate submission</span></a><a href="#5f9e6a02cec142f090760b8563338850" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">About Keras</span></a></nav></div><div class="PageSocial_pageSocial__2WqHl"><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://yihui-he.github.io" title="personal website" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="#000000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_github__slQ0z" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_linkedin__nElHT" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"></path></svg></div></a></div></aside></div></main><footer class="styles_footer__1r_c6"><div class="styles_copyright__3kWHj">Copyright 2022 <!-- -->Yihui He</div><div class="styles_social__235gY"><a class="styles_twitter__WwfaA" href="https://yihui-he.github.io" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></a><a class="styles_twitter__WwfaA" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a class="styles_github__32xIr" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__1XGvB" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"yihui-he.github.io","name":"Yihui He","rootNotionPageId":"bd32b787e47149f38941174a9c6846b6","rootNotionSpaceId":null,"description":"Yihui He, AI research scientist / full stack engineer"},"recordMap":{"block":{"e7a12f56-b48c-43bf-803e-96d70518685f":{"role":"reader","value":{"id":"e7a12f56-b48c-43bf-803e-96d70518685f","version":43,"type":"page","properties":{"title":[["Ultrasound Nerve Segmentation with TensorFlow"]]},"content":["e1b9cf1a-a06b-4aa5-b5df-4162ef02cf44","215376a7-72c6-4164-9077-3bd7b10c9045","44e15354-4aa3-49c2-a5d3-881611437f4a","2311aa37-a8f9-4237-956a-2e8335bf519c","0b713d06-db91-4cde-a115-e4193148f397","74b7f93b-da9f-4980-b275-23a119ce850a","b8f27689-2624-47d5-a95d-7ab5bf469d29","2d6504b8-030c-47ea-8959-5f564d27f5c7","d0dd1d23-3d70-42f0-b453-223ceb775a35","037d51ea-b631-4c3d-aab4-fe4650189864","5a799cc0-f27f-4708-b493-923f02acd6c8","9a87a66a-4b98-4cf3-855a-33653978854b","e8010729-84e4-4fed-8276-23a1dafe047f","9ee76d8c-f661-4204-91e3-087774858fc7","b3262fcf-0fb7-4795-b791-560cff6e5290","3025088b-858c-4cf3-a500-edb2d23d6129","2f03046d-cf8e-4fa7-817d-d8a271dd6b5d","8733a735-bc6e-4d6c-958b-3defa8849512","7a2d7b2a-fe6e-4763-adb2-bb0d9587c3a5","522eb28d-f78f-40af-a68c-93d2c7590774","4e5fd49a-6ebc-47f6-a3f9-69a83787a6f0","262e3a9a-c0db-4fd4-9882-f40ea4b57b15","f0505516-5b09-425c-937d-f40feb06d0de","e9d860e8-b810-4cb2-afdc-b8c5e5e5c7be","8a26f2e6-d7cf-43e5-bdbd-25e64e8bb6de","942ceb58-9d0f-458a-94fb-d5b90312c6c1","11aa751d-c3b1-41ab-8e07-d5ccbbed0e9b","c7e46795-147f-485b-815f-0908f0e2b52c","b56a0224-1ad3-42ea-9a05-5eb9156a3e47","29ed9f6b-9544-4bea-acd9-a40e1f38d669","0a475b3e-5d45-4d0e-b3ed-846f4ffdbdca","d108dc19-e065-4541-884c-df1a1970cbe8","33a3028d-6ad3-4541-8e38-ad7bdf89aac8","7352554b-6c36-4c86-96a3-b210f348aeaf","3df16b5f-f3c4-46d7-a189-703196080070","33d95177-f522-4a3f-9259-0dfbe06df2a3","6bc5c7f5-ca2a-4ba9-b91a-8561156036b5","84c3cad6-2e0b-4bc9-a867-64b5cabe4ace","99bc4547-468c-497e-9593-e6b2670167b5","1f505393-6a9b-469e-b750-a3e557fc7dc3","e4ebbf43-5fc8-4b91-9706-ecf6e8342621","be9457fc-03c0-4951-9807-5ae133a00ceb","be7b3467-7edb-4c87-a4c3-a5f0867a11f8","cc8c7eef-ef27-41b0-8cab-34f9dd9e642b","5f9e6a02-cec1-42f0-9076-0b8563338850","2ce77179-be6c-4434-b1fd-31bd365cda71","91af2294-0984-4f76-8ac7-dfe621a03ed4","b445ca86-fee4-44cf-be06-1bdc5faaee74","687759b5-83d8-44b6-bad7-64137e560f91"],"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"}],"created_time":1646011980424,"last_edited_time":1646015280000,"parent_id":"bd32b787-e471-49f3-8941-174a9c6846b6","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"bd32b787-e471-49f3-8941-174a9c6846b6":{"role":"reader","value":{"id":"bd32b787-e471-49f3-8941-174a9c6846b6","version":306,"type":"page","properties":{"title":[["Yihui He’s Blog"]]},"content":["cc62526f-e4eb-42e5-8e8f-a22c3abd7b46","eb534ff0-d0e6-4c83-b817-92c761ad9034","9229117d-ca53-4eee-b43b-a1ecf09cccdf","07cdf7d1-24c9-4fd7-abc0-e1f79f38aafd","41249f3f-76a2-458a-b13d-2bc06310337b","09ac442d-d3b5-4f43-9531-90cdae7640b7","76f263cc-0736-4657-91dd-551b1c541b4c","fad54c70-af5c-42a3-a00a-ff18bea9c382","9abe926f-8b36-4a60-a77e-e5436690a8c0","84c674a7-bbe3-4ac5-88c7-68e0c3909585","09517098-a843-437e-bb7a-4ac67866b29b","e7a12f56-b48c-43bf-803e-96d70518685f","932f5707-ab83-4542-af11-68fec7b5f28d","e6311219-0ef8-4142-8a3f-eeedc910dd3b","5f16dda4-316e-43af-a4ea-8fc6d6938404","23859d82-2201-4bb6-a86a-db693ea4992a","d5a94597-8cb4-43be-a6d0-0840b5518710","c54caf49-d28c-46ac-b72d-fc9bfa58bbdd","4751172a-1cb9-4633-84d6-21ba334ee3d7","8e23ed11-04f5-48ab-a95d-1f47a5cd22af","4b2fb8ba-1e4c-47ff-903b-0869360db746","010faf71-b9e5-4c47-832a-658dafc6e7ae","3db8e8fd-7383-4aea-8beb-608282db050e","60ff1c6d-fde5-426d-ac57-7bba2a5296a2","31008559-3d6f-4c33-bb50-4e463c9b19fb","201efca9-ba21-47c3-8b0d-35b41dd3f91f","04ced6eb-ca82-46db-8f8e-077f3bf6b29c","daf31581-086b-4fb1-9695-5a79117d4300","a0487f2c-eed0-4154-a6e8-2366ca1d7939","95728d67-dc1f-4086-881b-1cd1bf72f985","cba2b20a-aa13-466c-bcb6-a9f9c9f057da","5e0b979d-ef00-44a3-894f-38d28cfd7576","38a364d6-720a-4788-aeee-c2b224745860","6a838606-233d-4584-b2f1-3d198b1603e8","f93f1e88-d826-4cd3-ab2a-cd92ddd2ac53","e8156bf2-5303-4590-a44a-6db1404c8c7b","2411768d-af34-4f88-8772-c46ca4428256","52357a78-3241-4a0a-bcb7-46bfd15070a4","3f4c64fb-75dc-4ccd-9e22-ce5704d0dfdb","d1ef1d4d-bfa8-40c4-a78c-037da96f3681","8d0bc47c-6aaf-4ac2-ae09-a6830f9ad50c","8f822963-88c4-4e82-8eac-8d0f8b606895","7c5f4cc9-b8a0-498e-8484-ccc1dbf41912","0ef9110d-6766-4f69-9412-586d40d755a9"],"discussions":["a13be18e-5008-4f8c-b07a-0fadb99d7450"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63e89fe9-f5cd-4414-be0e-53d91aa54fc3/me_small.jpg"},"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"},{"role":"reader","type":"public_permission","added_timestamp":1645466303668}],"created_time":1645422720000,"last_edited_time":1665526260000,"parent_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1","parent_table":"space","alive":true,"file_ids":["e3d85759-49f2-40d2-ba30-d25ca401872e","cfc564cc-91ae-4a96-a912-fcc9a28ec5db","63e89fe9-f5cd-4414-be0e-53d91aa54fc3"],"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e1b9cf1a-a06b-4aa5-b5df-4162ef02cf44":{"role":"reader","value":{"id":"e1b9cf1a-a06b-4aa5-b5df-4162ef02cf44","version":8,"type":"bookmark","properties":{"link":[["https://github.com/yihui-he/u-net"]],"title":[["GitHub - yihui-he/u-net: U-Net: Convolutional Networks for Biomedical Image Segmentation"]],"description":[["You can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or window. Reload to refresh your session. Reload to refresh your session."]]},"format":{"bookmark_icon":"https://github.com/favicon.ico","bookmark_cover":"https://opengraph.githubassets.com/f6cf8b6e46f55fd0e2e077c91f82e78ecc7e2f3735e29d38263b695d8505977d/yihui-he/u-net"},"created_time":1646012420241,"last_edited_by":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_time":1646012400000,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"215376a7-72c6-4164-9077-3bd7b10c9045":{"role":"reader","value":{"id":"215376a7-72c6-4164-9077-3bd7b10c9045","version":6,"type":"text","properties":{"title":[["Deep Learning Tutorial for Kaggle Ultrasound Nerve Segmentation competition, using Keras",[["b"]]]]},"created_time":1646011980383,"last_edited_time":1646015280000,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"44e15354-4aa3-49c2-a5d3-881611437f4a":{"role":"reader","value":{"id":"44e15354-4aa3-49c2-a5d3-881611437f4a","version":1,"type":"text","properties":{"title":[["This tutorial shows how to use "],["Keras library",[["a","http://keras.io/"]]],[" to build deep neural network for ultrasound image nerve segmentation. More info on this Kaggle competition can be found on "],["https://www.kaggle.com/c/ultrasound-nerve-segmentation",[["a","https://www.kaggle.com/c/ultrasound-nerve-segmentation"]]],["."]]},"created_time":1646011980396,"last_edited_time":1646011980396,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2311aa37-a8f9-4237-956a-2e8335bf519c":{"role":"reader","value":{"id":"2311aa37-a8f9-4237-956a-2e8335bf519c","version":1,"type":"text","properties":{"title":[["This deep neural network achieves "],["~0.57 score on the leaderboard",[["b"]]],[" based on test images, and can be a good staring point for further, more serious approaches."]]},"created_time":1646011980397,"last_edited_time":1646011980397,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0b713d06-db91-4cde-a115-e4193148f397":{"role":"reader","value":{"id":"0b713d06-db91-4cde-a115-e4193148f397","version":1,"type":"text","properties":{"title":[["The architecture was inspired by "],["U-Net: Convolutional Networks for Biomedical Image Segmentation",[["a","http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/"]]],["."]]},"created_time":1646011980398,"last_edited_time":1646011980398,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"74b7f93b-da9f-4980-b275-23a119ce850a":{"role":"reader","value":{"id":"74b7f93b-da9f-4980-b275-23a119ce850a","version":1,"type":"divider","created_time":1646011980399,"last_edited_time":1646011980399,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b8f27689-2624-47d5-a95d-7ab5bf469d29":{"role":"reader","value":{"id":"b8f27689-2624-47d5-a95d-7ab5bf469d29","version":1,"type":"sub_header","properties":{"title":[["Overview"]]},"created_time":1646011980399,"last_edited_time":1646011980399,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2d6504b8-030c-47ea-8959-5f564d27f5c7":{"role":"reader","value":{"id":"2d6504b8-030c-47ea-8959-5f564d27f5c7","version":1,"type":"sub_sub_header","properties":{"title":[["Data"]]},"created_time":1646011980399,"last_edited_time":1646011980399,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d0dd1d23-3d70-42f0-b453-223ceb775a35":{"role":"reader","value":{"id":"d0dd1d23-3d70-42f0-b453-223ceb775a35","version":1,"type":"text","properties":{"title":[["Provided data",[["a","https://www.kaggle.com/c/ultrasound-nerve-segmentation/data"]]],[" is processed by "],["data.py",[["c"]]],[" script. This script just loads the images and saves them into NumPy binary format files "],[".npy",[["b"]]],[" for faster loading later."]]},"created_time":1646011980401,"last_edited_time":1646011980401,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"037d51ea-b631-4c3d-aab4-fe4650189864":{"role":"reader","value":{"id":"037d51ea-b631-4c3d-aab4-fe4650189864","version":1,"type":"sub_sub_header","properties":{"title":[["Pre-processing"]]},"created_time":1646011980401,"last_edited_time":1646011980401,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5a799cc0-f27f-4708-b493-923f02acd6c8":{"role":"reader","value":{"id":"5a799cc0-f27f-4708-b493-923f02acd6c8","version":1,"type":"text","properties":{"title":[["The images are not pre-processed in any way, except resizing to 64 x 80. Since the images are pretty noisy, I expect that some thoughtful pre-processing could yield better performance of the model."]]},"created_time":1646011980402,"last_edited_time":1646011980402,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9a87a66a-4b98-4cf3-855a-33653978854b":{"role":"reader","value":{"id":"9a87a66a-4b98-4cf3-855a-33653978854b","version":1,"type":"text","properties":{"title":[["Output images (masks) are scaled to [0, 1] interval."]]},"created_time":1646011980402,"last_edited_time":1646011980402,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e8010729-84e4-4fed-8276-23a1dafe047f":{"role":"reader","value":{"id":"e8010729-84e4-4fed-8276-23a1dafe047f","version":1,"type":"sub_sub_header","properties":{"title":[["Model"]]},"created_time":1646011980402,"last_edited_time":1646011980402,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9ee76d8c-f661-4204-91e3-087774858fc7":{"role":"reader","value":{"id":"9ee76d8c-f661-4204-91e3-087774858fc7","version":1,"type":"text","properties":{"title":[["The provided model is basically a convolutional auto-encoder, but with a twist - it has skip connections from encoder layers to decoder layers that are on the same “level”. See picture below (note that image size and numbers of convolutional filters in this tutorial differs from the original U-Net architecture)."]]},"created_time":1646011980403,"last_edited_time":1646011980403,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b3262fcf-0fb7-4795-b791-560cff6e5290":{"role":"reader","value":{"id":"b3262fcf-0fb7-4795-b791-560cff6e5290","version":8,"type":"image","properties":{"source":[["https://raw.githubusercontent.com/yihui-he/u-net/master/img/u-net-architecture.png"]]},"format":{"block_width":624,"display_source":"https://raw.githubusercontent.com/yihui-he/u-net/master/img/u-net-architecture.png","block_full_width":false,"block_page_width":false},"created_time":1646012040000,"last_edited_time":1646012040000,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3025088b-858c-4cf3-a500-edb2d23d6129":{"role":"reader","value":{"id":"3025088b-858c-4cf3-a500-edb2d23d6129","version":1,"type":"text","properties":{"title":[["This deep neural network is implemented with Keras functional API, which makes it extremely easy to experiment with different interesting architectures."]]},"created_time":1646011980404,"last_edited_time":1646011980404,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2f03046d-cf8e-4fa7-817d-d8a271dd6b5d":{"role":"reader","value":{"id":"2f03046d-cf8e-4fa7-817d-d8a271dd6b5d","version":1,"type":"text","properties":{"title":[["Output from the network is a 64 x 80 which represents mask that should be learned. Sigmoid activation function makes sure that mask pixels are in [0, 1] range."]]},"created_time":1646011980405,"last_edited_time":1646011980405,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8733a735-bc6e-4d6c-958b-3defa8849512":{"role":"reader","value":{"id":"8733a735-bc6e-4d6c-958b-3defa8849512","version":1,"type":"sub_sub_header","properties":{"title":[["Training"]]},"created_time":1646011980405,"last_edited_time":1646011980405,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7a2d7b2a-fe6e-4763-adb2-bb0d9587c3a5":{"role":"reader","value":{"id":"7a2d7b2a-fe6e-4763-adb2-bb0d9587c3a5","version":1,"type":"text","properties":{"title":[["The model is trained for 20 epochs, where each epoch took ~30 seconds on Titan X. Memory footprint of the model is ~800MB."]]},"created_time":1646011980405,"last_edited_time":1646011980405,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"522eb28d-f78f-40af-a68c-93d2c7590774":{"role":"reader","value":{"id":"522eb28d-f78f-40af-a68c-93d2c7590774","version":1,"type":"text","properties":{"title":[["After 20 epochs, calculated Dice coefficient is ~0.68, which yielded ~0.57 score on leaderboard, so obviously this model overfits (cross-validation pull requests anyone? ;))."]]},"created_time":1646011980406,"last_edited_time":1646011980406,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4e5fd49a-6ebc-47f6-a3f9-69a83787a6f0":{"role":"reader","value":{"id":"4e5fd49a-6ebc-47f6-a3f9-69a83787a6f0","version":1,"type":"text","properties":{"title":[["Loss function for the training is basically just a "],["negative of Dice coefficient",[["b"]]],[" (which is used as "],["evaluation metric on the competition",[["a","https://www.kaggle.com/c/ultrasound-nerve-segmentation/details/evaluation"]]],["), and this is implemented as custom loss function using Keras backend - check "],["dice_coef()",[["c"]]],[" and "],["dice_coef_loss()",[["c"]]],[" functions in "],["train.py",[["c"]]],[" for more detail. Also, for making the loss function smooth, a factor "],["smooth = 1",[["c"]]],[" factor is added."]]},"created_time":1646011980407,"last_edited_time":1646011980407,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"262e3a9a-c0db-4fd4-9882-f40ea4b57b15":{"role":"reader","value":{"id":"262e3a9a-c0db-4fd4-9882-f40ea4b57b15","version":1,"type":"text","properties":{"title":[["The weights are updated by Adam optimizer, with a 1e-5 learning rate. During training, model’s weights are saved in HDF5 format."]]},"created_time":1646011980408,"last_edited_time":1646011980408,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f0505516-5b09-425c-937d-f40feb06d0de":{"role":"reader","value":{"id":"f0505516-5b09-425c-937d-f40feb06d0de","version":1,"type":"divider","created_time":1646011980408,"last_edited_time":1646011980408,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e9d860e8-b810-4cb2-afdc-b8c5e5e5c7be":{"role":"reader","value":{"id":"e9d860e8-b810-4cb2-afdc-b8c5e5e5c7be","version":1,"type":"sub_header","properties":{"title":[["How to use"]]},"created_time":1646011980408,"last_edited_time":1646011980408,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8a26f2e6-d7cf-43e5-bdbd-25e64e8bb6de":{"role":"reader","value":{"id":"8a26f2e6-d7cf-43e5-bdbd-25e64e8bb6de","version":1,"type":"sub_sub_header","properties":{"title":[["Dependencies"]]},"created_time":1646011980408,"last_edited_time":1646011980408,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"942ceb58-9d0f-458a-94fb-d5b90312c6c1":{"role":"reader","value":{"id":"942ceb58-9d0f-458a-94fb-d5b90312c6c1","version":1,"type":"text","properties":{"title":[["This tutorial depends on the following libraries:"]]},"created_time":1646011980409,"last_edited_time":1646011980409,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"11aa751d-c3b1-41ab-8e07-d5ccbbed0e9b":{"role":"reader","value":{"id":"11aa751d-c3b1-41ab-8e07-d5ccbbed0e9b","version":1,"type":"bulleted_list","properties":{"title":[["cv2 (OpenCV)"]]},"created_time":1646011980411,"last_edited_time":1646011980411,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c7e46795-147f-485b-815f-0908f0e2b52c":{"role":"reader","value":{"id":"c7e46795-147f-485b-815f-0908f0e2b52c","version":1,"type":"bulleted_list","properties":{"title":[["Theano and/or Tensorflow"]]},"created_time":1646011980411,"last_edited_time":1646011980411,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b56a0224-1ad3-42ea-9a05-5eb9156a3e47":{"role":"reader","value":{"id":"b56a0224-1ad3-42ea-9a05-5eb9156a3e47","version":1,"type":"bulleted_list","properties":{"title":[["Keras \u003e= 1.0"]]},"created_time":1646011980411,"last_edited_time":1646011980411,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"29ed9f6b-9544-4bea-acd9-a40e1f38d669":{"role":"reader","value":{"id":"29ed9f6b-9544-4bea-acd9-a40e1f38d669","version":1,"type":"text","properties":{"title":[["Also, this code should be compatible with Python versions 2.7-3.5."]]},"created_time":1646011980412,"last_edited_time":1646011980412,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0a475b3e-5d45-4d0e-b3ed-846f4ffdbdca":{"role":"reader","value":{"id":"0a475b3e-5d45-4d0e-b3ed-846f4ffdbdca","version":1,"type":"sub_sub_header","properties":{"title":[["Prepare the data"]]},"created_time":1646011980412,"last_edited_time":1646011980412,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d108dc19-e065-4541-884c-df1a1970cbe8":{"role":"reader","value":{"id":"d108dc19-e065-4541-884c-df1a1970cbe8","version":1,"type":"text","properties":{"title":[["In order to extract raw images and save them to "],[".npy",[["i"]]],[" files, you should first prepare its structure. Make sure that "],["raw",[["c"]]],[" dir is located in the root of this project. Also, the tree of "],["raw",[["c"]]],[" dir must be like:"]]},"created_time":1646011980413,"last_edited_time":1646011980413,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"33a3028d-6ad3-4541-8e38-ad7bdf89aac8":{"role":"reader","value":{"id":"33a3028d-6ad3-4541-8e38-ad7bdf89aac8","version":1,"type":"code","properties":{"title":[["-raw\n |\n ---- train\n |    |\n |    ---- 1_1.tif\n |    |\n |    ---- …\n |\n ---- test\n      |\n      ---- 1.tif\n      |\n      ---- …"]],"language":[["Plain Text"]]},"format":{"code_wrap":true},"created_time":1646011980413,"last_edited_time":1646011980413,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7352554b-6c36-4c86-96a3-b210f348aeaf":{"role":"reader","value":{"id":"7352554b-6c36-4c86-96a3-b210f348aeaf","version":1,"type":"bulleted_list","properties":{"title":[["Now run "],["python data.py",[["c"]]],["."]]},"created_time":1646011980414,"last_edited_time":1646011980414,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3df16b5f-f3c4-46d7-a189-703196080070":{"role":"reader","value":{"id":"3df16b5f-f3c4-46d7-a189-703196080070","version":1,"type":"text","properties":{"title":[["Running this script will create train and test images and save them to "],[".npy",[["b"]]],[" files."]]},"created_time":1646011980414,"last_edited_time":1646011980414,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"33d95177-f522-4a3f-9259-0dfbe06df2a3":{"role":"reader","value":{"id":"33d95177-f522-4a3f-9259-0dfbe06df2a3","version":1,"type":"sub_sub_header","properties":{"title":[["Define the model"]]},"created_time":1646011980415,"last_edited_time":1646011980415,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6bc5c7f5-ca2a-4ba9-b91a-8561156036b5":{"role":"reader","value":{"id":"6bc5c7f5-ca2a-4ba9-b91a-8561156036b5","version":1,"type":"bulleted_list","properties":{"title":[["Check out "],["get_unet()",[["c"]]],[" in "],["train.py",[["c"]]],[" to modify the model, optimizer and loss function."]]},"created_time":1646011980415,"last_edited_time":1646011980415,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"84c3cad6-2e0b-4bc9-a867-64b5cabe4ace":{"role":"reader","value":{"id":"84c3cad6-2e0b-4bc9-a867-64b5cabe4ace","version":1,"type":"sub_sub_header","properties":{"title":[["Train the model and generate masks for test images"]]},"created_time":1646011980416,"last_edited_time":1646011980416,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"99bc4547-468c-497e-9593-e6b2670167b5":{"role":"reader","value":{"id":"99bc4547-468c-497e-9593-e6b2670167b5","version":1,"type":"bulleted_list","properties":{"title":[["Run "],["python train.py",[["c"]]],[" to train the model."]]},"created_time":1646011980416,"last_edited_time":1646011980416,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1f505393-6a9b-469e-b750-a3e557fc7dc3":{"role":"reader","value":{"id":"1f505393-6a9b-469e-b750-a3e557fc7dc3","version":1,"type":"text","properties":{"title":[["Check out "],["train_predict()",[["c"]]],[" to modify the number of iterations (epochs), batch size, etc."]]},"created_time":1646011980417,"last_edited_time":1646011980417,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e4ebbf43-5fc8-4b91-9706-ecf6e8342621":{"role":"reader","value":{"id":"e4ebbf43-5fc8-4b91-9706-ecf6e8342621","version":1,"type":"text","properties":{"title":[["After this script finishes, in "],["imgs_mask_test.npy",[["c"]]],[" masks for corresponding images in "],["imgs_test.npy",[["c"]]],[" should be generated. I suggest you examine these masks for getting further insight of your model’s performance."]]},"created_time":1646011980418,"last_edited_time":1646011980418,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"be9457fc-03c0-4951-9807-5ae133a00ceb":{"role":"reader","value":{"id":"be9457fc-03c0-4951-9807-5ae133a00ceb","version":1,"type":"sub_sub_header","properties":{"title":[["Generate submission"]]},"created_time":1646011980418,"last_edited_time":1646011980418,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"be7b3467-7edb-4c87-a4c3-a5f0867a11f8":{"role":"reader","value":{"id":"be7b3467-7edb-4c87-a4c3-a5f0867a11f8","version":1,"type":"bulleted_list","properties":{"title":[["Run "],["python submission.py",[["c"]]],[" to generate the submission file "],["submission.csv",[["c"]]],[" for the generated masks."]]},"created_time":1646011980419,"last_edited_time":1646011980419,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cc8c7eef-ef27-41b0-8cab-34f9dd9e642b":{"role":"reader","value":{"id":"cc8c7eef-ef27-41b0-8cab-34f9dd9e642b","version":1,"type":"text","properties":{"title":[["Check out function "],["submission()",[["c"]]],[" and "],["run_length_enc()",[["c"]]],[" (thanks woshialex) for details."]]},"created_time":1646011980420,"last_edited_time":1646011980420,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5f9e6a02-cec1-42f0-9076-0b8563338850":{"role":"reader","value":{"id":"5f9e6a02-cec1-42f0-9076-0b8563338850","version":1,"type":"sub_header","properties":{"title":[["About Keras"]]},"created_time":1646011980421,"last_edited_time":1646011980421,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2ce77179-be6c-4434-b1fd-31bd365cda71":{"role":"reader","value":{"id":"2ce77179-be6c-4434-b1fd-31bd365cda71","version":1,"type":"text","properties":{"title":[["Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research."]]},"created_time":1646011980421,"last_edited_time":1646011980421,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"91af2294-0984-4f76-8ac7-dfe621a03ed4":{"role":"reader","value":{"id":"91af2294-0984-4f76-8ac7-dfe621a03ed4","version":1,"type":"text","properties":{"title":[["Use Keras if you need a deep learning library that:"]]},"created_time":1646011980422,"last_edited_time":1646011980422,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b445ca86-fee4-44cf-be06-1bdc5faaee74":{"role":"reader","value":{"id":"b445ca86-fee4-44cf-be06-1bdc5faaee74","version":1,"type":"text","properties":{"title":[["allows for easy and fast prototyping (through total modularity, minimalism, and extensibility). supports both convolutional networks and recurrent networks, as well as combinations of the two. supports arbitrary connectivity schemes (including multi-input and multi-output training). runs seamlessly on CPU and GPU. Read the documentation "],["Keras.io",[["a","http://keras.io/"]]]]},"created_time":1646011980423,"last_edited_time":1646011980423,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"687759b5-83d8-44b6-bad7-64137e560f91":{"role":"reader","value":{"id":"687759b5-83d8-44b6-bad7-64137e560f91","version":1,"type":"text","properties":{"title":[["Keras is compatible with: Python 2.7-3.5."]]},"created_time":1646011980424,"last_edited_time":1646011980424,"parent_id":"e7a12f56-b48c-43bf-803e-96d70518685f","parent_table":"block","alive":true,"ignore_block_count":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}}},"space":{},"discussion":{},"comment":{},"collection":{},"collection_view":{},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{}},"pageId":"e7a12f56-b48c-43bf-803e-96d70518685f"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"ultrasound-nerve-segmentation-with-tensorflow"},"buildId":"sJmC7l3paKmXPUP3FZfmF","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>