<!DOCTYPE html><html lang="en"><head><link rel="shortcut icon" href="/favicon.png"/><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png"/><link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"/><link rel="manifest" href="/manifest.json"/><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="theme-color" content="#EB625A"/><meta property="og:type" content="website"/><meta property="og:title" content="Deployment &amp; Monitoring"/><meta property="og:site_name" content="Yihui He"/><meta name="twitter:title" content="Deployment &amp; Monitoring"/><meta property="twitter:domain" content="yihui-he.github.io"/><meta name="twitter:creator" content="@he_yi_hui"/><meta name="description" content="Yihui He, AI research scientist / full stack engineer"/><meta property="og:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:description" content="Yihui He, AI research scientist / full stack engineer"/><meta name="twitter:card" content="summary"/><link rel="canonical" href="https://yihui-he.github.io/deployment-monitoring"/><meta property="og:url" content="https://yihui-he.github.io/deployment-monitoring"/><meta property="twitter:url" content="https://yihui-he.github.io/deployment-monitoring"/><title>Deployment &amp; Monitoring</title><meta name="next-head-count" content="19"/><link rel="preload" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/ce9b11b02642a9642d6d.css" data-n-g=""/><link rel="preload" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/4b67152b49ef8d389eef.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/blog/_next/static/chunks/webpack-ed2d627650adadaab635.js" defer=""></script><script src="/blog/_next/static/chunks/framework-c93ed74a065331c4bd75.js" defer=""></script><script src="/blog/_next/static/chunks/main-0989120ac94443065aa9.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-e24568358f9b2ca9ee76.js" defer=""></script><script src="/blog/_next/static/chunks/1bfc9850-762e4e08544c8bec659c.js" defer=""></script><script src="/blog/_next/static/chunks/ae51ba48-7f31b5cf321fe3268476.js" defer=""></script><script src="/blog/_next/static/chunks/d7eeaac4-07e2c37279f27c6f41bc.js" defer=""></script><script src="/blog/_next/static/chunks/808-8e92cc1e8e3a13c0ecab.js" defer=""></script><script src="/blog/_next/static/chunks/270-25d82ee694bd9ee7ef5a.js" defer=""></script><script src="/blog/_next/static/chunks/pages/%5BpageId%5D-845d25c09c3d16569292.js" defer=""></script><script src="/blog/_next/static/YmEd5Nabys24EQBvAhdkU/_buildManifest.js" defer=""></script><script src="/blog/_next/static/YmEd5Nabys24EQBvAhdkU/_ssgManifest.js" defer=""></script></head><body><script src="noflash.js"></script><div id="__next"><div class="notion notion-app light-mode notion-block-d8ff64bd62184dcb9b3c7daed5c4aa3a"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/blog"><img class="icon notion-page-icon" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F63e89fe9-f5cd-4414-be0e-53d91aa54fc3%2Fme_small.jpg?table=block&amp;id=bd32b787-e471-49f3-8941-174a9c6846b6&amp;cache=v2" alt="Yihui He’s Blog" loading="lazy"/><span class="title">Yihui He’s Blog</span></a><span class="spacer">/</span><a class="breadcrumb" href="/blog/deep-learning-engineer-manual"><span class="title">deep learning engineer manual</span></a><span class="spacer">/</span><div class="breadcrumb active"><span class="title">Deployment &amp; Monitoring</span></div></div><div class="rhs"></div></div></header><div class="notion-page-scroller"><main class="notion-page notion-page-no-cover notion-page-no-icon notion-page-has-text-icon notion-full-page"><h1 class="notion-title"><b>Deployment &amp; Monitoring</b></h1><div class="notion-page-content notion-page-content-has-aside notion-page-content-has-toc"><article class="notion-page-content-inner"><div class="notion-text notion-block-d7bb06e252544821b12a8e0926bf1e3a">ML in production scales to meet users’ demands by delivering thousands to millions of predictions per second. On the other hand, models in notebooks only work if you run the cells in the right order. To be frank, <b>most data scientists and ML engineers do not know how to build production ML systems.</b> Therefore, the goal of this lecture is to give you different flavors of accomplishing that task.</div><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-ddbc4cae315042e880f6bf2779495134" data-id="ddbc4cae315042e880f6bf2779495134"><span><div id="ddbc4cae315042e880f6bf2779495134" class="notion-header-anchor"></div><a class="notion-hash-link" href="#ddbc4cae315042e880f6bf2779495134" title="I - Model Deployment"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">I - Model Deployment</span></span></h2><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-83c7b2de5e544cde888d46d2c855d301" data-id="83c7b2de5e544cde888d46d2c855d301"><span><div id="83c7b2de5e544cde888d46d2c855d301" class="notion-header-anchor"></div><a class="notion-hash-link" href="#83c7b2de5e544cde888d46d2c855d301" title="1 - Types of Deployment"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>1 - Types of Deployment</b></span></span></h3><div class="notion-text notion-block-cc684769b0e94358a9cb8eaee926eb54">One way to conceptualize different approaches to deploy ML models is to think about where to deploy them in your application’s overall architecture.</div><ul class="notion-list notion-list-disc notion-block-de72468797c4441d90a983d3da02736c"><li>The <b>client-side</b> runs locally on the user machine (web browser, mobile devices, etc..)</li></ul><ul class="notion-list notion-list-disc notion-block-0bfc862e88044479a6b97b5ef742e9f6"><li>It connects to the <b>server-side</b> that runs your code remotely.</li></ul><ul class="notion-list notion-list-disc notion-block-81c258ec743547139d2a268d27e66cf7"><li>The server connects with a <b>database</b> to pull data out, render the data, and show the data to the user.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-333f1ede6c7544518612375e874a9cc5" data-id="333f1ede6c7544518612375e874a9cc5"><span><div id="333f1ede6c7544518612375e874a9cc5" class="notion-header-anchor"></div><a class="notion-hash-link" href="#333f1ede6c7544518612375e874a9cc5" title="BATCH PREDICTION"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>BATCH PREDICTION</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-f14b4c1d2ad344039c6616229e4b93e1"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage16.png?table=block&amp;id=f14b4c1d-2ad3-4403-9c66-16229e4b93e1&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-fff58ca03c42402ea56af16a7a02fd1b">Batch prediction means that you train the models offline, dump the results into a database, then run the rest of the application normally. You periodically run your model on new data coming in and cache the results in a database. Batch prediction is commonly used in production when the universe of inputs is relatively small (e.g., one prediction per user per day).</div><div class="notion-text notion-block-c9587f25b11d428db81232b2e8c9189b">The pros of batch prediction:</div><ul class="notion-list notion-list-disc notion-block-5ca91d1417cc470f8d569dfe684dee40"><li>It is simple to implement.</li></ul><ul class="notion-list notion-list-disc notion-block-5a931d5b1bde42118ca47f2e8ca8ce8c"><li>It requires relatively low latency to the user.</li></ul><div class="notion-text notion-block-c50cb623abc84482bf688a36994381cf">The cons of batch prediction:</div><ul class="notion-list notion-list-disc notion-block-2a985fc4ec60425894f276d6d3f26ae5"><li>It does not scale to complex input types.</li></ul><ul class="notion-list notion-list-disc notion-block-e898ccf54f9647e98aa8ca1d7def8ed4"><li>Users do not get the most up-to-date predictions.</li></ul><ul class="notion-list notion-list-disc notion-block-b907773cefb0493d992e047bb58fb7e6"><li>Models frequently become “stale” and hard to detect.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-2088ed0619ce479f8a1f892aed6c039a" data-id="2088ed0619ce479f8a1f892aed6c039a"><span><div id="2088ed0619ce479f8a1f892aed6c039a" class="notion-header-anchor"></div><a class="notion-hash-link" href="#2088ed0619ce479f8a1f892aed6c039a" title="MODEL-IN-SERVICE"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MODEL-IN-SERVICE</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-11c5a78472104209951bab34d94c7167"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage11.png?table=block&amp;id=11c5a784-7210-4209-951b-ab34d94c7167&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-cb900a31bc294098b592022b3287fc70">Model-in-service means that you package up your model and include it in the deployed web server. Then, the web server loads the model and calls it to make predictions.</div><div class="notion-text notion-block-e45fcdea74684fdfb499417f56486800">The pros of model-in-service prediction:</div><ul class="notion-list notion-list-disc notion-block-69df312821be40b793efa901113a59d8"><li>It reuses your existing infrastructure.</li></ul><div class="notion-text notion-block-3c875bab7c94495b99bd0a94cf797e64">The cons of model-in-service prediction:</div><ul class="notion-list notion-list-disc notion-block-177f9d4ce71748f0b51818c97ed1c6a3"><li>The web server may be written in a different language.</li></ul><ul class="notion-list notion-list-disc notion-block-76bc67d1549e41bda54e2b42cb8ef769"><li>Models may change more frequently than the server code.</li></ul><ul class="notion-list notion-list-disc notion-block-c86dc35ef3ba4c2e99bea83dc3662e1e"><li>Large models can eat into the resources for your webserver.</li></ul><ul class="notion-list notion-list-disc notion-block-3b0658fa027c40f9bcaa971ed65bfe91"><li>Server hardware is not optimized for your model (e.g., no GPUs).</li></ul><ul class="notion-list notion-list-disc notion-block-3b90f5ef5d054f17964413ddb71b1f07"><li>Model and server may scale differently.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-2712ad117a114c5484f2cf8267579000" data-id="2712ad117a114c5484f2cf8267579000"><span><div id="2712ad117a114c5484f2cf8267579000" class="notion-header-anchor"></div><a class="notion-hash-link" href="#2712ad117a114c5484f2cf8267579000" title="MODEL-AS-SERVICE"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MODEL-AS-SERVICE</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-6b7e81a0927743f4b490c542045c4eeb"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage14.png?table=block&amp;id=6b7e81a0-9277-43f4-b490-c542045c4eeb&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-80caff591bfa4ea4bc75738eabfada2f">Model-as-service means that you deploy the model separately as its own service. The client and server can interact with the model by making requests to the model service and receiving responses.</div><div class="notion-text notion-block-324eeb94a91147609eb2ee0a565d01b4">The pros of model-as-service prediction:</div><ul class="notion-list notion-list-disc notion-block-c75a2a6ead124c999895d6735447a614"><li>It is dependable, as model bugs are less likely to crash the web app.</li></ul><ul class="notion-list notion-list-disc notion-block-c874def9bfe7483eb7ab2d5ec003e4f3"><li>It is scalable, as you can choose the optimal hardware for the model and scale it appropriately.</li></ul><ul class="notion-list notion-list-disc notion-block-18fbd66f8dd14095a7c82c5f64d98856"><li>It is flexible, as you can easily reuse the model across multiple applications.</li></ul><div class="notion-text notion-block-8709092a99274366ac8e8e96a590265f">The cons of model-as-service prediction:</div><ul class="notion-list notion-list-disc notion-block-ed9769a042524dfdbcf7987a5aae155c"><li>It adds latency.</li></ul><ul class="notion-list notion-list-disc notion-block-f38b998f2d7c49ca8ce33106dfba461c"><li>It adds infrastructural complexity.</li></ul><ul class="notion-list notion-list-disc notion-block-670352d2449b46ceb28380080b700f61"><li>Most importantly, you are now on the hook to run a model service...</li></ul><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-9d2e80af7319432199abc3832ebabad0" data-id="9d2e80af7319432199abc3832ebabad0"><span><div id="9d2e80af7319432199abc3832ebabad0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9d2e80af7319432199abc3832ebabad0" title="2 - Building A Model Service"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2 - Building A Model Service</b></span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-daf1d36531824bbea42d060411578a80" data-id="daf1d36531824bbea42d060411578a80"><span><div id="daf1d36531824bbea42d060411578a80" class="notion-header-anchor"></div><a class="notion-hash-link" href="#daf1d36531824bbea42d060411578a80" title="REST APIS"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>REST APIS</b></span></span></h4><div class="notion-text notion-block-5d06a3661aef49b1a8b467d8c8c61db5"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.sitepoint.com/developers-rest-api/"><span class="notion-inline-underscore"><b>REST APIs</b></span></a> represent a way of serving predictions in response to canonically formatted HTTP requests. There are alternatives such as <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://grpc.io/"><span class="notion-inline-underscore"><b>gRPC</b></span></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://graphql.org/"><span class="notion-inline-underscore"><b>GraphQL</b></span></a>. For instance, in your command line, you can use <em>curl</em> to post some data to an URL and get back JSON that contains the model predictions.</div><div class="notion-text notion-block-47c92ea65e9f458684c8a3f0520b2854">Sadly, there is no standard way of formatting the data that goes into an ML model.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-82f062cac2a6479ca5c7137ff4727e63" data-id="82f062cac2a6479ca5c7137ff4727e63"><span><div id="82f062cac2a6479ca5c7137ff4727e63" class="notion-header-anchor"></div><a class="notion-hash-link" href="#82f062cac2a6479ca5c7137ff4727e63" title="DEPENDENCY MANAGEMENT"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>DEPENDENCY MANAGEMENT</b></span></span></h4><div class="notion-text notion-block-ecdf3542f59146a6a3fba3feb5255597">Model predictions depend on the <b>code</b>, the <b>model weights</b>, and the <b>code dependencies</b>. All three need to be present on your webserver. For code and model weights, you can simply copy them locally (or write a script to extract them if they are large). But dependencies are trickier because they cause troubles. As they are hard to make consistent and update, your model behavior might change accordingly.</div><div class="notion-text notion-block-a45a49aaa6a849b4be9340608b637481">There are two high-level strategies to manage code dependencies:</div><ol start="1" class="notion-list notion-list-numbered notion-block-290a55ee45124e9c8afce7e78a78d487"><li>You constrain the dependencies of your model.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-86c8e652630045f69fe9f39d908a3a06"><li>You use containers.</li></ol><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-3e7aeb218ac74eef9b1394a791116ff9" data-id="3e7aeb218ac74eef9b1394a791116ff9"><span><div id="3e7aeb218ac74eef9b1394a791116ff9" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3e7aeb218ac74eef9b1394a791116ff9" title="ONNX"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>ONNX</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-b49ee30469cb485ab78eb06c3432a21f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage15.png?table=block&amp;id=b49ee304-69cb-485a-b78e-b06c3432a21f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-a84bc0f01a44458f931939102af0aff0"><b>If you go with the first strategy, you need a standard neural network format.</b> The <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://onnx.ai/"><span class="notion-inline-underscore"><b>Open Neural Network Exchange</b></span></a> (ONNX, for short) is designed to allow framework interoperability. The dream is to mix different frameworks, such that frameworks that are good for development (PyTorch) don’t also have to be good at inference (Caffe2).</div><ul class="notion-list notion-list-disc notion-block-3ad42750d0844434b5fea8196c8bbbdf"><li>The promise is that you can train a model with one tool stack and then deploy it using another for inference/prediction. ONNX is a robust and open standard for preventing framework lock-in and ensuring that your models will be usable in the long run.</li></ul><ul class="notion-list notion-list-disc notion-block-a6a9020527784e97942fb5ac3fb9a378"><li>The reality is that since ML libraries change quickly, there are often bugs in the translation layer. Furthermore, how do you deal with non-library code (like feature transformations)?</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-3d4e11ab622e4d00bfabe05d0d4ccf5e" data-id="3d4e11ab622e4d00bfabe05d0d4ccf5e"><span><div id="3d4e11ab622e4d00bfabe05d0d4ccf5e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3d4e11ab622e4d00bfabe05d0d4ccf5e" title="Docker"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Docker</b></span></span></h4><div class="notion-text notion-block-508e8655ba2a4626aee0daddc8497e6f"><b>If you go with the second strategy, you want to learn Docker</b>. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.docker.com/"><span class="notion-inline-underscore"><b>Docker</b></span></a> is a computer program that performs operating-system-level virtualization, also known as containerization. What is a container, you might ask? It is a standardized unit of fully packaged software used for local development, shipping code, and deploying system.</div><div class="notion-text notion-block-77271b722ff847009302c05c0ffe85fb">The best way to describe it intuitively is to think of a process surrounded by its filesystem. You run one or a few related processes, and they see a whole filesystem, not shared by anyone.</div><ul class="notion-list notion-list-disc notion-block-578a3100e2174d9a9b13cf4cf8a90b9b"><li>This makes containers <b>extremely portable</b>, as they are detached from the underlying hardware and the platform that runs them.</li></ul><ul class="notion-list notion-list-disc notion-block-209a714e128a44e9abc15dc3234b2fe3"><li>They are very <b>lightweight</b>, as a minimal amount of data needs to be included.</li></ul><ul class="notion-list notion-list-disc notion-block-d74ab2dc83b84c6bafff0a9dd40d837e"><li>They are <b>secure</b>, as the exposed attack surface of a container is extremely small.</li></ul><div class="notion-text notion-block-51ebf39d1493448c9b65caef9875e9d9">Note here that <b>containers are different from virtual machines</b>.</div><ul class="notion-list notion-list-disc notion-block-2a934e4dc98e479299ef4be26cc5acee"><li>Virtual machines require the hypervisor to virtualize a full hardware stack. There are also multiple guest operating systems, making them larger and more extended to boot. This is what AWS / GCP / Azure cloud instances are.</li></ul><ul class="notion-list notion-list-disc notion-block-8f767ef4caa042329ff5f8c3b417e3c2"><li>Containers, on the other hand, require no hypervisor/hardware virtualization. All containers share the same host kernel. There are dedicated isolated user-space environments, making them much smaller in size and faster to boot.</li></ul><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-c9abc232dde546c9bc31d99e2c7a387c"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage12.png?table=block&amp;id=c9abc232-dde5-46c9-bc31-d99e2c7a387c&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-b3c5d5b97f694920b4f76a11c7e52c6c">In brief, you should familiarize yourself with these basic concepts:</div><ol start="1" class="notion-list notion-list-numbered notion-block-815dfa4b15994ca8b92bb449c9436b02"><li><b>Dockerfile</b> defines how to build an image.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-458d710837604d62a2963345f4ddc297"><li><b>Image</b> is a built packaged environment.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-d57c561e3d9643ed82d847de05f16651"><li><b>Containe</b>r is where images are run inside.</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-03bb40952a5e46e68cd8b1a6a844770a"><li><b>Repository</b> hosts different versions of an image.</li></ol><ol start="5" class="notion-list notion-list-numbered notion-block-49afb1a678ee44e9bbc04dc271a83377"><li><b>Registry</b> is a set of repositories.</li></ol><div class="notion-text notion-block-0df0ead7403b407e9f0c76f29db21369">Furthermore, Docker has a robust ecosystem. It has the <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://hub.docker.com/"><span class="notion-inline-underscore"><b>DockerHub</b></span></a> for community-contributed images. It’s incredibly easy to search for images that meet your needs, ready to pull down and use with little-to-no modification.</div><div class="notion-text notion-block-0a1ad2a7c0df444ab7bdedab7173da06">Though Docker presents how to deal with each of the individual microservices, we also need <b>an orchestrator</b> to handle the whole cluster of services. Such an orchestrator distributes containers onto the underlying virtual machines or bare metal so that these containers talk to each other and coordinate to solve the task at hand. The standard container orchestration tool is <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://kubernetes.io/"><span class="notion-inline-underscore"><b>Kubernetes</b></span></a>.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-5f49b85dc2064114bd666c3dc9f932d0" data-id="5f49b85dc2064114bd666c3dc9f932d0"><span><div id="5f49b85dc2064114bd666c3dc9f932d0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5f49b85dc2064114bd666c3dc9f932d0" title="PERFORMANCE OPTIMIZATION"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>PERFORMANCE OPTIMIZATION</b></span></span></h4><div class="notion-text notion-block-3d65c20e6e0a40aaa8b9cb7a0092e6c5">We will talk mostly about how to run your model service faster on a single machine. Here are the key questions that you want to address:</div><ul class="notion-list notion-list-disc notion-block-de8fb983febd4c4dba03165974e9c075"><li>Do you want inference on a GPU or not?</li></ul><ul class="notion-list notion-list-disc notion-block-7bbbd7a1ed404282baba7aaab3543edc"><li>How can you run multiple copies of the model at the same time?</li></ul><ul class="notion-list notion-list-disc notion-block-22477281f00c4b13b25f7226c3e2d695"><li>How to make the model smaller?</li></ul><ul class="notion-list notion-list-disc notion-block-a1b4d927d23544e69126554a674d028b"><li>How to improve model performance via caching, batching, and GPU sharing?</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-327a58795e804634888e3ee571639c5c" data-id="327a58795e804634888e3ee571639c5c"><span><div id="327a58795e804634888e3ee571639c5c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#327a58795e804634888e3ee571639c5c" title="GPU or no GPU?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>GPU or no GPU?</b></span></span></h4><div class="notion-text notion-block-042d9ee265964494b44782631248578d">Here are the pros of GPU inference:</div><ul class="notion-list notion-list-disc notion-block-d635c38b97b34b49af35c9a2edc85222"><li>You use the same hardware that your model is trained on probably.</li></ul><ul class="notion-list notion-list-disc notion-block-858b27eb2fee4ce9937e8800808f5197"><li>If your model gets bigger and you want to limit model size or tune batch size, you will get high throughput.</li></ul><div class="notion-text notion-block-d7d6eb0e5ddc42bd88d34cb722475103">Here are the cons of GPU inference:</div><ul class="notion-list notion-list-disc notion-block-3ae81a8667db433bbd3cbd80ac56acfe"><li>GPU is complex to set up.</li></ul><ul class="notion-list notion-list-disc notion-block-a1cf01493b5a408ebe559c2c21727c9c"><li>GPUs are expensive.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-9935640836d14dfdbc0d2e22414cda7e" data-id="9935640836d14dfdbc0d2e22414cda7e"><span><div id="9935640836d14dfdbc0d2e22414cda7e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9935640836d14dfdbc0d2e22414cda7e" title="Concurrency"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Concurrency</b></span></span></h4><div class="notion-text notion-block-40f37f7512814d20bd82041cc61c2157">Instead of running a single model copy on your machine, you run multiple model copies on different CPUs or cores. In practice, you need to be careful about <b>thread tuning</b> - making sure that each model copy only uses the minimum number of threads required. Read <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/"><span class="notion-inline-underscore"><b>this blog post from Roblox</b></span></a> for the details.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-5b7f0fe8f6944b458d3178a3f6049f61" data-id="5b7f0fe8f6944b458d3178a3f6049f61"><span><div id="5b7f0fe8f6944b458d3178a3f6049f61" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5b7f0fe8f6944b458d3178a3f6049f61" title="Model distillation"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Model distillation</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-5661897e16d94a60b08872e79203e72f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage7.png?table=block&amp;id=5661897e-16d9-4a60-b088-72e79203e72f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-f8f3157459c7484dbf68db81b459618b">Model distillation is a compression technique in which a small “student” model is trained to reproduce the behavior of a large “teacher” model. The method was first proposed by <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf"><span class="notion-inline-underscore"><b>Bucila et al., 2006</b></span></a> and generalized by <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/pdf/1503.02531.pdf"><span class="notion-inline-underscore"><b>Hinton et al., 2015</b></span></a>. In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function. The target is the distribution of class probabilities predicted by the teacher model. That is — the output of a softmax function on the teacher model’s logits.</div><div class="notion-text notion-block-eb727e7af01d42109df25f961da27855">Distillation can be finicky to do yourself, so <b>it is infrequently used in practice</b>. Read <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://heartbeat.fritz.ai/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb"><span class="notion-inline-underscore"><b>this blog post from Derrick Mwiti</b></span></a> for several model distillation techniques for deep learning.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-1f154196cd5340488ab17942f4fa1f8c" data-id="1f154196cd5340488ab17942f4fa1f8c"><span><div id="1f154196cd5340488ab17942f4fa1f8c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1f154196cd5340488ab17942f4fa1f8c" title="Model quantization"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Model quantization</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-29bc374340624840bf72efed40ed5209"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage1.png?table=block&amp;id=29bc3743-4062-4840-bf72-efed40ed5209&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-76d5ed56673e4c10b6f7fb00bed8a9ee">Model quantization is a model compression technique that makes the model physically smaller to save disk space and require less memory during computation to run faster. It decreases the numerical precision of a model’s weights. In other words, each weight is permanently encoded using fewer bits. Note here that <b>there are tradeoffs with accuracy</b>.</div><ul class="notion-list notion-list-disc notion-block-ee9fe243b0414aabbe10ba06a9cf3527"><li>A straightforward method is implemented <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/lite/performance/quantization_spec"><span class="notion-inline-underscore"><b>in the TensorFlow Lite toolkit</b></span></a>. It turns a matrix of 32-bit floats into 8-bit integers by applying a simple “center-and-scale” transform to it: <em>W_8 = W_32 / scale + shift</em> (scale and shift are determined individually for each weight matrix). This way, the 8-bit W is used in matrix multiplication, and only the result is then corrected by applying the “center-and-scale” operation in reverse.</li></ul><ul class="notion-list notion-list-disc notion-block-19342d26a52545f287335a8caded34f6"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pytorch.org/blog/introduction-to-quantization-on-pytorch/"><span class="notion-inline-underscore"><b>PyTorch also has quantization built-in</b></span></a> that includes three techniques: dynamic quantization, post-training static quantization, and quantization-aware training.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-0827873e016f4e95a3948e105996859d" data-id="0827873e016f4e95a3948e105996859d"><span><div id="0827873e016f4e95a3948e105996859d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#0827873e016f4e95a3948e105996859d" title="Caching"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Caching</b></span></span></h4><div class="notion-text notion-block-893e42ae851b43c8a5e829d930c47a7f">For many ML models, the input distribution is non-uniform (some are more common than others). Caching takes advantage of that. Instead of constantly calling the model on every input no matter what, we first <b>cache the model’s frequently-used inputs</b>. Before calling the model, we check the cache and only call it on the frequently-used inputs.</div><div class="notion-text notion-block-8c7ee54e64e34b2cabb52ca0623faca6">Caching techniques can get very fancy, but the most basic way to get started is using Python’s <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://docs.python.org/3/library/functools.html"><span class="notion-inline-underscore"><b>functools</b></span></a>.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-06de6f97d9554729bd862ee521467a85"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage6.png?table=block&amp;id=06de6f97-d955-4729-bd86-2ee521467a85&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-011b1a5527fb4c0a85f31568c73124b5" data-id="011b1a5527fb4c0a85f31568c73124b5"><span><div id="011b1a5527fb4c0a85f31568c73124b5" class="notion-header-anchor"></div><a class="notion-hash-link" href="#011b1a5527fb4c0a85f31568c73124b5" title="Batching"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Batching</b></span></span></h4><div class="notion-text notion-block-4109ef77deea4e5894d67c92dddebde2">Typically, ML models achieve higher throughput when making predictions in parallel (especially true for GPU inference). At a high level, here’s how batching works:</div><ul class="notion-list notion-list-disc notion-block-699a9dd42dc84e93b0016320b8175451"><li>You gather predictions that are coming in until you have a batch for your system. Then, you run the model on that batch and return predictions to those users who request them.</li></ul><ul class="notion-list notion-list-disc notion-block-780cba656d5c4745894ad12652b805fc"><li>You need to tune the batch size and address the tradeoff between throughput and latency.</li></ul><ul class="notion-list notion-list-disc notion-block-35f21fa1fc6b475ebcf87d6f2f3cd81c"><li>You need to have a way to shortcut the process if latency becomes too long.</li></ul><ul class="notion-list notion-list-disc notion-block-437d7cbce2a247c3a6112011d958700b"><li>The last caveat is that <b>you probably do not want to implement batching yourself</b>.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-c458ce171fde416b9257a2b411b7fbe0" data-id="c458ce171fde416b9257a2b411b7fbe0"><span><div id="c458ce171fde416b9257a2b411b7fbe0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c458ce171fde416b9257a2b411b7fbe0" title="Sharing The GPU"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Sharing The GPU</b></span></span></h4><div class="notion-text notion-block-6351b16f1e254bf1847de04fc8fe3017">Your model may not take up all of the GPU memory with your inference batch size. <b>Why not run multiple models on the same GPU?</b> You probably want to use a model serving solution that supports this out of the box.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-fb0310d20ac648e78a2966f97f1816be" data-id="fb0310d20ac648e78a2966f97f1816be"><span><div id="fb0310d20ac648e78a2966f97f1816be" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fb0310d20ac648e78a2966f97f1816be" title="Model Serving Libraries"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Model Serving Libraries</b></span></span></h4><div class="notion-text notion-block-691ed7658f7b4d469a5a03ec63e9f698">There are canonical open-source model serving libraries for both PyTorch (<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pytorch.org/serve/"><span class="notion-inline-underscore"><b>TorchServe</b></span></a>) and TensorFlow (<a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/tfx/guide/serving"><span class="notion-inline-underscore"><b>TensorFlow Serving</b></span></a>). <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://docs.ray.io/en/master/serve/index.html"><span class="notion-inline-underscore"><b>Ray Serve</b></span></a> is another promising choice. Even NVIDIA has joined the game with <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://developer.nvidia.com/nvidia-triton-inference-server"><span class="notion-inline-underscore"><b>Triton Inference Server</b></span></a>.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-3c663d3a4d3f428fafd3420c33fd1bb7"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage20.png?table=block&amp;id=3c663d3a-4d3f-428f-afd3-420c33fd1bb7&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-4aabd7a100904da4998e6ff08af33bef" data-id="4aabd7a100904da4998e6ff08af33bef"><span><div id="4aabd7a100904da4998e6ff08af33bef" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4aabd7a100904da4998e6ff08af33bef" title="HORIZONTAL SCALING"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>HORIZONTAL SCALING</b></span></span></h4><div class="notion-text notion-block-8576978cc82a4948832be59442cbd2c7"><b>If you have too much traffic for a single machine, let’s split traffic among multiple machines</b>. At a high level, you duplicate your prediction service, use a load balancer to split traffic, and send the traffic to the appropriate copy of your service. In practice, there are two common methods:</div><ol start="1" class="notion-list notion-list-numbered notion-block-b2c82e4528cb4dbdaf4bc1cc9397cda7"><li>Use a container orchestration toolkit like Kubernetes.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-b8055fa6450d433497e2bcff8ca2b4b0"><li>Use a serverless option like AWS Lambda.</li></ol><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-0243f2e88ee94afe826a5902ae0e0c65" data-id="0243f2e88ee94afe826a5902ae0e0c65"><span><div id="0243f2e88ee94afe826a5902ae0e0c65" class="notion-header-anchor"></div><a class="notion-hash-link" href="#0243f2e88ee94afe826a5902ae0e0c65" title="Container Orchestration"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Container Orchestration</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-9a6ccb1cb38d4583bf73b1848f84ef60"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage21.png?table=block&amp;id=9a6ccb1c-b38d-4583-bf73-b1848f84ef60&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-190b2ac29e3940ddbb3e5322bb7eb904">In this paradigm, your Docker containers are coordinated by Kubernetes. K8s provides a single service for you to send requests to. Then it divides up traffic that gets sent to that service to virtual copies of your containers (that are running on your infrastructure).</div><div class="notion-text notion-block-b8e49bc41ec6487386cff8ed846adfa5">You can build a system like this yourself on top of K8s if you want to. But there are emerging frameworks that can handle all such infrastructure out of the box if you have a K8s cluster running. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kubeflow.org/docs/components/kfserving/"><span class="notion-inline-underscore"><b>KFServing</b></span></a> is a part of the <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.kubeflow.org/"><span class="notion-inline-underscore"><b>Kubeflow</b></span></a> package, a popular K8s-native ML infrastructure solution. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://seldon.io/"><span class="notion-inline-underscore"><b>Seldon</b></span></a> provides a model serving stack on top of K8s.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-7d91fa632c1c4bff96a7f237a22e2d3e" data-id="7d91fa632c1c4bff96a7f237a22e2d3e"><span><div id="7d91fa632c1c4bff96a7f237a22e2d3e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#7d91fa632c1c4bff96a7f237a22e2d3e" title="Deploying Code As Serverless Functions"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Deploying Code As Serverless Functions</b></span></span></h4><div class="notion-text notion-block-cb13bd668c0a44db88040d68b5f22654">The idea here is that the app code and dependencies are packaged into .zip files (or Docker containers) with a single entry point function. All the major cloud providers such as AWS Lambda, Google Cloud Functions, or Azure Functions will manage everything else: instant scaling to 10,000+ requests per second, load balancing, etc.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-b68fa00fe7c742abbf29f65b5dee9488"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage10.png?table=block&amp;id=b68fa00f-e7c7-42ab-bf29-f65b5dee9488&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-0c3ca592da6647e0a59d977fb46a61d5">The good thing is that <b>you only pay for compute-time</b>. Furthermore, this approach lowers your DevOps load, as you do not own any servers.</div><div class="notion-text notion-block-4830cfc0931746c58f3c3f9802d42932">The tradeoff is that you have to work with <b>severe constraints</b>:</div><ol start="1" class="notion-list notion-list-numbered notion-block-4a0ae5eff6244ca8bd8a02e558f63814"><li>Your entire deployment package is quite limited.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-09e52cb8a8da4fa8925e15f701bd56ee"><li>You can only do CPU execution.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-8543371544ae418495963a38c67249ed"><li>It can be challenging to build model pipelines.</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-8b5e0ebc534b402ab9e79128d90e430d"><li>There are limited state management and deployment tooling.</li></ol><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-e1ba2c9dc70d48c18cb315528f6741c6" data-id="e1ba2c9dc70d48c18cb315528f6741c6"><span><div id="e1ba2c9dc70d48c18cb315528f6741c6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e1ba2c9dc70d48c18cb315528f6741c6" title="MODEL DEPLOYMENT"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MODEL DEPLOYMENT</b></span></span></h4><div class="notion-text notion-block-fee9cd051e0e4ba7b7740cb7132b9179">If serving is how you turn a model into something that can respond to requests, <b>deployment</b> is how you roll out, manage, and update these services. You probably want to be able to <b>roll out gradually</b>, <b>roll back instantly</b>, and <b>deploy pipelines of models</b>. Many challenging infrastructure considerations go into this, but hopefully, your deployment library will take care of this for you.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-37ec2859e283497a836f4be65c847bbc" data-id="37ec2859e283497a836f4be65c847bbc"><span><div id="37ec2859e283497a836f4be65c847bbc" class="notion-header-anchor"></div><a class="notion-hash-link" href="#37ec2859e283497a836f4be65c847bbc" title="MANAGED OPTIONS"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MANAGED OPTIONS</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-31b5dfc2c14d4e49b6590435ac8c56ea"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage18.png?table=block&amp;id=31b5dfc2-c14d-4e49-b659-0435ac8c56ea&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-a58942ce7c2b43e3967114e97487888a">If you do not want to deal with any of the things mentioned thus far, there are managed options in the market. All major cloud providers have ones that enable you to package your model in a predefined way and turn it into an API. Startups like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://algorithmia.com/"><span class="notion-inline-underscore"><b>Algorithmia</b></span></a> and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.cortex.dev/"><span class="notion-inline-underscore"><b>Cortex</b></span></a> are some alternatives. The big drawback is that <b>pricing tends to be high, so you pay a premium fee in exchange for convenience</b>.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-142d18332cd14a369b2a55e2ad08b01c" data-id="142d18332cd14a369b2a55e2ad08b01c"><span><div id="142d18332cd14a369b2a55e2ad08b01c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#142d18332cd14a369b2a55e2ad08b01c" title="TAKEAWAYS"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>TAKEAWAYS</b></span></span></h4><ul class="notion-list notion-list-disc notion-block-435a4931279f45539f41f87d9a53bc55"><li>If you are making CPU inference, you can get away with scaling by launching more servers or going serverless.</li></ul><ul class="notion-list notion-list-disc notion-block-5e9ecfd7e0aa4561b1dce8eda97b0c28"><li>Serverless makes sense if you can get away with CPUs, and traffic is spiky or low-volume.</li></ul><ul class="notion-list notion-list-disc notion-block-0d1c832a4e89473485bf8fd6bead430b"><li>If you are using GPU inference, serving tools will save you time.</li></ul><ul class="notion-list notion-list-disc notion-block-ed05d6a36db94a9a8ff80e7d6efc6a61"><li>It’s worth keeping an eye on startups in this space for GPU inference.</li></ul><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-03facf93ce664b5f950e3815667fee5f" data-id="03facf93ce664b5f950e3815667fee5f"><span><div id="03facf93ce664b5f950e3815667fee5f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#03facf93ce664b5f950e3815667fee5f" title="3 - Edge Deployment"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>3 - Edge Deployment</b></span></span></h3><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-13ae46aaeb9548e4b48ad389b8c5f08f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage9.png?table=block&amp;id=13ae46aa-eb95-48e4-b48a-d389b8c5f08f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-e08f2df4fb9e4816b51905392e8a39d5">Edge prediction means that you first send the model weights to the client edge device. Then, the client loads the model and interacts with it directly.</div><div class="notion-text notion-block-a37cb27e96244d0c969693dca70e4abd">The pros of edge prediction:</div><ul class="notion-list notion-list-disc notion-block-f7cc6261f15a473faeb613a7d2f927b0"><li>It has low latency.</li></ul><ul class="notion-list notion-list-disc notion-block-a089f1c33b8f4b4297a2bcc6621d31bf"><li>It does not require an Internet connection.</li></ul><ul class="notion-list notion-list-disc notion-block-b7c2f2ac7cd749d197212271baec1292"><li>It satisfies data security requirements, as data does not need to leave the user’s device.</li></ul><div class="notion-text notion-block-fdabcd48e4a6442b9986b24143a06597">The cons of edge prediction:</div><ul class="notion-list notion-list-disc notion-block-c8932335ebbe4962903f407eecab16ee"><li>The client often has limited hardware resources available.</li></ul><ul class="notion-list notion-list-disc notion-block-2162416c0e614641b3ebc90781fec74a"><li>Embedded and mobile frameworks are less full-featured than TensorFlow and PyTorch.</li></ul><ul class="notion-list notion-list-disc notion-block-03923d546b0549e28e285db0865308b0"><li>It is challenging to update models.</li></ul><ul class="notion-list notion-list-disc notion-block-4bed221b6c7d4eb290a26062f6fb0765"><li>It is difficult to monitor and debug when things go wrong.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-4fd0827bb5b84fcfaa4c09043532cef0" data-id="4fd0827bb5b84fcfaa4c09043532cef0"><span><div id="4fd0827bb5b84fcfaa4c09043532cef0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4fd0827bb5b84fcfaa4c09043532cef0" title="TOOLS FOR EDGE DEPLOYMENT"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>TOOLS FOR EDGE DEPLOYMENT</b></span></span></h4><div class="notion-text notion-block-dc3f710f28c8427587ebc509ffd957f8"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://developer.nvidia.com/tensorrt"><span class="notion-inline-underscore"><b>TensorRT</b></span></a> is NVIDIA’s framework meant to help you optimize models for inference on NVIDIA devices in data centers and embedded/automotive environments. TensorRT is also integrated with application-specific SDKs to provide developers a unified path to deploy conversational AI, recommender, video conference, and streaming apps in production.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-653fc752537d4341a8c84c7c4f352e69"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage13.png?table=block&amp;id=653fc752-537d-4341-a8c8-4c7c4f352e69&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-855ab00b5a314608857e3a734714274d"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://tvm.apache.org/"><span class="notion-inline-underscore"><b>ApacheTVM</b></span></a> is an open-source machine learning compiler framework for CPUs, GPUs, and ML accelerators. It aims to enable ML engineers to optimize and run computations efficiently on any hardware backend. In particular, it compiles ML models into minimum deployable modules and provides the infrastructure to automatically optimize models on more backends with better performance.</div><div class="notion-text notion-block-e603a36820df49e6bf3f320fb151b178"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/lite/"><span class="notion-inline-underscore"><b>Tensorflow Lite</b></span></a> provides a trained TensorFlow model framework to be compressed and deployed to a mobile or embedded application. TensorFlow’s computationally expensive training process can still be performed in the environment that best suits it (personal server, cloud, overclocked computer). TensorFlow Lite then takes the resulting model (frozen graph, SavedModel, or HDF5 model) as input, packages, deploys, and then interprets it in the client application, handling the resource-conserving optimizations along the way.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-439f028d9e8c4396a12c6a79c0ecc40e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><img style="object-fit:contain" src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage3.png?table=block&amp;id=439f028d-9e8c-4396-a12c-6a79c0ecc40e&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-fcc121ff45e9401f82d894713b3b5b25"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pytorch.org/mobile/home/"><span class="notion-inline-underscore"><b>PyTorch Mobile</b></span></a> is a framework for helping mobile developers and machine learning engineers embed PyTorch models on-device. Currently, it allows any <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"><span class="notion-inline-underscore"><b>TorchScript model</b></span></a> to run directly inside iOS and Android applications. PyTorch Mobile’s initial release supports many different quantization techniques, which shrink model sizes without significantly affecting performance. PyTorch Mobile also allows developers to directly convert a PyTorch model to a mobile-ready format without needing to work through other tools/frameworks.</div><div class="notion-text notion-block-a7917beeb61940449342796764300b3e">JavaScript is a portable way of running code on different devices. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.tensorflow.org/js"><span class="notion-inline-underscore"><b>Tensorflow.js</b></span></a> enables you to run TensorFlow code in JavaScript. You can use off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js, retrain pre-existing ML models using your data, and build/train models directly in JavaScript using flexible and intuitive APIs.</div><div class="notion-text notion-block-c763fe051217405e8e2c66945ed1d0b8"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://developer.apple.com/documentation/coreml"><span class="notion-inline-underscore"><b>Core ML</b></span></a> was released by Apple back in 2017. It is optimized for on-device performance, which minimizes a model’s memory footprint and power consumption. Running strictly on the device also ensures that user data is kept secure. The app runs even in the absence of a network connection. Generally speaking, it is straightforward to use with just a few lines of code needed to integrate a complete ML model into your device. The downside is that you can only make the model inference, as no model training is possible.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-61be00383a2544d9b9ad47d543772d3f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><img style="object-fit:contain" src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage4.png?table=block&amp;id=61be0038-3a25-44d9-b9ad-47d543772d3f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-b736ce24e72043ec9a8c67c45e7ce659"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://firebase.google.com/docs/ml-kit/"><span class="notion-inline-underscore"><b>ML Kit</b></span></a> was announced by Google Firebase in 2018. It enables developers to utilize ML in mobile apps either with (1) inference in the cloud via API or (2) inference on-device (like Core ML). For the former option, ML Kit offers six base APIs with pertained models such as Image Labeling, Text Recognition, and Barcode Scanning. For the latter option, ML Kit offers lower accuracy but more security to user data, compared to the cloud version.</div><div class="notion-text notion-block-eb90d93329bd4420906736c2980d2526">If you are interested in either of the above options, check out <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://heartbeat.fritz.ai/core-ml-vs-ml-kit-which-mobile-machine-learning-framework-is-right-for-you-e25c5d34c765"><span class="notion-inline-underscore"><b>this comparison</b></span></a> by the FritzAI team. Additionally, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.fritz.ai/features/"><span class="notion-inline-underscore"><b>FritzAI</b></span></a> is an ML platform for mobile developers that provide pre-trained models, developer tools, and SDKs for iOS, Android, and Unity.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-8c9db52d464648099fd30eb6621e4b45" data-id="8c9db52d464648099fd30eb6621e4b45"><span><div id="8c9db52d464648099fd30eb6621e4b45" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8c9db52d464648099fd30eb6621e4b45" title="MORE EFFICIENT MODELS"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MORE EFFICIENT MODELS</b></span></span></h4><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-21728e6c05884111a96fcb1c9ee6cdb6"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><img style="object-fit:contain" src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage22.png?table=block&amp;id=21728e6c-0588-4111-a96f-cb1c9ee6cdb6&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-a1926dec593f4688b2885bc369085c74">Another thing to consider for edge deployment is to make the models more efficient. One way to do this is to use the same quantization and distillation techniques discussed above. Another way is to <b>pick mobile-friendly model architectures</b>. The first successful example is <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html"><span class="notion-inline-underscore"><b>MobileNet</b></span></a>, which performs various downsampling techniques to a traditional ConvNet architecture to maximize accuracy while being mindful of the restricted resources for a mobile or an embedded device. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d"><span class="notion-inline-underscore"><b>This analysis</b></span></a> by Yusuke Uchida explains why MobileNet and its variants are fast.</div><div class="notion-text notion-block-b0c90d046122452b99fda6a2736a4cdc">A well-known case study of applying knowledge distillation in practice is Hugging Face’s <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://medium.com/huggingface/distilbert-8cf3380435b5"><span class="notion-inline-underscore"><b>DistilBERT</b></span></a>, a smaller language model derived from the supervision of the popular <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/abs/1706.03762"><span class="notion-inline-underscore"><b>BERT</b></span></a> language model. DistilBERT removes the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of parameters of the BERT base and retains 95% of BERT’s performances on the language understanding benchmark GLUE.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-8e46de0ce9874d36868dce8851b5e104" data-id="8e46de0ce9874d36868dce8851b5e104"><span><div id="8e46de0ce9874d36868dce8851b5e104" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8e46de0ce9874d36868dce8851b5e104" title="MINDSET FOR EDGE DEPLOYMENT"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MINDSET FOR EDGE DEPLOYMENT</b></span></span></h4><ul class="notion-list notion-list-disc notion-block-6fa27d240e684839a6caf1a5f2a71f9e"><li>It is crucial to <b>choose your architecture with your target hardware in mind</b>. Specifically, you can make up a factor of 2-10 through distillation, quantization, and other tricks (but not more than that).</li></ul><ul class="notion-list notion-list-disc notion-block-616e1c75c4ba4f9ab997a0c0b5b4689a"><li>Once you have a model that works on your edge device, you can <b>iterate locally</b> as long as you add model size and latency to your metrics and avoid regression.</li></ul><ul class="notion-list notion-list-disc notion-block-85d954e8d2d54a8c8cf3795d4b9119d1"><li>You should treat <b>tuning the model for your device as an additional risk</b> in the deployment cycle and test it accordingly. In other words, you should always test your models on production hardware before deploying them for real.</li></ul><ul class="notion-list notion-list-disc notion-block-83166b5171534f7fb2f6879591f5d373"><li>Since models can be finicky, it’s a good idea to build <b>fallback mechanisms</b> into the application if the model fails or is too slow.</li></ul><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-554ae83d88e744e1837218e24f77385f" data-id="554ae83d88e744e1837218e24f77385f"><span><div id="554ae83d88e744e1837218e24f77385f" class="notion-header-anchor"></div><a class="notion-hash-link" href="#554ae83d88e744e1837218e24f77385f" title="TAKEAWAYS"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>TAKEAWAYS</b></span></span></h4><ul class="notion-list notion-list-disc notion-block-52078d2565f441cf9985010337a4c912"><li>Web deployment is easier, so only perform edge deployment if you need to.</li></ul><ul class="notion-list notion-list-disc notion-block-6a96fe5c618e47d48e9bb6c9526bc3ab"><li>You should choose your framework to match the available hardware and corresponding mobile frameworks. Else, you can try Apache TVM to be more flexible.</li></ul><ul class="notion-list notion-list-disc notion-block-e80b95f680be443e926ae8d6444ce711"><li>You should start considering hardware constraints at the beginning of the project and choose the architectures accordingly.</li></ul><h2 class="notion-h notion-h1 notion-h-indent-0 notion-block-2844e39d9ae44ac1a6e697b0d18ef31e" data-id="2844e39d9ae44ac1a6e697b0d18ef31e"><span><div id="2844e39d9ae44ac1a6e697b0d18ef31e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#2844e39d9ae44ac1a6e697b0d18ef31e" title="II - Model Monitoring"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title">II - Model Monitoring</span></span></h2><div class="notion-text notion-block-87f90e87b25b4f05be092e86963e8dff">Once you deploy models, how do you make sure they are staying healthy and working well? Enter model monitoring.</div><div class="notion-text notion-block-507cb72ab463499283f51fd36644cb4b">Many things can go wrong with a model once it’s been trained. This can happen even if your model has been trained properly, with a reasonable validation and test loss, as well as robust performance across various slices and quality predictions. Even after you’ve troubleshot and tested a model, things can still go wrong!</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-cc5896d316e84d5e8cc47461931ce191" data-id="cc5896d316e84d5e8cc47461931ce191"><span><div id="cc5896d316e84d5e8cc47461931ce191" class="notion-header-anchor"></div><a class="notion-hash-link" href="#cc5896d316e84d5e8cc47461931ce191" title="1 - Why Model Degrades Post-Deployment?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>1 - Why Model Degrades Post-Deployment?</b></span></span></h3><div class="notion-text notion-block-aa01a3dcd72e4a7884b4354bf423e2c5">Model performance tends to degrade after you’ve deployed a model. Why does this occur? In supervised learning, we seek to fit a function f to approximate a posterior using the data available to us. If any component of this process changes (i.e., the data x), the deployed model can see an unexpectedly degraded performance. See the below chart for examples of how such post-deployment degradations can occur theoretically and in practice:</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-9de2a6e222c04763bc40eae9e0035e29"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage17.png?table=block&amp;id=9de2a6e2-22c0-4763-bc40-eae9e0035e29&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-f4640d2760584750865eb8f229e92832">In summary, there are three core ways that the model’s performance can degrade: <b>data drift</b>, <b>concept drift</b>, and <b>domain shift</b>.</div><ol start="1" class="notion-list notion-list-numbered notion-block-70391bff2b8843db86d36027549e4c6d"><li>In data drift, the underlying data expectation that your model is built can unexpectedly change, perhaps through a bug in the upstream data pipeline or even due to malicious users feeding the model bad data.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-ca96fc806433422f9a57f161d284462b"><li>In concept drift, the actual outcome you seek to model, or the relationship between the data and the outcome, may fray. For example, users may start to pick movies in a different manner based on the output of your model, thereby changing the fundamental “concept” the model needs to approximate.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-5e4d29da3e544dc88c3ce24dae9fa3aa"><li>Finally, in domain shift, if your dataset does not appropriately sample the production, post-deployment setting, the model’s performance may suffer; this could be considered a “long tail” scenario, where many rare examples that are not present in the development data occur.</li></ol><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-c24b551459d34e13a95c309534326d7c" data-id="c24b551459d34e13a95c309534326d7c"><span><div id="c24b551459d34e13a95c309534326d7c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c24b551459d34e13a95c309534326d7c" title="2 - Data Drift"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2 - Data Drift</b></span></span></h3><div class="notion-text notion-block-621f8b5ec73b4a3ea870c68f2828f8b4">There are a few different types of data drift:</div><ul class="notion-list notion-list-disc notion-block-09841bb5bcf74ebbb8c03e215e14f8f3"><li><b>Instantaneous drift</b>: In this situation, the paradigm of the draft dramatically shifts. Examples are deploying the model in a new domain (e.g., self-driving car model in a new city), a bug in the preprocessing pipeline, or even major external shifts like COVID.</li></ul><ul class="notion-list notion-list-disc notion-block-316d42409a8243c3884e6f173e7449a4"><li><b>Gradual drift</b>: In this situation, the value of data gradually changes with time. For example, users’ preferences may change over time, or new concepts can get introduced to the domain.</li></ul><ul class="notion-list notion-list-disc notion-block-dc297cb1c4fc427c8841b4221c928e29"><li><b>Periodic drift</b>: Data can have fluctuating value due to underlying patterns like seasonality or time zones.</li></ul><ul class="notion-list notion-list-disc notion-block-c040053f7a4a427ca926e1556e879932"><li><b>Temporary drift</b>: The most difficult to detect, drift can occur through a short-term change in the data that shifts back to normal. This could be via a short-lived malicious attack, or even simply because a user with different demographics or behaviors uses your product in a way that it’s not designed to be used.</li></ul><div class="notion-text notion-block-92daa2b9267b4556888521437a03444c">While these categories may seem like purely academic categories, the consequences of data shift are very <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/"><span class="notion-inline-underscore"><b>real</b></span></a>. This is a real problem that affects many companies and is only now starting to get the attention it merits.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-4781243d0d9e404b85727e83cd1d7d48" data-id="4781243d0d9e404b85727e83cd1d7d48"><span><div id="4781243d0d9e404b85727e83cd1d7d48" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4781243d0d9e404b85727e83cd1d7d48" title="3 - What Should You Monitor?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>3 - What Should You Monitor?</b></span></span></h3><div class="notion-text notion-block-93c284f09c064a25bca685b296c18ef9">There are four core types of signals to monitor for machine learning models.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-94857d4a4b85476dac00a6ce4cec194f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage19.png?table=block&amp;id=94857d4a-4b85-476d-ac00-a6ce4cec194f&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-c2da6103866d4923bf3cad7e18fd5289">These metrics trade off with another in terms of how informative they are and how easy they are to access. <b>Put simply, the harder a metric may be to monitor, the more useful it likely is.</b></div><ul class="notion-list notion-list-disc notion-block-ce62509a41524406ae22e2c695a44d6a"><li>The hardest and best metrics to monitor are <b>model performance metrics</b>, though these can be difficult to acquire in real-time (labels are hard to come by).</li></ul><ul class="notion-list notion-list-disc notion-block-1a414ed0ed6049f388842bfa3b17d05c"><li><b>Business metrics</b> can be helpful signals of model degradation in monitoring but can easily be confounded by other impactful considerations.</li></ul><ul class="notion-list notion-list-disc notion-block-cb5ac27db9e34532939d270d93bea0cf"><li><b>Model inputs and predictions</b> are a simple way to identify high-level drift and are very easy to gather. Still, they can be difficult to assess in terms of actual performance impact, leaving it more of an art than science.</li></ul><ul class="notion-list notion-list-disc notion-block-92fcd2103edb4a9cbca368e1bc5dcb58"><li>Finally, <b>system performance</b> (e.g., GPU usage) can be a coarse method of catching serious bugs.</li></ul><div class="notion-text notion-block-d10b75f5cfcd4fa6b325eebc903b9402">In considering which metrics to focus on, prioritize ground-truth metrics (model and business metrics), then approximate performance metrics (business and input/outputs), and finally, system health metrics.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-1f2290cab6ca48d3a5719f623b67fa4c" data-id="1f2290cab6ca48d3a5719f623b67fa4c"><span><div id="1f2290cab6ca48d3a5719f623b67fa4c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1f2290cab6ca48d3a5719f623b67fa4c" title="4 - How Do You Measure Distribution Changes?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>4 - How Do You Measure Distribution Changes?</b></span></span></h3><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-cd6ad45e1211433496c7dbed8e46c67e" data-id="cd6ad45e1211433496c7dbed8e46c67e"><span><div id="cd6ad45e1211433496c7dbed8e46c67e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#cd6ad45e1211433496c7dbed8e46c67e" title="SELECT A REFERENCE WINDOW"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>SELECT A REFERENCE WINDOW</b></span></span></h4><div class="notion-text notion-block-760952115e3b4583a29ed70b6d4dcef9">To measure distribution changes in metrics you’re monitoring, start by picking a reference set of production data to compare new data to. There are a few different ways of picking this reference data (e.g., <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/abs/1908.04240"><span class="notion-inline-underscore"><b>sliding window</b></span></a> or fixed window of production data), but the most practical thing to do is <b>to use your training or evaluation data as the reference</b>. Data coming in looking different from what you developed your model using is an important signal to act on.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-f6451f3ee42048cfb2ba6e4aad23fd58" data-id="f6451f3ee42048cfb2ba6e4aad23fd58"><span><div id="f6451f3ee42048cfb2ba6e4aad23fd58" class="notion-header-anchor"></div><a class="notion-hash-link" href="#f6451f3ee42048cfb2ba6e4aad23fd58" title="SELECT A MEASUREMENT WINDOW"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>SELECT A MEASUREMENT WINDOW</b></span></span></h4><div class="notion-text notion-block-db5632627c384baf928be6bfda6cc44d">After picking a reference window, the next step is to choose a measurement window to compare, measure distance, and evaluate for drift. The challenge is that selecting a measurement window is highly problem-dependent. One solution is <b>to pick one or several window sizes and slide them over the data</b>. To avoid recomputing metrics over and over again, when you slide the window, it’s worth looking into <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.sketchingbigdata.org/"><span class="notion-inline-underscore"><b>the literature on mergeable (quantile) sketching algorithms</b></span></a>.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-e8a60f5ca18e4f268b877bf0147fc4cf" data-id="e8a60f5ca18e4f268b877bf0147fc4cf"><span><div id="e8a60f5ca18e4f268b877bf0147fc4cf" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e8a60f5ca18e4f268b877bf0147fc4cf" title="COMPARE WINDOWS USING A DISTANCE METRIC"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>COMPARE WINDOWS USING A DISTANCE METRIC</b></span></span></h4><div class="notion-text notion-block-b2dfec1a5e634a22841cf51ca03423df">What distance metrics should we use to compare the reference window to the measurement window? Some 1-D metric categories are:</div><ol start="1" class="notion-list notion-list-numbered notion-block-e9209cc354e045d8bfce80c7eff38c73"><li><b>Rule-based distance metrics</b> (e.g., data quality): Summary statistics, the volume of data points, number of missing values, or more complex tests like overall comparisons are common data quality checks that can be applied. <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://greatexpectations.io/"><span class="notion-inline-underscore"><b>Great Expectations</b></span></a> is a valuable library for this. <b>Definitely invest in simple rule-based metrics.</b> They catch a large number of bugs, as publications from Amazon and Google detail.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-5f14a81995ae4bd29ce05b1ba12e78c9"><li><b>Statistical distance metrics</b> (e.g., KS statistics, KL divergence, D_1 distance, etc.)</li><ol class="notion-list notion-list-numbered notion-block-5f14a81995ae4bd29ce05b1ba12e78c9"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://machinelearningmastery.com/divergence-between-probability-distributions/"><span class="notion-inline-underscore"><b>KL Divergence</b></span></a>: Defined as the expectation of a ratio of logs of two different distributions, this commonly known metric is very sensitive to what happens in the tails of the distribution. It’s not well-suited to data shift testing since it’s easily disturbed, is not interpretable, and struggles with data in different ranges.</li><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.statisticshowto.com/kolmogorov-smirnov-test/"><span class="notion-inline-underscore"><b>KS Statistic</b></span></a>: This metric is defined as the max distance between CDFs, which is easy to interpret and is thus used widely in practice. Say yes to the KS statistic!</li><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://mlsys.org/Conferences/2019/doc/2019/167.pdf"><span class="notion-inline-underscore"><b>D1 Distance</b></span></a>: Defined as the sum of distances between PDFs, this is a metric used at Google. Despite seeming less principled, it’s easily interpretable and has the added benefit of knowing Google uses it (so why not you?).</li></ol></ol><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-189df4e03a8a4992a98f902896b69a81"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage5.png?table=block&amp;id=189df4e0-3a8a-4992-a98f-902896b69a81&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-5619e9c676474bec8995b24a148f347d">An open area of research is understanding the impact of differing drift patterns on distance metrics and model performance. Another open area of research is high-dimensional distance metrics. Some options here are:</div><ol start="1" class="notion-list notion-list-numbered notion-block-208418e651b2425c90dae3bd4b890488"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://alex.smola.org/teaching/iconip2006/iconip_3.pdf"><span class="notion-inline-underscore"><b>Maximum mean discrepancy</b></span></a></li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-60188ef107ed40879ae6073630c0d74d"><li>Performing multiple 1D comparisons across the data: While suffering from <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://en.wikipedia.org/wiki/Bonferroni_correction"><span class="notion-inline-underscore"><b>the multiple hypothesis testing problem</b></span></a>, this is a practical approach.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-695c93f0c4cc47279de6870d0d445541"><li>Prioritize some features for 1D comparisons: Another option is to avoid testing all the features and only focus on those that merit comparison; for example, those features you know may have shifted in the data.</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-7a9ef50bbf234218bf71222e50c7773a"><li><a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arxiv.org/abs/1810.11953"><span class="notion-inline-underscore"><b>Projections</b></span></a>: In this approach, large data points are put through a dimensionality reduction process and then subject to a two-sample statistical test. Reducing the dimensionality with a domain-specific approach (e.g., mean pixel value for images, length of sentence) is recommended.</li></ol><div class="notion-text notion-block-21f3fec02e544f23a321c9d0fc3cb40e">At a high level, this entire distance metric work aims to identify not just a score for any data shift but also understand its impact on the model. While choosing a metric can be complicated with all the possible options, you should focus on understanding your model’s robustness in a post-deployment scenario.</div><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-7385695660314084b9599f0f6250bdb9" data-id="7385695660314084b9599f0f6250bdb9"><span><div id="7385695660314084b9599f0f6250bdb9" class="notion-header-anchor"></div><a class="notion-hash-link" href="#7385695660314084b9599f0f6250bdb9" title="5 - How Do You Tell If A Change Is Bad?"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>5 - How Do You Tell If A Change Is Bad?</b></span></span></h3><div class="notion-text notion-block-e382b926d7a9426d971187bcce1b73f0">There’s no hard and fast rule for finding if a change in the data is bad. An easy option is to set thresholds on the test values. Don’t use a statistical test like the KS test, as they are too sensitive to small shifts. Other <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://blog.anomalo.com/dynamic-data-testing-f831435dba90?gi=6c18774717d2"><span class="notion-inline-underscore"><b>options</b></span></a> include setting manual ranges, comparing values over time, or even applying an unsupervised model to detect outliers. In practice, fixed rules and specified ranges of test values are used most in practice.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-104ccb28b0e144a5b96fce9b3934eb0e"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage2.png?table=block&amp;id=104ccb28-b0e1-44a5-b96f-ce9b3934eb0e&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-6935acb9415a4d5fa6d8ff2db55cdb6e" data-id="6935acb9415a4d5fa6d8ff2db55cdb6e"><span><div id="6935acb9415a4d5fa6d8ff2db55cdb6e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#6935acb9415a4d5fa6d8ff2db55cdb6e" title="6 - Tools For Monitoring"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>6 - Tools For Monitoring</b></span></span></h3><div class="notion-text notion-block-b8f681f4541746488011856987dbacf2">There are three categories of tools useful for monitoring:</div><ol start="1" class="notion-list notion-list-numbered notion-block-5b8428014858429697cec08291d1d20c"><li><b>System monitoring tools</b> like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://aws.amazon.com/cloudwatch/"><span class="notion-inline-underscore"><b>AWS CloudWatch</b></span></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.datadoghq.com/"><span class="notion-inline-underscore"><b>Datadog</b></span></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://newrelic.com/"><span class="notion-inline-underscore"><b>New Relic</b></span></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.honeycomb.io/"><span class="notion-inline-underscore"><b>honeycomb</b></span></a> test traditional performance metrics</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-c74b3c09e6c245388339b81533a56431"><li><b>Data quality tools</b> like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://greatexpectations.io/"><span class="notion-inline-underscore"><b>Great Expectations</b></span></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.anomalo.com/"><span class="notion-inline-underscore"><b>Anomalo</b></span></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.montecarlodata.com/"><span class="notion-inline-underscore"><b>Monte Carlo</b></span></a> test if specific windows of data violate rules or assumptions.</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-e881e5c39f084e0c823b1076d5531d1e"><li><b>ML monitoring tools</b> like <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://arize.com/"><span class="notion-inline-underscore"><b>Arize</b></span></a>, <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.fiddler.ai/"><span class="notion-inline-underscore"><b>Fiddler</b></span></a>, and <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.arthur.ai/"><span class="notion-inline-underscore"><b>Arthur</b></span></a> can also be useful, as they specifically test models.</li></ol><h3 class="notion-h notion-h2 notion-h-indent-1 notion-block-33d5ee456f1a494d8cf35e0cf1457fdc" data-id="33d5ee456f1a494d8cf35e0cf1457fdc"><span><div id="33d5ee456f1a494d8cf35e0cf1457fdc" class="notion-header-anchor"></div><a class="notion-hash-link" href="#33d5ee456f1a494d8cf35e0cf1457fdc" title="7 - Evaluation Store"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>7 - Evaluation Store</b></span></span></h3><div class="notion-text notion-block-d442d06c0de1488e9dda63b064b45f76"><b>Monitoring is more central to ML than for traditional software</b>.</div><ul class="notion-list notion-list-disc notion-block-1a07382f0326452aa6e3cee764438677"><li>In traditional SWE, most bugs cause loud failures, and the data that is monitored is most valuable to detect and diagnose problems. If the system is working well, the data from these metrics and monitoring systems may not be useful.</li></ul><ul class="notion-list notion-list-disc notion-block-a8e89d16f19b4b51a6fa84ab17863857"><li>In machine learning, however, monitoring plays a different role. First off, bugs in ML systems often lead to silent degradations in performance. Furthermore, the data that is monitored in ML is literally the code used to train the next iteration of models.</li></ul><div class="notion-text notion-block-aac7cc3178df4257b3c28b8493876809">Because monitoring is so essential to ML systems, tightly integrating it into the ML system architecture brings major benefits. In particular, <b>better integrating and monitoring practices, or creating an evaluation store, can close the data flywheel loop</b>, a concept we talked about earlier in the class.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-42786fa4e82f41f5bc52d3480f84f5af"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><img src="https://www.notion.so/image/https%3A%2F%2Ffullstackdeeplearning.com%2Fspring2021%2Flecture-11-notes-media%2Fimage8.png?table=block&amp;id=42786fa4-e82f-41f5-bc52-d3480f84f5af&amp;cache=v2" loading="lazy" alt="notion image" decoding="async"/></div></figure><div class="notion-text notion-block-e194ed6e825547939018380b4d376582">As we build models, we create a mapping between data and model. As the data changes and we retrain models, monitoring these changes doesn’t become an endpoint--it becomes a part of the entire model development process. Monitoring, via an evaluation store, should touch all parts of your stack. One challenge that this process helps solve is effectively choosing which data points to collect, store, and label. Evaluation stores can help identify which data to collect more points for based on uncertain performance. As more data is collected and labeled, efficient retraining can be performed using evaluation store guidance.</div><h4 class="notion-h notion-h3 notion-h-indent-2 notion-block-8cd79d4bbc11490fb335d2f4ae8cffcf" data-id="8cd79d4bbc11490fb335d2f4ae8cffcf"><span><div id="8cd79d4bbc11490fb335d2f4ae8cffcf" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8cd79d4bbc11490fb335d2f4ae8cffcf" title="Conclusion"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>Conclusion</b></span></span></h4><div class="notion-text notion-block-6a9dc031a76d4c81ac24c0aee884b6c4">In summary, make sure to monitor your models!</div><ul class="notion-list notion-list-disc notion-block-63fc89c8e80849d687b0a6a5c9443c86"><li>Something will always go wrong, and you should have a system to catch errors.</li></ul><ul class="notion-list notion-list-disc notion-block-77936720d69447548d03c2d7e2ed58d4"><li>Start by looking at data quality metrics and system metrics, as they are easiest.</li></ul><ul class="notion-list notion-list-disc notion-block-eb4917bdaf5a4cd0aaecf3089c46a235"><li>In a perfect world, the testing and monitoring should be linked, and they should help you close the data flywheel.</li></ul><ul class="notion-list notion-list-disc notion-block-77c57bc2b5cb4a88a6ff6d2be8feaa93"><li>There will be a lot of tooling and research that will hopefully come soon!</li></ul></article><aside class="notion-aside"><div class="notion-aside-table-of-contents"><div class="notion-aside-table-of-contents-header">Table of Contents</div><nav class="notion-table-of-contents notion-gray"><a href="#ddbc4cae315042e880f6bf2779495134" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">I - Model Deployment</span></a><a href="#83c7b2de5e544cde888d46d2c855d301" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">1 - Types of Deployment</span></a><a href="#333f1ede6c7544518612375e874a9cc5" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">BATCH PREDICTION</span></a><a href="#2088ed0619ce479f8a1f892aed6c039a" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MODEL-IN-SERVICE</span></a><a href="#2712ad117a114c5484f2cf8267579000" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MODEL-AS-SERVICE</span></a><a href="#9d2e80af7319432199abc3832ebabad0" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">2 - Building A Model Service</span></a><a href="#daf1d36531824bbea42d060411578a80" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">REST APIS</span></a><a href="#82f062cac2a6479ca5c7137ff4727e63" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">DEPENDENCY MANAGEMENT</span></a><a href="#3e7aeb218ac74eef9b1394a791116ff9" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">ONNX</span></a><a href="#3d4e11ab622e4d00bfabe05d0d4ccf5e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Docker</span></a><a href="#5f49b85dc2064114bd666c3dc9f932d0" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">PERFORMANCE OPTIMIZATION</span></a><a href="#327a58795e804634888e3ee571639c5c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">GPU or no GPU?</span></a><a href="#9935640836d14dfdbc0d2e22414cda7e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Concurrency</span></a><a href="#5b7f0fe8f6944b458d3178a3f6049f61" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Model distillation</span></a><a href="#1f154196cd5340488ab17942f4fa1f8c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Model quantization</span></a><a href="#0827873e016f4e95a3948e105996859d" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Caching</span></a><a href="#011b1a5527fb4c0a85f31568c73124b5" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Batching</span></a><a href="#c458ce171fde416b9257a2b411b7fbe0" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Sharing The GPU</span></a><a href="#fb0310d20ac648e78a2966f97f1816be" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Model Serving Libraries</span></a><a href="#4aabd7a100904da4998e6ff08af33bef" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">HORIZONTAL SCALING</span></a><a href="#0243f2e88ee94afe826a5902ae0e0c65" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Container Orchestration</span></a><a href="#7d91fa632c1c4bff96a7f237a22e2d3e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Deploying Code As Serverless Functions</span></a><a href="#e1ba2c9dc70d48c18cb315528f6741c6" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MODEL DEPLOYMENT</span></a><a href="#37ec2859e283497a836f4be65c847bbc" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MANAGED OPTIONS</span></a><a href="#142d18332cd14a369b2a55e2ad08b01c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">TAKEAWAYS</span></a><a href="#03facf93ce664b5f950e3815667fee5f" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">3 - Edge Deployment</span></a><a href="#4fd0827bb5b84fcfaa4c09043532cef0" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">TOOLS FOR EDGE DEPLOYMENT</span></a><a href="#8c9db52d464648099fd30eb6621e4b45" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MORE EFFICIENT MODELS</span></a><a href="#8e46de0ce9874d36868dce8851b5e104" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">MINDSET FOR EDGE DEPLOYMENT</span></a><a href="#554ae83d88e744e1837218e24f77385f" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">TAKEAWAYS</span></a><a href="#2844e39d9ae44ac1a6e697b0d18ef31e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-0"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">II - Model Monitoring</span></a><a href="#cc5896d316e84d5e8cc47461931ce191" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">1 - Why Model Degrades Post-Deployment?</span></a><a href="#c24b551459d34e13a95c309534326d7c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">2 - Data Drift</span></a><a href="#4781243d0d9e404b85727e83cd1d7d48" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">3 - What Should You Monitor?</span></a><a href="#1f2290cab6ca48d3a5719f623b67fa4c" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">4 - How Do You Measure Distribution Changes?</span></a><a href="#cd6ad45e1211433496c7dbed8e46c67e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">SELECT A REFERENCE WINDOW</span></a><a href="#f6451f3ee42048cfb2ba6e4aad23fd58" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">SELECT A MEASUREMENT WINDOW</span></a><a href="#e8a60f5ca18e4f268b877bf0147fc4cf" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">COMPARE WINDOWS USING A DISTANCE METRIC</span></a><a href="#7385695660314084b9599f0f6250bdb9" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">5 - How Do You Tell If A Change Is Bad?</span></a><a href="#6935acb9415a4d5fa6d8ff2db55cdb6e" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">6 - Tools For Monitoring</span></a><a href="#33d5ee456f1a494d8cf35e0cf1457fdc" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-1"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:16px">7 - Evaluation Store</span></a><a href="#8cd79d4bbc11490fb335d2f4ae8cffcf" class="notion-table-of-contents-item notion-table-of-contents-item-indent-level-2"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:32px">Conclusion</span></a></nav></div><div class="PageSocial_pageSocial__2WqHl"><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://yihui-he.github.io" title="personal website" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="#000000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M3 9l9-7 9 7v11a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2z"></path><polyline points="9 22 9 12 15 12 15 22"></polyline></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_twitter__-BgFt" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_github__slQ0z" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></div></a><a class="PageSocial_action__2zgVt PageSocial_linkedin__nElHT" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><div class="PageSocial_actionBg__3CigO"><div class="PageSocial_actionBgPane__gbBkL"></div></div><div class="PageSocial_actionBg__3CigO"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"></path></svg></div></a></div></aside></div></main><footer class="styles_footer__1r_c6"><div class="styles_copyright__3kWHj">Copyright 2022 <!-- -->Yihui He</div><div class="styles_social__235gY"><a class="styles_twitter__WwfaA" href="https://yihui-he.github.io" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 576 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M280.37 148.26L96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47L488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"></path></svg></a><a class="styles_twitter__WwfaA" href="https://twitter.com/he_yi_hui" title="Twitter @he_yi_hui" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a class="styles_github__32xIr" href="https://github.com/yihui-he" title="GitHub @yihui-he" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__1XGvB" href="https://www.linkedin.com/in/yihui-he-a4257aab" title="LinkedIn Yihui He" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a></div></footer></div></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"yihui-he.github.io","name":"Yihui He","rootNotionPageId":"bd32b787e47149f38941174a9c6846b6","rootNotionSpaceId":null,"description":"Yihui He, AI research scientist / full stack engineer"},"recordMap":{"block":{"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a":{"role":"reader","value":{"id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","version":245,"type":"page","properties":{"title":[["Deployment \u0026 Monitoring",[["b"]]]]},"content":["d7bb06e2-5254-4821-b12a-8e0926bf1e3a","ddbc4cae-3150-42e8-80f6-bf2779495134","83c7b2de-5e54-4cde-888d-46d2c855d301","cc684769-b0e9-4358-a9cb-8eaee926eb54","de724687-97c4-441d-90a9-83d3da02736c","0bfc862e-8804-4479-a6b9-7b5ef742e9f6","81c258ec-7435-4713-9d2a-268d27e66cf7","333f1ede-6c75-4451-8612-375e874a9cc5","f14b4c1d-2ad3-4403-9c66-16229e4b93e1","fff58ca0-3c42-402e-a56a-f16a7a02fd1b","c9587f25-b11d-428d-b812-32b2e8c9189b","5ca91d14-17cc-470f-8d56-9dfe684dee40","5a931d5b-1bde-4211-8ca4-7f2e8ca8ce8c","c50cb623-abc8-4482-bf68-8a36994381cf","2a985fc4-ec60-4258-94f2-76d6d3f26ae5","e898ccf5-4f96-47e9-8aa8-ca1d7def8ed4","b907773c-efb0-493d-992e-047bb58fb7e6","2088ed06-19ce-479f-8a1f-892aed6c039a","11c5a784-7210-4209-951b-ab34d94c7167","cb900a31-bc29-4098-b592-022b3287fc70","e45fcdea-7468-4fdf-b499-417f56486800","69df3128-21be-40b7-93ef-a901113a59d8","3c875bab-7c94-495b-99bd-0a94cf797e64","177f9d4c-e717-48f0-b518-18c97ed1c6a3","76bc67d1-549e-41bd-a54e-2b42cb8ef769","c86dc35e-f3ba-4c2e-99be-a83dc3662e1e","3b0658fa-027c-40f9-bcaa-971ed65bfe91","3b90f5ef-5d05-4f17-9644-13ddb71b1f07","2712ad11-7a11-4c54-84f2-cf8267579000","6b7e81a0-9277-43f4-b490-c542045c4eeb","80caff59-1bfa-4ea4-bc75-738eabfada2f","324eeb94-a911-4760-9eb2-ee0a565d01b4","c75a2a6e-ad12-4c99-9895-d6735447a614","c874def9-bfe7-483e-b7ab-2d5ec003e4f3","18fbd66f-8dd1-4095-a7c8-2c5f64d98856","8709092a-9927-4366-ac8e-8e96a590265f","ed9769a0-4252-4dfd-bcf7-987a5aae155c","f38b998f-2d7c-49ca-8ce3-3106dfba461c","670352d2-449b-46ce-b283-80080b700f61","9d2e80af-7319-4321-99ab-c3832ebabad0","daf1d365-3182-4bbe-a42d-060411578a80","5d06a366-1aef-49b1-a8b4-67d8c8c61db5","47c92ea6-5e9f-4586-84c8-a3f0520b2854","82f062ca-c2a6-479c-a5c7-137ff4727e63","ecdf3542-f591-46a6-a3fb-a3feb5255597","a45a49aa-a6a8-49b4-be93-40608b637481","290a55ee-4512-4e9c-8afc-e7e78a78d487","86c8e652-6300-45f6-9fe9-f39d908a3a06","3e7aeb21-8ac7-4eef-9b13-94a791116ff9","b49ee304-69cb-485a-b78e-b06c3432a21f","a84bc0f0-1a44-458f-9319-39102af0aff0","3ad42750-d084-4434-b5fe-a8196c8bbbdf","a6a90205-2778-4e97-942f-b5ac3fb9a378","3d4e11ab-622e-4d00-bfab-e05d0d4ccf5e","508e8655-ba2a-4626-aee0-daddc8497e6f","77271b72-2ff8-4700-9302-c05c0ffe85fb","578a3100-e217-4d9a-9b13-cf4cf8a90b9b","209a714e-128a-44e9-abc1-5dc3234b2fe3","d74ab2dc-83b8-4c6b-afff-0a9dd40d837e","51ebf39d-1493-448c-9b65-caef9875e9d9","2a934e4d-c98e-4792-99ef-4be26cc5acee","8f767ef4-caa0-4232-9ff5-f8c3b417e3c2","c9abc232-dde5-46c9-bc31-d99e2c7a387c","b3c5d5b9-7f69-4920-b4f7-6a11c7e52c6c","815dfa4b-1599-4ca8-b92b-b449c9436b02","458d7108-3760-4d62-a296-3345f4ddc297","d57c561e-3d96-43ed-82d8-47de05f16651","03bb4095-2a5e-46e6-8cd8-b1a6a844770a","49afb1a6-78ee-44e9-bbc0-4dc271a83377","0df0ead7-403b-407e-9f0c-76f29db21369","0a1ad2a7-c0df-444a-b7bd-edab7173da06","5f49b85d-c206-4114-bd66-6c3dc9f932d0","3d65c20e-6e0a-40aa-a8b9-cb7a0092e6c5","de8fb983-febd-4c4d-ba03-165974e9c075","7bbbd7a1-ed40-4282-baba-7aaab3543edc","22477281-f00c-4b13-b25f-7226c3e2d695","a1b4d927-d235-44e6-9126-554a674d028b","327a5879-5e80-4634-888e-3ee571639c5c","042d9ee2-6596-4494-b447-82631248578d","d635c38b-97b3-4b49-af35-c9a2edc85222","858b27eb-2fee-4ce9-937e-8800808f5197","d7d6eb0e-5ddc-42bd-88d3-4cb722475103","3ae81a86-67db-433b-bd3c-bd80ac56acfe","a1cf0149-3b5a-408e-be55-9c2c21727c9c","99356408-36d1-4dfd-bc0d-2e22414cda7e","40f37f75-1281-4d20-bd82-041cc61c2157","5b7f0fe8-f694-4b45-8d31-78a3f6049f61","5661897e-16d9-4a60-b088-72e79203e72f","f8f31574-59c7-484d-bf68-db81b459618b","eb727e7a-f01d-4210-9df2-5f961da27855","1f154196-cd53-4048-8ab1-7942f4fa1f8c","29bc3743-4062-4840-bf72-efed40ed5209","76d5ed56-673e-4c10-b6f7-fb00bed8a9ee","ee9fe243-b041-4aab-be10-ba06a9cf3527","19342d26-a525-45f2-8733-5a8caded34f6","0827873e-016f-4e95-a394-8e105996859d","893e42ae-851b-43c8-a5e8-29d930c47a7f","8c7ee54e-64e3-4b2c-abb5-2ca0623faca6","06de6f97-d955-4729-bd86-2ee521467a85","011b1a55-27fb-4c0a-85f3-1568c73124b5","4109ef77-deea-4e58-94d6-7c92dddebde2","699a9dd4-2dc8-4e93-b001-6320b8175451","780cba65-6d5c-4745-894a-d12652b805fc","35f21fa1-fc6b-475e-bcf8-7d6f2f3cd81c","437d7cbc-e2a2-47c3-a611-2011d958700b","c458ce17-1fde-416b-9257-a2b411b7fbe0","6351b16f-1e25-4bf1-847d-e04fc8fe3017","fb0310d2-0ac6-48e7-8a29-66f97f1816be","691ed765-8f7b-4d46-9a5a-03ec63e9f698","3c663d3a-4d3f-428f-afd3-420c33fd1bb7","4aabd7a1-0090-4da4-998e-6ff08af33bef","8576978c-c82a-4948-832b-e59442cbd2c7","b2c82e45-28cb-4dbd-af4b-c1cc9397cda7","b8055fa6-450d-4334-97e2-bcff8ca2b4b0","0243f2e8-8ee9-4afe-826a-5902ae0e0c65","9a6ccb1c-b38d-4583-bf73-b1848f84ef60","190b2ac2-9e39-40dd-bb3e-5322bb7eb904","b8e49bc4-1ec6-4873-86cf-f8ed846adfa5","7d91fa63-2c1c-4bff-96a7-f237a22e2d3e","cb13bd66-8c0a-44db-8804-0d68b5f22654","b68fa00f-e7c7-42ab-bf29-f65b5dee9488","0c3ca592-da66-47e0-a59d-977fb46a61d5","4830cfc0-9317-46c5-8f3c-3f9802d42932","4a0ae5ef-f624-4ca8-bd8a-02e558f63814","09e52cb8-a8da-4fa8-925e-15f701bd56ee","85433715-44ae-4184-9596-3a38c67249ed","8b5e0ebc-534b-402a-b9e7-9128d90e430d","e1ba2c9d-c70d-48c1-8cb3-15528f6741c6","fee9cd05-1e0e-4ba7-b774-0cb7132b9179","37ec2859-e283-497a-836f-4be65c847bbc","31b5dfc2-c14d-4e49-b659-0435ac8c56ea","a58942ce-7c2b-43e3-9671-14e97487888a","142d1833-2cd1-4a36-9b2a-55e2ad08b01c","435a4931-279f-4553-9f41-f87d9a53bc55","5e9ecfd7-e0aa-4561-b1dc-e8eda97b0c28","0d1c832a-4e89-4734-85bf-8fd6bead430b","ed05d6a3-6db9-4a9a-8ff8-0e7d6efc6a61","03facf93-ce66-4b5f-950e-3815667fee5f","13ae46aa-eb95-48e4-b48a-d389b8c5f08f","e08f2df4-fb9e-4816-b519-05392e8a39d5","a37cb27e-9624-4d0c-9696-93dca70e4abd","f7cc6261-f15a-473f-aeb6-13a7d2f927b0","a089f1c3-3b8f-4b42-97a2-bcc6621d31bf","b7c2f2ac-7cd7-49d1-9721-2271baec1292","fdabcd48-e4a6-442b-9986-b24143a06597","c8932335-ebbe-4962-903f-407eecab16ee","2162416c-0e61-4641-b3eb-c90781fec74a","03923d54-6b05-49e2-8e28-5db0865308b0","4bed221b-6c7d-4eb2-90a2-6062f6fb0765","4fd0827b-b5b8-4fcf-aa4c-09043532cef0","dc3f710f-28c8-4275-87eb-c509ffd957f8","653fc752-537d-4341-a8c8-4c7c4f352e69","855ab00b-5a31-4608-857e-3a734714274d","e603a368-20df-49e6-bf3f-320fb151b178","439f028d-9e8c-4396-a12c-6a79c0ecc40e","fcc121ff-45e9-401f-82d8-94713b3b5b25","a7917bee-b619-4044-9342-796764300b3e","c763fe05-1217-405e-8e2c-66945ed1d0b8","61be0038-3a25-44d9-b9ad-47d543772d3f","b736ce24-e720-43ec-9a8c-67c45e7ce659","eb90d933-29bd-4420-9067-36c2980d2526","8c9db52d-4646-4809-9fd3-0eb6621e4b45","21728e6c-0588-4111-a96f-cb1c9ee6cdb6","a1926dec-593f-4688-b288-5bc369085c74","b0c90d04-6122-452b-99fd-a6a2736a4cdc","8e46de0c-e987-4d36-868d-ce8851b5e104","6fa27d24-0e68-4839-a6ca-f1a5f2a71f9e","616e1c75-c4ba-4f9a-b997-a0c0b5b4689a","85d954e8-d2d5-4a8c-8cf3-795d4b9119d1","83166b51-7153-4f7f-b2f6-879591f5d373","554ae83d-88e7-44e1-8372-18e24f77385f","52078d25-65f4-41cf-9985-010337a4c912","6a96fe5c-618e-47d4-8e9b-b6c9526bc3ab","e80b95f6-80be-443e-926a-e8d6444ce711","2844e39d-9ae4-4ac1-a6e6-97b0d18ef31e","87f90e87-b25b-4f05-be09-2e86963e8dff","507cb72a-b463-4992-83f5-1fd36644cb4b","cc5896d3-16e8-4d5e-8cc4-7461931ce191","aa01a3dc-d72e-4a78-84b4-354bf423e2c5","9de2a6e2-22c0-4763-bc40-eae9e0035e29","f4640d27-6058-4750-865e-b8f229e92832","70391bff-2b88-43db-86d3-6027549e4c6d","ca96fc80-6433-422f-9a57-f161d284462b","5e4d29da-3e54-4dc8-8c3c-e24dae9fa3aa","c24b5514-59d3-4e13-a95c-309534326d7c","621f8b5e-c73b-4a3e-a870-c68f2828f8b4","09841bb5-bcf7-4ebb-b8c0-3e215e14f8f3","316d4240-9a82-43c3-884e-6f173e7449a4","dc297cb1-c4fc-427c-8841-b4221c928e29","c040053f-7a4a-427c-a926-e1556e879932","92daa2b9-267b-4556-8885-21437a03444c","4781243d-0d9e-404b-8572-7e83cd1d7d48","93c284f0-9c06-4a25-bca6-85b296c18ef9","94857d4a-4b85-476d-ac00-a6ce4cec194f","c2da6103-866d-4923-bf3c-ad7e18fd5289","ce62509a-4152-4406-ae22-e2c695a44d6a","1a414ed0-ed60-49f3-8884-2bfa3b17d05c","cb5ac27d-b9e3-4532-939d-270d93bea0cf","92fcd210-3edb-4a9c-bca3-68e1bc5dcb58","d10b75f5-cfcd-4fa6-b325-eebc903b9402","1f2290ca-b6ca-48d3-a571-9f623b67fa4c","cd6ad45e-1211-4334-96c7-dbed8e46c67e","76095211-5e3b-4583-a29e-d70b6d4dcef9","f6451f3e-e420-48cf-b2ba-6e4aad23fd58","db563262-7c38-4baf-928b-e6bfda6cc44d","e8a60f5c-a18e-4f26-8b87-7bf0147fc4cf","b2dfec1a-5e63-4a22-841c-f51ca03423df","e9209cc3-54e0-45d8-bfce-80c7eff38c73","5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9","189df4e0-3a8a-4992-a98f-902896b69a81","5619e9c6-7647-4bec-8995-b24a148f347d","208418e6-51b2-425c-90da-e3bd4b890488","60188ef1-07ed-4087-9ae6-073630c0d74d","695c93f0-c4cc-4727-9de6-870d0d445541","7a9ef50b-bf23-4218-bf71-222e50c7773a","21f3fec0-2e54-4f23-a321-c9d0fc3cb40e","73856956-6031-4084-b959-9f0f6250bdb9","e382b926-d7a9-426d-9711-87bcce1b73f0","104ccb28-b0e1-44a5-b96f-ce9b3934eb0e","6935acb9-415a-4d5f-a6d8-ff2db55cdb6e","b8f681f4-5417-4648-8011-856987dbacf2","5b842801-4858-4296-97ce-c08291d1d20c","c74b3c09-e6c2-4538-8339-b81533a56431","e881e5c3-9f08-4e0c-823b-1076d5531d1e","33d5ee45-6f1a-494d-8cf3-5e0cf1457fdc","d442d06c-0de1-488e-9dda-63b064b45f76","1a07382f-0326-452a-a6e3-cee764438677","a8e89d16-f19b-4b51-a6fa-84ab17863857","aac7cc31-78df-4257-b3c2-8b8493876809","42786fa4-e82f-41f5-bc52-d3480f84f5af","e194ed6e-8255-4793-9018-380b4d376582","8cd79d4b-bc11-490f-b335-d2f4ae8cffcf","6a9dc031-a76d-4c81-ac24-c0aee884b6c4","63fc89c8-e808-49d6-87b0-a6a5c9443c86","77936720-d694-4754-8d03-c2d7e2ed58d4","eb4917bd-af5a-4cd0-aaec-f3089c46a235","77c57bc2-b5cb-4a88-a6ff-6d2be8feaa93"],"created_time":1646024820000,"last_edited_time":1646024940000,"parent_id":"41249f3f-76a2-458a-b13d-2bc06310337b","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"41249f3f-76a2-458a-b13d-2bc06310337b":{"role":"reader","value":{"id":"41249f3f-76a2-458a-b13d-2bc06310337b","version":205,"type":"page","properties":{"title":[["deep learning engineer manual"]]},"content":["fcd4d688-6b0e-4ee7-8838-53a4d48a0af4","0b626c07-cbba-41a8-9d08-978d41ee2200","e9d171d8-17bc-4aa6-88f7-79391cf072d1","6dd6d37e-aa69-45af-8c37-4bb14f716fa2","94cea945-881b-4d6b-be5c-505834b715db","a1ce7ac8-2778-42ea-b8fb-f03493372730","4f6aae13-77aa-40ad-a5fb-7db5b90c3ae3","f9e18efc-827a-4f89-a8fc-e8975dcadc02","3476e08f-8137-4d53-a65f-9f0ea5c2ccf0","49123afb-c329-487d-afa0-3c5efe8cb545","45eddebe-91be-4903-b581-278254e0b525","d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","bd908e48-f0ff-4c31-b2c8-095924adfaf7","032e90f7-6a0b-43bd-ad11-65169e05639c","92d36485-cfb3-4100-9fa9-567395e4385c","4ffbb2cd-f634-421a-9e5a-9f2758193609"],"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"}],"created_time":1645559700000,"last_edited_time":1646026920000,"parent_id":"bd32b787-e471-49f3-8941-174a9c6846b6","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"bd32b787-e471-49f3-8941-174a9c6846b6":{"role":"reader","value":{"id":"bd32b787-e471-49f3-8941-174a9c6846b6","version":292,"type":"page","properties":{"title":[["Yihui He’s Blog"]]},"content":["cc62526f-e4eb-42e5-8e8f-a22c3abd7b46","eb534ff0-d0e6-4c83-b817-92c761ad9034","07cdf7d1-24c9-4fd7-abc0-e1f79f38aafd","41249f3f-76a2-458a-b13d-2bc06310337b","09ac442d-d3b5-4f43-9531-90cdae7640b7","76f263cc-0736-4657-91dd-551b1c541b4c","fad54c70-af5c-42a3-a00a-ff18bea9c382","9abe926f-8b36-4a60-a77e-e5436690a8c0","84c674a7-bbe3-4ac5-88c7-68e0c3909585","09517098-a843-437e-bb7a-4ac67866b29b","e7a12f56-b48c-43bf-803e-96d70518685f","932f5707-ab83-4542-af11-68fec7b5f28d","e6311219-0ef8-4142-8a3f-eeedc910dd3b","5f16dda4-316e-43af-a4ea-8fc6d6938404","23859d82-2201-4bb6-a86a-db693ea4992a","d5a94597-8cb4-43be-a6d0-0840b5518710","c54caf49-d28c-46ac-b72d-fc9bfa58bbdd","4751172a-1cb9-4633-84d6-21ba334ee3d7","8e23ed11-04f5-48ab-a95d-1f47a5cd22af","4b2fb8ba-1e4c-47ff-903b-0869360db746","010faf71-b9e5-4c47-832a-658dafc6e7ae","3db8e8fd-7383-4aea-8beb-608282db050e","60ff1c6d-fde5-426d-ac57-7bba2a5296a2","31008559-3d6f-4c33-bb50-4e463c9b19fb","201efca9-ba21-47c3-8b0d-35b41dd3f91f","04ced6eb-ca82-46db-8f8e-077f3bf6b29c","daf31581-086b-4fb1-9695-5a79117d4300","a0487f2c-eed0-4154-a6e8-2366ca1d7939","95728d67-dc1f-4086-881b-1cd1bf72f985","cba2b20a-aa13-466c-bcb6-a9f9c9f057da","5e0b979d-ef00-44a3-894f-38d28cfd7576","38a364d6-720a-4788-aeee-c2b224745860","6a838606-233d-4584-b2f1-3d198b1603e8","f93f1e88-d826-4cd3-ab2a-cd92ddd2ac53","e8156bf2-5303-4590-a44a-6db1404c8c7b","2411768d-af34-4f88-8772-c46ca4428256","52357a78-3241-4a0a-bcb7-46bfd15070a4","3f4c64fb-75dc-4ccd-9e22-ce5704d0dfdb","d1ef1d4d-bfa8-40c4-a78c-037da96f3681","8d0bc47c-6aaf-4ac2-ae09-a6830f9ad50c","8f822963-88c4-4e82-8eac-8d0f8b606895","7c5f4cc9-b8a0-498e-8484-ccc1dbf41912","0ef9110d-6766-4f69-9412-586d40d755a9"],"discussions":["a13be18e-5008-4f8c-b07a-0fadb99d7450"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63e89fe9-f5cd-4414-be0e-53d91aa54fc3/me_small.jpg"},"permissions":[{"role":"editor","type":"user_permission","user_id":"9e9c4442-4350-473a-b900-c954b0bd7a95"},{"role":"reader","type":"public_permission","added_timestamp":1645466303668}],"created_time":1645422720000,"last_edited_time":1646636220000,"parent_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1","parent_table":"space","alive":true,"file_ids":["e3d85759-49f2-40d2-ba30-d25ca401872e","cfc564cc-91ae-4a96-a912-fcc9a28ec5db","63e89fe9-f5cd-4414-be0e-53d91aa54fc3"],"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d7bb06e2-5254-4821-b12a-8e0926bf1e3a":{"role":"reader","value":{"id":"d7bb06e2-5254-4821-b12a-8e0926bf1e3a","version":2,"type":"text","properties":{"title":[["ML in production scales to meet users’ demands by delivering thousands to millions of predictions per second. On the other hand, models in notebooks only work if you run the cells in the right order. To be frank, "],["most data scientists and ML engineers do not know how to build production ML systems.",[["b"]]],[" Therefore, the goal of this lecture is to give you different flavors of accomplishing that task."]]},"created_time":1646024849921,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ddbc4cae-3150-42e8-80f6-bf2779495134":{"role":"reader","value":{"id":"ddbc4cae-3150-42e8-80f6-bf2779495134","version":7,"type":"header","properties":{"title":[["I - Model Deployment"]]},"created_time":1646024849921,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"83c7b2de-5e54-4cde-888d-46d2c855d301":{"role":"reader","value":{"id":"83c7b2de-5e54-4cde-888d-46d2c855d301","version":8,"type":"sub_header","properties":{"title":[["1 - Types of Deployment",[["b"]]]]},"created_time":1646024849921,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cc684769-b0e9-4358-a9cb-8eaee926eb54":{"role":"reader","value":{"id":"cc684769-b0e9-4358-a9cb-8eaee926eb54","version":2,"type":"text","properties":{"title":[["One way to conceptualize different approaches to deploy ML models is to think about where to deploy them in your application’s overall architecture."]]},"created_time":1646024849921,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"de724687-97c4-441d-90a9-83d3da02736c":{"role":"reader","value":{"id":"de724687-97c4-441d-90a9-83d3da02736c","version":2,"type":"bulleted_list","properties":{"title":[["The "],["client-side",[["b"]]],[" runs locally on the user machine (web browser, mobile devices, etc..)"]]},"created_time":1646024849922,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0bfc862e-8804-4479-a6b9-7b5ef742e9f6":{"role":"reader","value":{"id":"0bfc862e-8804-4479-a6b9-7b5ef742e9f6","version":2,"type":"bulleted_list","properties":{"title":[["It connects to the "],["server-side",[["b"]]],[" that runs your code remotely."]]},"created_time":1646024849922,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"81c258ec-7435-4713-9d2a-268d27e66cf7":{"role":"reader","value":{"id":"81c258ec-7435-4713-9d2a-268d27e66cf7","version":2,"type":"bulleted_list","properties":{"title":[["The server connects with a "],["database",[["b"]]],[" to pull data out, render the data, and show the data to the user."]]},"created_time":1646024849922,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"333f1ede-6c75-4451-8612-375e874a9cc5":{"role":"reader","value":{"id":"333f1ede-6c75-4451-8612-375e874a9cc5","version":2,"type":"sub_sub_header","properties":{"title":[["BATCH PREDICTION",[["b"]]]]},"created_time":1646024849922,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f14b4c1d-2ad3-4403-9c66-16229e4b93e1":{"role":"reader","value":{"id":"f14b4c1d-2ad3-4403-9c66-16229e4b93e1","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image16.png"]]},"created_time":1646024849922,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fff58ca0-3c42-402e-a56a-f16a7a02fd1b":{"role":"reader","value":{"id":"fff58ca0-3c42-402e-a56a-f16a7a02fd1b","version":2,"type":"text","properties":{"title":[["Batch prediction means that you train the models offline, dump the results into a database, then run the rest of the application normally. You periodically run your model on new data coming in and cache the results in a database. Batch prediction is commonly used in production when the universe of inputs is relatively small (e.g., one prediction per user per day)."]]},"created_time":1646024849934,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c9587f25-b11d-428d-b812-32b2e8c9189b":{"role":"reader","value":{"id":"c9587f25-b11d-428d-b812-32b2e8c9189b","version":2,"type":"text","properties":{"title":[["The pros of batch prediction:"]]},"created_time":1646024849934,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5ca91d14-17cc-470f-8d56-9dfe684dee40":{"role":"reader","value":{"id":"5ca91d14-17cc-470f-8d56-9dfe684dee40","version":2,"type":"bulleted_list","properties":{"title":[["It is simple to implement."]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5a931d5b-1bde-4211-8ca4-7f2e8ca8ce8c":{"role":"reader","value":{"id":"5a931d5b-1bde-4211-8ca4-7f2e8ca8ce8c","version":2,"type":"bulleted_list","properties":{"title":[["It requires relatively low latency to the user."]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c50cb623-abc8-4482-bf68-8a36994381cf":{"role":"reader","value":{"id":"c50cb623-abc8-4482-bf68-8a36994381cf","version":2,"type":"text","properties":{"title":[["The cons of batch prediction:"]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2a985fc4-ec60-4258-94f2-76d6d3f26ae5":{"role":"reader","value":{"id":"2a985fc4-ec60-4258-94f2-76d6d3f26ae5","version":2,"type":"bulleted_list","properties":{"title":[["It does not scale to complex input types."]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e898ccf5-4f96-47e9-8aa8-ca1d7def8ed4":{"role":"reader","value":{"id":"e898ccf5-4f96-47e9-8aa8-ca1d7def8ed4","version":2,"type":"bulleted_list","properties":{"title":[["Users do not get the most up-to-date predictions."]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b907773c-efb0-493d-992e-047bb58fb7e6":{"role":"reader","value":{"id":"b907773c-efb0-493d-992e-047bb58fb7e6","version":2,"type":"bulleted_list","properties":{"title":[["Models frequently become “stale” and hard to detect."]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2088ed06-19ce-479f-8a1f-892aed6c039a":{"role":"reader","value":{"id":"2088ed06-19ce-479f-8a1f-892aed6c039a","version":2,"type":"sub_sub_header","properties":{"title":[["MODEL-IN-SERVICE",[["b"]]]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"11c5a784-7210-4209-951b-ab34d94c7167":{"role":"reader","value":{"id":"11c5a784-7210-4209-951b-ab34d94c7167","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image11.png"]]},"created_time":1646024849935,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cb900a31-bc29-4098-b592-022b3287fc70":{"role":"reader","value":{"id":"cb900a31-bc29-4098-b592-022b3287fc70","version":2,"type":"text","properties":{"title":[["Model-in-service means that you package up your model and include it in the deployed web server. Then, the web server loads the model and calls it to make predictions."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e45fcdea-7468-4fdf-b499-417f56486800":{"role":"reader","value":{"id":"e45fcdea-7468-4fdf-b499-417f56486800","version":2,"type":"text","properties":{"title":[["The pros of model-in-service prediction:"]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"69df3128-21be-40b7-93ef-a901113a59d8":{"role":"reader","value":{"id":"69df3128-21be-40b7-93ef-a901113a59d8","version":2,"type":"bulleted_list","properties":{"title":[["It reuses your existing infrastructure."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3c875bab-7c94-495b-99bd-0a94cf797e64":{"role":"reader","value":{"id":"3c875bab-7c94-495b-99bd-0a94cf797e64","version":2,"type":"text","properties":{"title":[["The cons of model-in-service prediction:"]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"177f9d4c-e717-48f0-b518-18c97ed1c6a3":{"role":"reader","value":{"id":"177f9d4c-e717-48f0-b518-18c97ed1c6a3","version":2,"type":"bulleted_list","properties":{"title":[["The web server may be written in a different language."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"76bc67d1-549e-41bd-a54e-2b42cb8ef769":{"role":"reader","value":{"id":"76bc67d1-549e-41bd-a54e-2b42cb8ef769","version":2,"type":"bulleted_list","properties":{"title":[["Models may change more frequently than the server code."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c86dc35e-f3ba-4c2e-99be-a83dc3662e1e":{"role":"reader","value":{"id":"c86dc35e-f3ba-4c2e-99be-a83dc3662e1e","version":2,"type":"bulleted_list","properties":{"title":[["Large models can eat into the resources for your webserver."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3b0658fa-027c-40f9-bcaa-971ed65bfe91":{"role":"reader","value":{"id":"3b0658fa-027c-40f9-bcaa-971ed65bfe91","version":2,"type":"bulleted_list","properties":{"title":[["Server hardware is not optimized for your model (e.g., no GPUs)."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3b90f5ef-5d05-4f17-9644-13ddb71b1f07":{"role":"reader","value":{"id":"3b90f5ef-5d05-4f17-9644-13ddb71b1f07","version":2,"type":"bulleted_list","properties":{"title":[["Model and server may scale differently."]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2712ad11-7a11-4c54-84f2-cf8267579000":{"role":"reader","value":{"id":"2712ad11-7a11-4c54-84f2-cf8267579000","version":2,"type":"sub_sub_header","properties":{"title":[["MODEL-AS-SERVICE",[["b"]]]]},"created_time":1646024849936,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6b7e81a0-9277-43f4-b490-c542045c4eeb":{"role":"reader","value":{"id":"6b7e81a0-9277-43f4-b490-c542045c4eeb","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image14.png"]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"80caff59-1bfa-4ea4-bc75-738eabfada2f":{"role":"reader","value":{"id":"80caff59-1bfa-4ea4-bc75-738eabfada2f","version":2,"type":"text","properties":{"title":[["Model-as-service means that you deploy the model separately as its own service. The client and server can interact with the model by making requests to the model service and receiving responses."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"324eeb94-a911-4760-9eb2-ee0a565d01b4":{"role":"reader","value":{"id":"324eeb94-a911-4760-9eb2-ee0a565d01b4","version":2,"type":"text","properties":{"title":[["The pros of model-as-service prediction:"]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c75a2a6e-ad12-4c99-9895-d6735447a614":{"role":"reader","value":{"id":"c75a2a6e-ad12-4c99-9895-d6735447a614","version":2,"type":"bulleted_list","properties":{"title":[["It is dependable, as model bugs are less likely to crash the web app."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c874def9-bfe7-483e-b7ab-2d5ec003e4f3":{"role":"reader","value":{"id":"c874def9-bfe7-483e-b7ab-2d5ec003e4f3","version":2,"type":"bulleted_list","properties":{"title":[["It is scalable, as you can choose the optimal hardware for the model and scale it appropriately."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"18fbd66f-8dd1-4095-a7c8-2c5f64d98856":{"role":"reader","value":{"id":"18fbd66f-8dd1-4095-a7c8-2c5f64d98856","version":2,"type":"bulleted_list","properties":{"title":[["It is flexible, as you can easily reuse the model across multiple applications."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8709092a-9927-4366-ac8e-8e96a590265f":{"role":"reader","value":{"id":"8709092a-9927-4366-ac8e-8e96a590265f","version":2,"type":"text","properties":{"title":[["The cons of model-as-service prediction:"]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ed9769a0-4252-4dfd-bcf7-987a5aae155c":{"role":"reader","value":{"id":"ed9769a0-4252-4dfd-bcf7-987a5aae155c","version":2,"type":"bulleted_list","properties":{"title":[["It adds latency."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f38b998f-2d7c-49ca-8ce3-3106dfba461c":{"role":"reader","value":{"id":"f38b998f-2d7c-49ca-8ce3-3106dfba461c","version":2,"type":"bulleted_list","properties":{"title":[["It adds infrastructural complexity."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"670352d2-449b-46ce-b283-80080b700f61":{"role":"reader","value":{"id":"670352d2-449b-46ce-b283-80080b700f61","version":2,"type":"bulleted_list","properties":{"title":[["Most importantly, you are now on the hook to run a model service..."]]},"created_time":1646024849937,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9d2e80af-7319-4321-99ab-c3832ebabad0":{"role":"reader","value":{"id":"9d2e80af-7319-4321-99ab-c3832ebabad0","version":8,"type":"sub_header","properties":{"title":[["2 - Building A Model Service",[["b"]]]]},"created_time":1646024849938,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"daf1d365-3182-4bbe-a42d-060411578a80":{"role":"reader","value":{"id":"daf1d365-3182-4bbe-a42d-060411578a80","version":2,"type":"sub_sub_header","properties":{"title":[["REST APIS",[["b"]]]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5d06a366-1aef-49b1-a8b4-67d8c8c61db5":{"role":"reader","value":{"id":"5d06a366-1aef-49b1-a8b4-67d8c8c61db5","version":2,"type":"text","properties":{"title":[["REST APIs",[["b"],["_"],["a","https://www.sitepoint.com/developers-rest-api/"]]],[" represent a way of serving predictions in response to canonically formatted HTTP requests. There are alternatives such as "],["gRPC",[["b"],["_"],["a","https://grpc.io/"]]],[" and "],["GraphQL",[["b"],["_"],["a","https://graphql.org/"]]],[". For instance, in your command line, you can use "],["curl",[["i"]]],[" to post some data to an URL and get back JSON that contains the model predictions."]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"47c92ea6-5e9f-4586-84c8-a3f0520b2854":{"role":"reader","value":{"id":"47c92ea6-5e9f-4586-84c8-a3f0520b2854","version":2,"type":"text","properties":{"title":[["Sadly, there is no standard way of formatting the data that goes into an ML model."]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"82f062ca-c2a6-479c-a5c7-137ff4727e63":{"role":"reader","value":{"id":"82f062ca-c2a6-479c-a5c7-137ff4727e63","version":2,"type":"sub_sub_header","properties":{"title":[["DEPENDENCY MANAGEMENT",[["b"]]]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ecdf3542-f591-46a6-a3fb-a3feb5255597":{"role":"reader","value":{"id":"ecdf3542-f591-46a6-a3fb-a3feb5255597","version":2,"type":"text","properties":{"title":[["Model predictions depend on the "],["code",[["b"]]],[", the "],["model weights",[["b"]]],[", and the "],["code dependencies",[["b"]]],[". All three need to be present on your webserver. For code and model weights, you can simply copy them locally (or write a script to extract them if they are large). But dependencies are trickier because they cause troubles. As they are hard to make consistent and update, your model behavior might change accordingly."]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a45a49aa-a6a8-49b4-be93-40608b637481":{"role":"reader","value":{"id":"a45a49aa-a6a8-49b4-be93-40608b637481","version":2,"type":"text","properties":{"title":[["There are two high-level strategies to manage code dependencies:"]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"290a55ee-4512-4e9c-8afc-e7e78a78d487":{"role":"reader","value":{"id":"290a55ee-4512-4e9c-8afc-e7e78a78d487","version":2,"type":"numbered_list","properties":{"title":[["You constrain the dependencies of your model."]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"86c8e652-6300-45f6-9fe9-f39d908a3a06":{"role":"reader","value":{"id":"86c8e652-6300-45f6-9fe9-f39d908a3a06","version":2,"type":"numbered_list","properties":{"title":[["You use containers."]]},"created_time":1646024849938,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3e7aeb21-8ac7-4eef-9b13-94a791116ff9":{"role":"reader","value":{"id":"3e7aeb21-8ac7-4eef-9b13-94a791116ff9","version":2,"type":"sub_sub_header","properties":{"title":[["ONNX",[["b"]]]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b49ee304-69cb-485a-b78e-b06c3432a21f":{"role":"reader","value":{"id":"b49ee304-69cb-485a-b78e-b06c3432a21f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image15.png"]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a84bc0f0-1a44-458f-9319-39102af0aff0":{"role":"reader","value":{"id":"a84bc0f0-1a44-458f-9319-39102af0aff0","version":2,"type":"text","properties":{"title":[["If you go with the first strategy, you need a standard neural network format.",[["b"]]],[" The "],["Open Neural Network Exchange",[["b"],["_"],["a","https://onnx.ai/"]]],[" (ONNX, for short) is designed to allow framework interoperability. The dream is to mix different frameworks, such that frameworks that are good for development (PyTorch) don’t also have to be good at inference (Caffe2)."]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3ad42750-d084-4434-b5fe-a8196c8bbbdf":{"role":"reader","value":{"id":"3ad42750-d084-4434-b5fe-a8196c8bbbdf","version":2,"type":"bulleted_list","properties":{"title":[["The promise is that you can train a model with one tool stack and then deploy it using another for inference/prediction. ONNX is a robust and open standard for preventing framework lock-in and ensuring that your models will be usable in the long run."]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a6a90205-2778-4e97-942f-b5ac3fb9a378":{"role":"reader","value":{"id":"a6a90205-2778-4e97-942f-b5ac3fb9a378","version":2,"type":"bulleted_list","properties":{"title":[["The reality is that since ML libraries change quickly, there are often bugs in the translation layer. Furthermore, how do you deal with non-library code (like feature transformations)?"]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3d4e11ab-622e-4d00-bfab-e05d0d4ccf5e":{"role":"reader","value":{"id":"3d4e11ab-622e-4d00-bfab-e05d0d4ccf5e","version":2,"type":"sub_sub_header","properties":{"title":[["Docker",[["b"]]]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"508e8655-ba2a-4626-aee0-daddc8497e6f":{"role":"reader","value":{"id":"508e8655-ba2a-4626-aee0-daddc8497e6f","version":2,"type":"text","properties":{"title":[["If you go with the second strategy, you want to learn Docker",[["b"]]],[". "],["Docker",[["b"],["_"],["a","https://www.docker.com/"]]],[" is a computer program that performs operating-system-level virtualization, also known as containerization. What is a container, you might ask? It is a standardized unit of fully packaged software used for local development, shipping code, and deploying system."]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"77271b72-2ff8-4700-9302-c05c0ffe85fb":{"role":"reader","value":{"id":"77271b72-2ff8-4700-9302-c05c0ffe85fb","version":2,"type":"text","properties":{"title":[["The best way to describe it intuitively is to think of a process surrounded by its filesystem. You run one or a few related processes, and they see a whole filesystem, not shared by anyone."]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"578a3100-e217-4d9a-9b13-cf4cf8a90b9b":{"role":"reader","value":{"id":"578a3100-e217-4d9a-9b13-cf4cf8a90b9b","version":2,"type":"bulleted_list","properties":{"title":[["This makes containers "],["extremely portable",[["b"]]],[", as they are detached from the underlying hardware and the platform that runs them."]]},"created_time":1646024849939,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"209a714e-128a-44e9-abc1-5dc3234b2fe3":{"role":"reader","value":{"id":"209a714e-128a-44e9-abc1-5dc3234b2fe3","version":2,"type":"bulleted_list","properties":{"title":[["They are very "],["lightweight",[["b"]]],[", as a minimal amount of data needs to be included."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d74ab2dc-83b8-4c6b-afff-0a9dd40d837e":{"role":"reader","value":{"id":"d74ab2dc-83b8-4c6b-afff-0a9dd40d837e","version":2,"type":"bulleted_list","properties":{"title":[["They are "],["secure",[["b"]]],[", as the exposed attack surface of a container is extremely small."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"51ebf39d-1493-448c-9b65-caef9875e9d9":{"role":"reader","value":{"id":"51ebf39d-1493-448c-9b65-caef9875e9d9","version":2,"type":"text","properties":{"title":[["Note here that "],["containers are different from virtual machines",[["b"]]],["."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2a934e4d-c98e-4792-99ef-4be26cc5acee":{"role":"reader","value":{"id":"2a934e4d-c98e-4792-99ef-4be26cc5acee","version":2,"type":"bulleted_list","properties":{"title":[["Virtual machines require the hypervisor to virtualize a full hardware stack. There are also multiple guest operating systems, making them larger and more extended to boot. This is what AWS / GCP / Azure cloud instances are."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8f767ef4-caa0-4232-9ff5-f8c3b417e3c2":{"role":"reader","value":{"id":"8f767ef4-caa0-4232-9ff5-f8c3b417e3c2","version":2,"type":"bulleted_list","properties":{"title":[["Containers, on the other hand, require no hypervisor/hardware virtualization. All containers share the same host kernel. There are dedicated isolated user-space environments, making them much smaller in size and faster to boot."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c9abc232-dde5-46c9-bc31-d99e2c7a387c":{"role":"reader","value":{"id":"c9abc232-dde5-46c9-bc31-d99e2c7a387c","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image12.png"]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b3c5d5b9-7f69-4920-b4f7-6a11c7e52c6c":{"role":"reader","value":{"id":"b3c5d5b9-7f69-4920-b4f7-6a11c7e52c6c","version":2,"type":"text","properties":{"title":[["In brief, you should familiarize yourself with these basic concepts:"]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"815dfa4b-1599-4ca8-b92b-b449c9436b02":{"role":"reader","value":{"id":"815dfa4b-1599-4ca8-b92b-b449c9436b02","version":2,"type":"numbered_list","properties":{"title":[["Dockerfile",[["b"]]],[" defines how to build an image."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"458d7108-3760-4d62-a296-3345f4ddc297":{"role":"reader","value":{"id":"458d7108-3760-4d62-a296-3345f4ddc297","version":2,"type":"numbered_list","properties":{"title":[["Image",[["b"]]],[" is a built packaged environment."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d57c561e-3d96-43ed-82d8-47de05f16651":{"role":"reader","value":{"id":"d57c561e-3d96-43ed-82d8-47de05f16651","version":2,"type":"numbered_list","properties":{"title":[["Containe",[["b"]]],["r is where images are run inside."]]},"created_time":1646024849940,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"03bb4095-2a5e-46e6-8cd8-b1a6a844770a":{"role":"reader","value":{"id":"03bb4095-2a5e-46e6-8cd8-b1a6a844770a","version":2,"type":"numbered_list","properties":{"title":[["Repository",[["b"]]],[" hosts different versions of an image."]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"49afb1a6-78ee-44e9-bbc0-4dc271a83377":{"role":"reader","value":{"id":"49afb1a6-78ee-44e9-bbc0-4dc271a83377","version":2,"type":"numbered_list","properties":{"title":[["Registry",[["b"]]],[" is a set of repositories."]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0df0ead7-403b-407e-9f0c-76f29db21369":{"role":"reader","value":{"id":"0df0ead7-403b-407e-9f0c-76f29db21369","version":2,"type":"text","properties":{"title":[["Furthermore, Docker has a robust ecosystem. It has the "],["DockerHub",[["b"],["_"],["a","https://hub.docker.com/"]]],[" for community-contributed images. It’s incredibly easy to search for images that meet your needs, ready to pull down and use with little-to-no modification."]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0a1ad2a7-c0df-444a-b7bd-edab7173da06":{"role":"reader","value":{"id":"0a1ad2a7-c0df-444a-b7bd-edab7173da06","version":2,"type":"text","properties":{"title":[["Though Docker presents how to deal with each of the individual microservices, we also need "],["an orchestrator",[["b"]]],[" to handle the whole cluster of services. Such an orchestrator distributes containers onto the underlying virtual machines or bare metal so that these containers talk to each other and coordinate to solve the task at hand. The standard container orchestration tool is "],["Kubernetes",[["b"],["_"],["a","https://kubernetes.io/"]]],["."]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5f49b85d-c206-4114-bd66-6c3dc9f932d0":{"role":"reader","value":{"id":"5f49b85d-c206-4114-bd66-6c3dc9f932d0","version":2,"type":"sub_sub_header","properties":{"title":[["PERFORMANCE OPTIMIZATION",[["b"]]]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3d65c20e-6e0a-40aa-a8b9-cb7a0092e6c5":{"role":"reader","value":{"id":"3d65c20e-6e0a-40aa-a8b9-cb7a0092e6c5","version":2,"type":"text","properties":{"title":[["We will talk mostly about how to run your model service faster on a single machine. Here are the key questions that you want to address:"]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"de8fb983-febd-4c4d-ba03-165974e9c075":{"role":"reader","value":{"id":"de8fb983-febd-4c4d-ba03-165974e9c075","version":2,"type":"bulleted_list","properties":{"title":[["Do you want inference on a GPU or not?"]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7bbbd7a1-ed40-4282-baba-7aaab3543edc":{"role":"reader","value":{"id":"7bbbd7a1-ed40-4282-baba-7aaab3543edc","version":2,"type":"bulleted_list","properties":{"title":[["How can you run multiple copies of the model at the same time?"]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"22477281-f00c-4b13-b25f-7226c3e2d695":{"role":"reader","value":{"id":"22477281-f00c-4b13-b25f-7226c3e2d695","version":2,"type":"bulleted_list","properties":{"title":[["How to make the model smaller?"]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a1b4d927-d235-44e6-9126-554a674d028b":{"role":"reader","value":{"id":"a1b4d927-d235-44e6-9126-554a674d028b","version":2,"type":"bulleted_list","properties":{"title":[["How to improve model performance via caching, batching, and GPU sharing?"]]},"created_time":1646024849941,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"327a5879-5e80-4634-888e-3ee571639c5c":{"role":"reader","value":{"id":"327a5879-5e80-4634-888e-3ee571639c5c","version":2,"type":"sub_sub_header","properties":{"title":[["GPU or no GPU?",[["b"]]]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"042d9ee2-6596-4494-b447-82631248578d":{"role":"reader","value":{"id":"042d9ee2-6596-4494-b447-82631248578d","version":2,"type":"text","properties":{"title":[["Here are the pros of GPU inference:"]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d635c38b-97b3-4b49-af35-c9a2edc85222":{"role":"reader","value":{"id":"d635c38b-97b3-4b49-af35-c9a2edc85222","version":2,"type":"bulleted_list","properties":{"title":[["You use the same hardware that your model is trained on probably."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"858b27eb-2fee-4ce9-937e-8800808f5197":{"role":"reader","value":{"id":"858b27eb-2fee-4ce9-937e-8800808f5197","version":2,"type":"bulleted_list","properties":{"title":[["If your model gets bigger and you want to limit model size or tune batch size, you will get high throughput."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d7d6eb0e-5ddc-42bd-88d3-4cb722475103":{"role":"reader","value":{"id":"d7d6eb0e-5ddc-42bd-88d3-4cb722475103","version":2,"type":"text","properties":{"title":[["Here are the cons of GPU inference:"]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3ae81a86-67db-433b-bd3c-bd80ac56acfe":{"role":"reader","value":{"id":"3ae81a86-67db-433b-bd3c-bd80ac56acfe","version":2,"type":"bulleted_list","properties":{"title":[["GPU is complex to set up."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a1cf0149-3b5a-408e-be55-9c2c21727c9c":{"role":"reader","value":{"id":"a1cf0149-3b5a-408e-be55-9c2c21727c9c","version":2,"type":"bulleted_list","properties":{"title":[["GPUs are expensive."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"99356408-36d1-4dfd-bc0d-2e22414cda7e":{"role":"reader","value":{"id":"99356408-36d1-4dfd-bc0d-2e22414cda7e","version":2,"type":"sub_sub_header","properties":{"title":[["Concurrency",[["b"]]]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"40f37f75-1281-4d20-bd82-041cc61c2157":{"role":"reader","value":{"id":"40f37f75-1281-4d20-bd82-041cc61c2157","version":2,"type":"text","properties":{"title":[["Instead of running a single model copy on your machine, you run multiple model copies on different CPUs or cores. In practice, you need to be careful about "],["thread tuning",[["b"]]],[" - making sure that each model copy only uses the minimum number of threads required. Read "],["this blog post from Roblox",[["b"],["_"],["a","https://blog.roblox.com/2020/05/scaled-bert-serve-1-billion-daily-requests-cpus/"]]],[" for the details."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5b7f0fe8-f694-4b45-8d31-78a3f6049f61":{"role":"reader","value":{"id":"5b7f0fe8-f694-4b45-8d31-78a3f6049f61","version":2,"type":"sub_sub_header","properties":{"title":[["Model distillation",[["b"]]]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5661897e-16d9-4a60-b088-72e79203e72f":{"role":"reader","value":{"id":"5661897e-16d9-4a60-b088-72e79203e72f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image7.png"]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f8f31574-59c7-484d-bf68-db81b459618b":{"role":"reader","value":{"id":"f8f31574-59c7-484d-bf68-db81b459618b","version":2,"type":"text","properties":{"title":[["Model distillation is a compression technique in which a small “student” model is trained to reproduce the behavior of a large “teacher” model. The method was first proposed by "],["Bucila et al., 2006",[["b"],["_"],["a","https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf"]]],[" and generalized by "],["Hinton et al., 2015",[["b"],["_"],["a","https://arxiv.org/pdf/1503.02531.pdf"]]],[". In distillation, knowledge is transferred from the teacher model to the student by minimizing a loss function. The target is the distribution of class probabilities predicted by the teacher model. That is — the output of a softmax function on the teacher model’s logits."]]},"created_time":1646024849942,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"eb727e7a-f01d-4210-9df2-5f961da27855":{"role":"reader","value":{"id":"eb727e7a-f01d-4210-9df2-5f961da27855","version":2,"type":"text","properties":{"title":[["Distillation can be finicky to do yourself, so "],["it is infrequently used in practice",[["b"]]],[". Read "],["this blog post from Derrick Mwiti",[["b"],["_"],["a","https://heartbeat.fritz.ai/research-guide-model-distillation-techniques-for-deep-learning-4a100801c0eb"]]],[" for several model distillation techniques for deep learning."]]},"created_time":1646024849943,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1f154196-cd53-4048-8ab1-7942f4fa1f8c":{"role":"reader","value":{"id":"1f154196-cd53-4048-8ab1-7942f4fa1f8c","version":2,"type":"sub_sub_header","properties":{"title":[["Model quantization",[["b"]]]]},"created_time":1646024849943,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"29bc3743-4062-4840-bf72-efed40ed5209":{"role":"reader","value":{"id":"29bc3743-4062-4840-bf72-efed40ed5209","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image1.png"]]},"created_time":1646024849943,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"76d5ed56-673e-4c10-b6f7-fb00bed8a9ee":{"role":"reader","value":{"id":"76d5ed56-673e-4c10-b6f7-fb00bed8a9ee","version":2,"type":"text","properties":{"title":[["Model quantization is a model compression technique that makes the model physically smaller to save disk space and require less memory during computation to run faster. It decreases the numerical precision of a model’s weights. In other words, each weight is permanently encoded using fewer bits. Note here that "],["there are tradeoffs with accuracy",[["b"]]],["."]]},"created_time":1646024849943,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ee9fe243-b041-4aab-be10-ba06a9cf3527":{"role":"reader","value":{"id":"ee9fe243-b041-4aab-be10-ba06a9cf3527","version":2,"type":"bulleted_list","properties":{"title":[["A straightforward method is implemented "],["in the TensorFlow Lite toolkit",[["b"],["_"],["a","https://www.tensorflow.org/lite/performance/quantization_spec"]]],[". It turns a matrix of 32-bit floats into 8-bit integers by applying a simple “center-and-scale” transform to it: "],["W_8 = W_32 / scale + shift",[["i"]]],[" (scale and shift are determined individually for each weight matrix). This way, the 8-bit W is used in matrix multiplication, and only the result is then corrected by applying the “center-and-scale” operation in reverse."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"19342d26-a525-45f2-8733-5a8caded34f6":{"role":"reader","value":{"id":"19342d26-a525-45f2-8733-5a8caded34f6","version":2,"type":"bulleted_list","properties":{"title":[["PyTorch also has quantization built-in",[["b"],["_"],["a","https://pytorch.org/blog/introduction-to-quantization-on-pytorch/"]]],[" that includes three techniques: dynamic quantization, post-training static quantization, and quantization-aware training."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0827873e-016f-4e95-a394-8e105996859d":{"role":"reader","value":{"id":"0827873e-016f-4e95-a394-8e105996859d","version":2,"type":"sub_sub_header","properties":{"title":[["Caching",[["b"]]]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"893e42ae-851b-43c8-a5e8-29d930c47a7f":{"role":"reader","value":{"id":"893e42ae-851b-43c8-a5e8-29d930c47a7f","version":2,"type":"text","properties":{"title":[["For many ML models, the input distribution is non-uniform (some are more common than others). Caching takes advantage of that. Instead of constantly calling the model on every input no matter what, we first "],["cache the model’s frequently-used inputs",[["b"]]],[". Before calling the model, we check the cache and only call it on the frequently-used inputs."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8c7ee54e-64e3-4b2c-abb5-2ca0623faca6":{"role":"reader","value":{"id":"8c7ee54e-64e3-4b2c-abb5-2ca0623faca6","version":2,"type":"text","properties":{"title":[["Caching techniques can get very fancy, but the most basic way to get started is using Python’s "],["functools",[["b"],["_"],["a","https://docs.python.org/3/library/functools.html"]]],["."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"06de6f97-d955-4729-bd86-2ee521467a85":{"role":"reader","value":{"id":"06de6f97-d955-4729-bd86-2ee521467a85","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image6.png"]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"011b1a55-27fb-4c0a-85f3-1568c73124b5":{"role":"reader","value":{"id":"011b1a55-27fb-4c0a-85f3-1568c73124b5","version":2,"type":"sub_sub_header","properties":{"title":[["Batching",[["b"]]]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4109ef77-deea-4e58-94d6-7c92dddebde2":{"role":"reader","value":{"id":"4109ef77-deea-4e58-94d6-7c92dddebde2","version":2,"type":"text","properties":{"title":[["Typically, ML models achieve higher throughput when making predictions in parallel (especially true for GPU inference). At a high level, here’s how batching works:"]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"699a9dd4-2dc8-4e93-b001-6320b8175451":{"role":"reader","value":{"id":"699a9dd4-2dc8-4e93-b001-6320b8175451","version":2,"type":"bulleted_list","properties":{"title":[["You gather predictions that are coming in until you have a batch for your system. Then, you run the model on that batch and return predictions to those users who request them."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"780cba65-6d5c-4745-894a-d12652b805fc":{"role":"reader","value":{"id":"780cba65-6d5c-4745-894a-d12652b805fc","version":2,"type":"bulleted_list","properties":{"title":[["You need to tune the batch size and address the tradeoff between throughput and latency."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"35f21fa1-fc6b-475e-bcf8-7d6f2f3cd81c":{"role":"reader","value":{"id":"35f21fa1-fc6b-475e-bcf8-7d6f2f3cd81c","version":2,"type":"bulleted_list","properties":{"title":[["You need to have a way to shortcut the process if latency becomes too long."]]},"created_time":1646024849944,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"437d7cbc-e2a2-47c3-a611-2011d958700b":{"role":"reader","value":{"id":"437d7cbc-e2a2-47c3-a611-2011d958700b","version":2,"type":"bulleted_list","properties":{"title":[["The last caveat is that "],["you probably do not want to implement batching yourself",[["b"]]],["."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c458ce17-1fde-416b-9257-a2b411b7fbe0":{"role":"reader","value":{"id":"c458ce17-1fde-416b-9257-a2b411b7fbe0","version":2,"type":"sub_sub_header","properties":{"title":[["Sharing The GPU",[["b"]]]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6351b16f-1e25-4bf1-847d-e04fc8fe3017":{"role":"reader","value":{"id":"6351b16f-1e25-4bf1-847d-e04fc8fe3017","version":2,"type":"text","properties":{"title":[["Your model may not take up all of the GPU memory with your inference batch size. "],["Why not run multiple models on the same GPU?",[["b"]]],[" You probably want to use a model serving solution that supports this out of the box."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fb0310d2-0ac6-48e7-8a29-66f97f1816be":{"role":"reader","value":{"id":"fb0310d2-0ac6-48e7-8a29-66f97f1816be","version":2,"type":"sub_sub_header","properties":{"title":[["Model Serving Libraries",[["b"]]]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"691ed765-8f7b-4d46-9a5a-03ec63e9f698":{"role":"reader","value":{"id":"691ed765-8f7b-4d46-9a5a-03ec63e9f698","version":2,"type":"text","properties":{"title":[["There are canonical open-source model serving libraries for both PyTorch ("],["TorchServe",[["b"],["_"],["a","https://pytorch.org/serve/"]]],[") and TensorFlow ("],["TensorFlow Serving",[["b"],["_"],["a","https://www.tensorflow.org/tfx/guide/serving"]]],["). "],["Ray Serve",[["b"],["_"],["a","https://docs.ray.io/en/master/serve/index.html"]]],[" is another promising choice. Even NVIDIA has joined the game with "],["Triton Inference Server",[["b"],["_"],["a","https://developer.nvidia.com/nvidia-triton-inference-server"]]],["."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"3c663d3a-4d3f-428f-afd3-420c33fd1bb7":{"role":"reader","value":{"id":"3c663d3a-4d3f-428f-afd3-420c33fd1bb7","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image20.png"]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4aabd7a1-0090-4da4-998e-6ff08af33bef":{"role":"reader","value":{"id":"4aabd7a1-0090-4da4-998e-6ff08af33bef","version":2,"type":"sub_sub_header","properties":{"title":[["HORIZONTAL SCALING",[["b"]]]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8576978c-c82a-4948-832b-e59442cbd2c7":{"role":"reader","value":{"id":"8576978c-c82a-4948-832b-e59442cbd2c7","version":2,"type":"text","properties":{"title":[["If you have too much traffic for a single machine, let’s split traffic among multiple machines",[["b"]]],[". At a high level, you duplicate your prediction service, use a load balancer to split traffic, and send the traffic to the appropriate copy of your service. In practice, there are two common methods:"]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b2c82e45-28cb-4dbd-af4b-c1cc9397cda7":{"role":"reader","value":{"id":"b2c82e45-28cb-4dbd-af4b-c1cc9397cda7","version":2,"type":"numbered_list","properties":{"title":[["Use a container orchestration toolkit like Kubernetes."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b8055fa6-450d-4334-97e2-bcff8ca2b4b0":{"role":"reader","value":{"id":"b8055fa6-450d-4334-97e2-bcff8ca2b4b0","version":2,"type":"numbered_list","properties":{"title":[["Use a serverless option like AWS Lambda."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0243f2e8-8ee9-4afe-826a-5902ae0e0c65":{"role":"reader","value":{"id":"0243f2e8-8ee9-4afe-826a-5902ae0e0c65","version":2,"type":"sub_sub_header","properties":{"title":[["Container Orchestration",[["b"]]]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9a6ccb1c-b38d-4583-bf73-b1848f84ef60":{"role":"reader","value":{"id":"9a6ccb1c-b38d-4583-bf73-b1848f84ef60","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image21.png"]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"190b2ac2-9e39-40dd-bb3e-5322bb7eb904":{"role":"reader","value":{"id":"190b2ac2-9e39-40dd-bb3e-5322bb7eb904","version":2,"type":"text","properties":{"title":[["In this paradigm, your Docker containers are coordinated by Kubernetes. K8s provides a single service for you to send requests to. Then it divides up traffic that gets sent to that service to virtual copies of your containers (that are running on your infrastructure)."]]},"created_time":1646024849945,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b8e49bc4-1ec6-4873-86cf-f8ed846adfa5":{"role":"reader","value":{"id":"b8e49bc4-1ec6-4873-86cf-f8ed846adfa5","version":2,"type":"text","properties":{"title":[["You can build a system like this yourself on top of K8s if you want to. But there are emerging frameworks that can handle all such infrastructure out of the box if you have a K8s cluster running. "],["KFServing",[["b"],["_"],["a","https://www.kubeflow.org/docs/components/kfserving/"]]],[" is a part of the "],["Kubeflow",[["b"],["_"],["a","https://www.kubeflow.org/"]]],[" package, a popular K8s-native ML infrastructure solution. "],["Seldon",[["b"],["_"],["a","http://seldon.io/"]]],[" provides a model serving stack on top of K8s."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7d91fa63-2c1c-4bff-96a7-f237a22e2d3e":{"role":"reader","value":{"id":"7d91fa63-2c1c-4bff-96a7-f237a22e2d3e","version":2,"type":"sub_sub_header","properties":{"title":[["Deploying Code As Serverless Functions",[["b"]]]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cb13bd66-8c0a-44db-8804-0d68b5f22654":{"role":"reader","value":{"id":"cb13bd66-8c0a-44db-8804-0d68b5f22654","version":2,"type":"text","properties":{"title":[["The idea here is that the app code and dependencies are packaged into .zip files (or Docker containers) with a single entry point function. All the major cloud providers such as AWS Lambda, Google Cloud Functions, or Azure Functions will manage everything else: instant scaling to 10,000+ requests per second, load balancing, etc."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b68fa00f-e7c7-42ab-bf29-f65b5dee9488":{"role":"reader","value":{"id":"b68fa00f-e7c7-42ab-bf29-f65b5dee9488","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image10.png"]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0c3ca592-da66-47e0-a59d-977fb46a61d5":{"role":"reader","value":{"id":"0c3ca592-da66-47e0-a59d-977fb46a61d5","version":2,"type":"text","properties":{"title":[["The good thing is that "],["you only pay for compute-time",[["b"]]],[". Furthermore, this approach lowers your DevOps load, as you do not own any servers."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4830cfc0-9317-46c5-8f3c-3f9802d42932":{"role":"reader","value":{"id":"4830cfc0-9317-46c5-8f3c-3f9802d42932","version":2,"type":"text","properties":{"title":[["The tradeoff is that you have to work with "],["severe constraints",[["b"]]],[":"]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4a0ae5ef-f624-4ca8-bd8a-02e558f63814":{"role":"reader","value":{"id":"4a0ae5ef-f624-4ca8-bd8a-02e558f63814","version":2,"type":"numbered_list","properties":{"title":[["Your entire deployment package is quite limited."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"09e52cb8-a8da-4fa8-925e-15f701bd56ee":{"role":"reader","value":{"id":"09e52cb8-a8da-4fa8-925e-15f701bd56ee","version":2,"type":"numbered_list","properties":{"title":[["You can only do CPU execution."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"85433715-44ae-4184-9596-3a38c67249ed":{"role":"reader","value":{"id":"85433715-44ae-4184-9596-3a38c67249ed","version":2,"type":"numbered_list","properties":{"title":[["It can be challenging to build model pipelines."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8b5e0ebc-534b-402a-b9e7-9128d90e430d":{"role":"reader","value":{"id":"8b5e0ebc-534b-402a-b9e7-9128d90e430d","version":2,"type":"numbered_list","properties":{"title":[["There are limited state management and deployment tooling."]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e1ba2c9d-c70d-48c1-8cb3-15528f6741c6":{"role":"reader","value":{"id":"e1ba2c9d-c70d-48c1-8cb3-15528f6741c6","version":2,"type":"sub_sub_header","properties":{"title":[["MODEL DEPLOYMENT",[["b"]]]]},"created_time":1646024849946,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fee9cd05-1e0e-4ba7-b774-0cb7132b9179":{"role":"reader","value":{"id":"fee9cd05-1e0e-4ba7-b774-0cb7132b9179","version":2,"type":"text","properties":{"title":[["If serving is how you turn a model into something that can respond to requests, "],["deployment",[["b"]]],[" is how you roll out, manage, and update these services. You probably want to be able to "],["roll out gradually",[["b"]]],[", "],["roll back instantly",[["b"]]],[", and "],["deploy pipelines of models",[["b"]]],[". Many challenging infrastructure considerations go into this, but hopefully, your deployment library will take care of this for you."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"37ec2859-e283-497a-836f-4be65c847bbc":{"role":"reader","value":{"id":"37ec2859-e283-497a-836f-4be65c847bbc","version":2,"type":"sub_sub_header","properties":{"title":[["MANAGED OPTIONS",[["b"]]]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"31b5dfc2-c14d-4e49-b659-0435ac8c56ea":{"role":"reader","value":{"id":"31b5dfc2-c14d-4e49-b659-0435ac8c56ea","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image18.png"]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a58942ce-7c2b-43e3-9671-14e97487888a":{"role":"reader","value":{"id":"a58942ce-7c2b-43e3-9671-14e97487888a","version":2,"type":"text","properties":{"title":[["If you do not want to deal with any of the things mentioned thus far, there are managed options in the market. All major cloud providers have ones that enable you to package your model in a predefined way and turn it into an API. Startups like "],["Algorithmia",[["b"],["_"],["a","https://algorithmia.com/"]]],[" and "],["Cortex",[["b"],["_"],["a","https://www.cortex.dev/"]]],[" are some alternatives. The big drawback is that "],["pricing tends to be high, so you pay a premium fee in exchange for convenience",[["b"]]],["."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"142d1833-2cd1-4a36-9b2a-55e2ad08b01c":{"role":"reader","value":{"id":"142d1833-2cd1-4a36-9b2a-55e2ad08b01c","version":2,"type":"sub_sub_header","properties":{"title":[["TAKEAWAYS",[["b"]]]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"435a4931-279f-4553-9f41-f87d9a53bc55":{"role":"reader","value":{"id":"435a4931-279f-4553-9f41-f87d9a53bc55","version":2,"type":"bulleted_list","properties":{"title":[["If you are making CPU inference, you can get away with scaling by launching more servers or going serverless."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5e9ecfd7-e0aa-4561-b1dc-e8eda97b0c28":{"role":"reader","value":{"id":"5e9ecfd7-e0aa-4561-b1dc-e8eda97b0c28","version":2,"type":"bulleted_list","properties":{"title":[["Serverless makes sense if you can get away with CPUs, and traffic is spiky or low-volume."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0d1c832a-4e89-4734-85bf-8fd6bead430b":{"role":"reader","value":{"id":"0d1c832a-4e89-4734-85bf-8fd6bead430b","version":2,"type":"bulleted_list","properties":{"title":[["If you are using GPU inference, serving tools will save you time."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ed05d6a3-6db9-4a9a-8ff8-0e7d6efc6a61":{"role":"reader","value":{"id":"ed05d6a3-6db9-4a9a-8ff8-0e7d6efc6a61","version":2,"type":"bulleted_list","properties":{"title":[["It’s worth keeping an eye on startups in this space for GPU inference."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"03facf93-ce66-4b5f-950e-3815667fee5f":{"role":"reader","value":{"id":"03facf93-ce66-4b5f-950e-3815667fee5f","version":8,"type":"sub_header","properties":{"title":[["3 - Edge Deployment",[["b"]]]]},"created_time":1646024849947,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"13ae46aa-eb95-48e4-b48a-d389b8c5f08f":{"role":"reader","value":{"id":"13ae46aa-eb95-48e4-b48a-d389b8c5f08f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image9.png"]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e08f2df4-fb9e-4816-b519-05392e8a39d5":{"role":"reader","value":{"id":"e08f2df4-fb9e-4816-b519-05392e8a39d5","version":2,"type":"text","properties":{"title":[["Edge prediction means that you first send the model weights to the client edge device. Then, the client loads the model and interacts with it directly."]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a37cb27e-9624-4d0c-9696-93dca70e4abd":{"role":"reader","value":{"id":"a37cb27e-9624-4d0c-9696-93dca70e4abd","version":2,"type":"text","properties":{"title":[["The pros of edge prediction:"]]},"created_time":1646024849947,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f7cc6261-f15a-473f-aeb6-13a7d2f927b0":{"role":"reader","value":{"id":"f7cc6261-f15a-473f-aeb6-13a7d2f927b0","version":2,"type":"bulleted_list","properties":{"title":[["It has low latency."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a089f1c3-3b8f-4b42-97a2-bcc6621d31bf":{"role":"reader","value":{"id":"a089f1c3-3b8f-4b42-97a2-bcc6621d31bf","version":2,"type":"bulleted_list","properties":{"title":[["It does not require an Internet connection."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b7c2f2ac-7cd7-49d1-9721-2271baec1292":{"role":"reader","value":{"id":"b7c2f2ac-7cd7-49d1-9721-2271baec1292","version":2,"type":"bulleted_list","properties":{"title":[["It satisfies data security requirements, as data does not need to leave the user’s device."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fdabcd48-e4a6-442b-9986-b24143a06597":{"role":"reader","value":{"id":"fdabcd48-e4a6-442b-9986-b24143a06597","version":2,"type":"text","properties":{"title":[["The cons of edge prediction:"]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c8932335-ebbe-4962-903f-407eecab16ee":{"role":"reader","value":{"id":"c8932335-ebbe-4962-903f-407eecab16ee","version":2,"type":"bulleted_list","properties":{"title":[["The client often has limited hardware resources available."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2162416c-0e61-4641-b3eb-c90781fec74a":{"role":"reader","value":{"id":"2162416c-0e61-4641-b3eb-c90781fec74a","version":2,"type":"bulleted_list","properties":{"title":[["Embedded and mobile frameworks are less full-featured than TensorFlow and PyTorch."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"03923d54-6b05-49e2-8e28-5db0865308b0":{"role":"reader","value":{"id":"03923d54-6b05-49e2-8e28-5db0865308b0","version":2,"type":"bulleted_list","properties":{"title":[["It is challenging to update models."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4bed221b-6c7d-4eb2-90a2-6062f6fb0765":{"role":"reader","value":{"id":"4bed221b-6c7d-4eb2-90a2-6062f6fb0765","version":2,"type":"bulleted_list","properties":{"title":[["It is difficult to monitor and debug when things go wrong."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4fd0827b-b5b8-4fcf-aa4c-09043532cef0":{"role":"reader","value":{"id":"4fd0827b-b5b8-4fcf-aa4c-09043532cef0","version":2,"type":"sub_sub_header","properties":{"title":[["TOOLS FOR EDGE DEPLOYMENT",[["b"]]]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"dc3f710f-28c8-4275-87eb-c509ffd957f8":{"role":"reader","value":{"id":"dc3f710f-28c8-4275-87eb-c509ffd957f8","version":2,"type":"text","properties":{"title":[["TensorRT",[["b"],["_"],["a","https://developer.nvidia.com/tensorrt"]]],[" is NVIDIA’s framework meant to help you optimize models for inference on NVIDIA devices in data centers and embedded/automotive environments. TensorRT is also integrated with application-specific SDKs to provide developers a unified path to deploy conversational AI, recommender, video conference, and streaming apps in production."]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"653fc752-537d-4341-a8c8-4c7c4f352e69":{"role":"reader","value":{"id":"653fc752-537d-4341-a8c8-4c7c4f352e69","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image13.png"]]},"created_time":1646024849948,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"855ab00b-5a31-4608-857e-3a734714274d":{"role":"reader","value":{"id":"855ab00b-5a31-4608-857e-3a734714274d","version":2,"type":"text","properties":{"title":[["ApacheTVM",[["b"],["_"],["a","https://tvm.apache.org/"]]],[" is an open-source machine learning compiler framework for CPUs, GPUs, and ML accelerators. It aims to enable ML engineers to optimize and run computations efficiently on any hardware backend. In particular, it compiles ML models into minimum deployable modules and provides the infrastructure to automatically optimize models on more backends with better performance."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e603a368-20df-49e6-bf3f-320fb151b178":{"role":"reader","value":{"id":"e603a368-20df-49e6-bf3f-320fb151b178","version":2,"type":"text","properties":{"title":[["Tensorflow Lite",[["b"],["_"],["a","https://www.tensorflow.org/lite/"]]],[" provides a trained TensorFlow model framework to be compressed and deployed to a mobile or embedded application. TensorFlow’s computationally expensive training process can still be performed in the environment that best suits it (personal server, cloud, overclocked computer). TensorFlow Lite then takes the resulting model (frozen graph, SavedModel, or HDF5 model) as input, packages, deploys, and then interprets it in the client application, handling the resource-conserving optimizations along the way."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"439f028d-9e8c-4396-a12c-6a79c0ecc40e":{"role":"reader","value":{"id":"439f028d-9e8c-4396-a12c-6a79c0ecc40e","version":4,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image3.png"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1646024849949,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"fcc121ff-45e9-401f-82d8-94713b3b5b25":{"role":"reader","value":{"id":"fcc121ff-45e9-401f-82d8-94713b3b5b25","version":2,"type":"text","properties":{"title":[["PyTorch Mobile",[["b"],["_"],["a","https://pytorch.org/mobile/home/"]]],[" is a framework for helping mobile developers and machine learning engineers embed PyTorch models on-device. Currently, it allows any "],["TorchScript model",[["b"],["_"],["a","https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html"]]],[" to run directly inside iOS and Android applications. PyTorch Mobile’s initial release supports many different quantization techniques, which shrink model sizes without significantly affecting performance. PyTorch Mobile also allows developers to directly convert a PyTorch model to a mobile-ready format without needing to work through other tools/frameworks."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a7917bee-b619-4044-9342-796764300b3e":{"role":"reader","value":{"id":"a7917bee-b619-4044-9342-796764300b3e","version":2,"type":"text","properties":{"title":[["JavaScript is a portable way of running code on different devices. "],["Tensorflow.js",[["b"],["_"],["a","https://www.tensorflow.org/js"]]],[" enables you to run TensorFlow code in JavaScript. You can use off-the-shelf JavaScript models or convert Python TensorFlow models to run in the browser or under Node.js, retrain pre-existing ML models using your data, and build/train models directly in JavaScript using flexible and intuitive APIs."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c763fe05-1217-405e-8e2c-66945ed1d0b8":{"role":"reader","value":{"id":"c763fe05-1217-405e-8e2c-66945ed1d0b8","version":2,"type":"text","properties":{"title":[["Core ML",[["b"],["_"],["a","https://developer.apple.com/documentation/coreml"]]],[" was released by Apple back in 2017. It is optimized for on-device performance, which minimizes a model’s memory footprint and power consumption. Running strictly on the device also ensures that user data is kept secure. The app runs even in the absence of a network connection. Generally speaking, it is straightforward to use with just a few lines of code needed to integrate a complete ML model into your device. The downside is that you can only make the model inference, as no model training is possible."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"61be0038-3a25-44d9-b9ad-47d543772d3f":{"role":"reader","value":{"id":"61be0038-3a25-44d9-b9ad-47d543772d3f","version":4,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image4.png"]]},"format":{"block_width":432,"block_full_width":false,"block_page_width":false},"created_time":1646024849949,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b736ce24-e720-43ec-9a8c-67c45e7ce659":{"role":"reader","value":{"id":"b736ce24-e720-43ec-9a8c-67c45e7ce659","version":2,"type":"text","properties":{"title":[["ML Kit",[["b"],["_"],["a","https://firebase.google.com/docs/ml-kit/"]]],[" was announced by Google Firebase in 2018. It enables developers to utilize ML in mobile apps either with (1) inference in the cloud via API or (2) inference on-device (like Core ML). For the former option, ML Kit offers six base APIs with pertained models such as Image Labeling, Text Recognition, and Barcode Scanning. For the latter option, ML Kit offers lower accuracy but more security to user data, compared to the cloud version."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"eb90d933-29bd-4420-9067-36c2980d2526":{"role":"reader","value":{"id":"eb90d933-29bd-4420-9067-36c2980d2526","version":2,"type":"text","properties":{"title":[["If you are interested in either of the above options, check out "],["this comparison",[["b"],["_"],["a","https://heartbeat.fritz.ai/core-ml-vs-ml-kit-which-mobile-machine-learning-framework-is-right-for-you-e25c5d34c765"]]],[" by the FritzAI team. Additionally, "],["FritzAI",[["b"],["_"],["a","https://www.fritz.ai/features/"]]],[" is an ML platform for mobile developers that provide pre-trained models, developer tools, and SDKs for iOS, Android, and Unity."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8c9db52d-4646-4809-9fd3-0eb6621e4b45":{"role":"reader","value":{"id":"8c9db52d-4646-4809-9fd3-0eb6621e4b45","version":2,"type":"sub_sub_header","properties":{"title":[["MORE EFFICIENT MODELS",[["b"]]]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"21728e6c-0588-4111-a96f-cb1c9ee6cdb6":{"role":"reader","value":{"id":"21728e6c-0588-4111-a96f-cb1c9ee6cdb6","version":4,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image22.png"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1646024849949,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a1926dec-593f-4688-b288-5bc369085c74":{"role":"reader","value":{"id":"a1926dec-593f-4688-b288-5bc369085c74","version":2,"type":"text","properties":{"title":[["Another thing to consider for edge deployment is to make the models more efficient. One way to do this is to use the same quantization and distillation techniques discussed above. Another way is to "],["pick mobile-friendly model architectures",[["b"]]],[". The first successful example is "],["MobileNet",[["b"],["_"],["a","https://ai.googleblog.com/2017/06/mobilenets-open-source-models-for.html"]]],[", which performs various downsampling techniques to a traditional ConvNet architecture to maximize accuracy while being mindful of the restricted resources for a mobile or an embedded device. "],["This analysis",[["b"],["_"],["a","https://medium.com/@yu4u/why-mobilenet-and-its-variants-e-g-shufflenet-are-fast-1c7048b9618d"]]],[" by Yusuke Uchida explains why MobileNet and its variants are fast."]]},"created_time":1646024849949,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b0c90d04-6122-452b-99fd-a6a2736a4cdc":{"role":"reader","value":{"id":"b0c90d04-6122-452b-99fd-a6a2736a4cdc","version":2,"type":"text","properties":{"title":[["A well-known case study of applying knowledge distillation in practice is Hugging Face’s "],["DistilBERT",[["b"],["_"],["a","https://medium.com/huggingface/distilbert-8cf3380435b5"]]],[", a smaller language model derived from the supervision of the popular "],["BERT",[["b"],["_"],["a","https://arxiv.org/abs/1706.03762"]]],[" language model. DistilBERT removes the toke-type embeddings and the pooler (used for the next sentence classification task) from BERT while keeping the rest of the architecture identical and reducing the number of layers by a factor of two. Overall, DistilBERT has about half the total number of parameters of the BERT base and retains 95% of BERT’s performances on the language understanding benchmark GLUE."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8e46de0c-e987-4d36-868d-ce8851b5e104":{"role":"reader","value":{"id":"8e46de0c-e987-4d36-868d-ce8851b5e104","version":2,"type":"sub_sub_header","properties":{"title":[["MINDSET FOR EDGE DEPLOYMENT",[["b"]]]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6fa27d24-0e68-4839-a6ca-f1a5f2a71f9e":{"role":"reader","value":{"id":"6fa27d24-0e68-4839-a6ca-f1a5f2a71f9e","version":2,"type":"bulleted_list","properties":{"title":[["It is crucial to "],["choose your architecture with your target hardware in mind",[["b"]]],[". Specifically, you can make up a factor of 2-10 through distillation, quantization, and other tricks (but not more than that)."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"616e1c75-c4ba-4f9a-b997-a0c0b5b4689a":{"role":"reader","value":{"id":"616e1c75-c4ba-4f9a-b997-a0c0b5b4689a","version":2,"type":"bulleted_list","properties":{"title":[["Once you have a model that works on your edge device, you can "],["iterate locally",[["b"]]],[" as long as you add model size and latency to your metrics and avoid regression."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"85d954e8-d2d5-4a8c-8cf3-795d4b9119d1":{"role":"reader","value":{"id":"85d954e8-d2d5-4a8c-8cf3-795d4b9119d1","version":2,"type":"bulleted_list","properties":{"title":[["You should treat "],["tuning the model for your device as an additional risk",[["b"]]],[" in the deployment cycle and test it accordingly. In other words, you should always test your models on production hardware before deploying them for real."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"83166b51-7153-4f7f-b2f6-879591f5d373":{"role":"reader","value":{"id":"83166b51-7153-4f7f-b2f6-879591f5d373","version":2,"type":"bulleted_list","properties":{"title":[["Since models can be finicky, it’s a good idea to build "],["fallback mechanisms",[["b"]]],[" into the application if the model fails or is too slow."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"554ae83d-88e7-44e1-8372-18e24f77385f":{"role":"reader","value":{"id":"554ae83d-88e7-44e1-8372-18e24f77385f","version":2,"type":"sub_sub_header","properties":{"title":[["TAKEAWAYS",[["b"]]]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"52078d25-65f4-41cf-9985-010337a4c912":{"role":"reader","value":{"id":"52078d25-65f4-41cf-9985-010337a4c912","version":2,"type":"bulleted_list","properties":{"title":[["Web deployment is easier, so only perform edge deployment if you need to."]]},"created_time":1646024849950,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6a96fe5c-618e-47d4-8e9b-b6c9526bc3ab":{"role":"reader","value":{"id":"6a96fe5c-618e-47d4-8e9b-b6c9526bc3ab","version":2,"type":"bulleted_list","properties":{"title":[["You should choose your framework to match the available hardware and corresponding mobile frameworks. Else, you can try Apache TVM to be more flexible."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e80b95f6-80be-443e-926a-e8d6444ce711":{"role":"reader","value":{"id":"e80b95f6-80be-443e-926a-e8d6444ce711","version":2,"type":"bulleted_list","properties":{"title":[["You should start considering hardware constraints at the beginning of the project and choose the architectures accordingly."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"2844e39d-9ae4-4ac1-a6e6-97b0d18ef31e":{"role":"reader","value":{"id":"2844e39d-9ae4-4ac1-a6e6-97b0d18ef31e","version":7,"type":"header","properties":{"title":[["II - Model Monitoring"]]},"created_time":1646024849951,"last_edited_time":1646024880000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"87f90e87-b25b-4f05-be09-2e86963e8dff":{"role":"reader","value":{"id":"87f90e87-b25b-4f05-be09-2e86963e8dff","version":2,"type":"text","properties":{"title":[["Once you deploy models, how do you make sure they are staying healthy and working well? Enter model monitoring."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"507cb72a-b463-4992-83f5-1fd36644cb4b":{"role":"reader","value":{"id":"507cb72a-b463-4992-83f5-1fd36644cb4b","version":2,"type":"text","properties":{"title":[["Many things can go wrong with a model once it’s been trained. This can happen even if your model has been trained properly, with a reasonable validation and test loss, as well as robust performance across various slices and quality predictions. Even after you’ve troubleshot and tested a model, things can still go wrong!"]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cc5896d3-16e8-4d5e-8cc4-7461931ce191":{"role":"reader","value":{"id":"cc5896d3-16e8-4d5e-8cc4-7461931ce191","version":8,"type":"sub_header","properties":{"title":[["1 - Why Model Degrades Post-Deployment?",[["b"]]]]},"created_time":1646024849951,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"aa01a3dc-d72e-4a78-84b4-354bf423e2c5":{"role":"reader","value":{"id":"aa01a3dc-d72e-4a78-84b4-354bf423e2c5","version":2,"type":"text","properties":{"title":[["Model performance tends to degrade after you’ve deployed a model. Why does this occur? In supervised learning, we seek to fit a function f to approximate a posterior using the data available to us. If any component of this process changes (i.e., the data x), the deployed model can see an unexpectedly degraded performance. See the below chart for examples of how such post-deployment degradations can occur theoretically and in practice:"]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"9de2a6e2-22c0-4763-bc40-eae9e0035e29":{"role":"reader","value":{"id":"9de2a6e2-22c0-4763-bc40-eae9e0035e29","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image17.png"]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f4640d27-6058-4750-865e-b8f229e92832":{"role":"reader","value":{"id":"f4640d27-6058-4750-865e-b8f229e92832","version":2,"type":"text","properties":{"title":[["In summary, there are three core ways that the model’s performance can degrade: "],["data drift",[["b"]]],[", "],["concept drift",[["b"]]],[", and "],["domain shift",[["b"]]],["."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"70391bff-2b88-43db-86d3-6027549e4c6d":{"role":"reader","value":{"id":"70391bff-2b88-43db-86d3-6027549e4c6d","version":2,"type":"numbered_list","properties":{"title":[["In data drift, the underlying data expectation that your model is built can unexpectedly change, perhaps through a bug in the upstream data pipeline or even due to malicious users feeding the model bad data."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ca96fc80-6433-422f-9a57-f161d284462b":{"role":"reader","value":{"id":"ca96fc80-6433-422f-9a57-f161d284462b","version":2,"type":"numbered_list","properties":{"title":[["In concept drift, the actual outcome you seek to model, or the relationship between the data and the outcome, may fray. For example, users may start to pick movies in a different manner based on the output of your model, thereby changing the fundamental “concept” the model needs to approximate."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5e4d29da-3e54-4dc8-8c3c-e24dae9fa3aa":{"role":"reader","value":{"id":"5e4d29da-3e54-4dc8-8c3c-e24dae9fa3aa","version":2,"type":"numbered_list","properties":{"title":[["Finally, in domain shift, if your dataset does not appropriately sample the production, post-deployment setting, the model’s performance may suffer; this could be considered a “long tail” scenario, where many rare examples that are not present in the development data occur."]]},"created_time":1646024849951,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c24b5514-59d3-4e13-a95c-309534326d7c":{"role":"reader","value":{"id":"c24b5514-59d3-4e13-a95c-309534326d7c","version":8,"type":"sub_header","properties":{"title":[["2 - Data Drift",[["b"]]]]},"created_time":1646024849951,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"621f8b5e-c73b-4a3e-a870-c68f2828f8b4":{"role":"reader","value":{"id":"621f8b5e-c73b-4a3e-a870-c68f2828f8b4","version":2,"type":"text","properties":{"title":[["There are a few different types of data drift:"]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"09841bb5-bcf7-4ebb-b8c0-3e215e14f8f3":{"role":"reader","value":{"id":"09841bb5-bcf7-4ebb-b8c0-3e215e14f8f3","version":2,"type":"bulleted_list","properties":{"title":[["Instantaneous drift",[["b"]]],[": In this situation, the paradigm of the draft dramatically shifts. Examples are deploying the model in a new domain (e.g., self-driving car model in a new city), a bug in the preprocessing pipeline, or even major external shifts like COVID."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"316d4240-9a82-43c3-884e-6f173e7449a4":{"role":"reader","value":{"id":"316d4240-9a82-43c3-884e-6f173e7449a4","version":2,"type":"bulleted_list","properties":{"title":[["Gradual drift",[["b"]]],[": In this situation, the value of data gradually changes with time. For example, users’ preferences may change over time, or new concepts can get introduced to the domain."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"dc297cb1-c4fc-427c-8841-b4221c928e29":{"role":"reader","value":{"id":"dc297cb1-c4fc-427c-8841-b4221c928e29","version":2,"type":"bulleted_list","properties":{"title":[["Periodic drift",[["b"]]],[": Data can have fluctuating value due to underlying patterns like seasonality or time zones."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c040053f-7a4a-427c-a926-e1556e879932":{"role":"reader","value":{"id":"c040053f-7a4a-427c-a926-e1556e879932","version":2,"type":"bulleted_list","properties":{"title":[["Temporary drift",[["b"]]],[": The most difficult to detect, drift can occur through a short-term change in the data that shifts back to normal. This could be via a short-lived malicious attack, or even simply because a user with different demographics or behaviors uses your product in a way that it’s not designed to be used."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"92daa2b9-267b-4556-8885-21437a03444c":{"role":"reader","value":{"id":"92daa2b9-267b-4556-8885-21437a03444c","version":2,"type":"text","properties":{"title":[["While these categories may seem like purely academic categories, the consequences of data shift are very "],["real",[["b"],["_"],["a","https://www.technologyreview.com/2020/05/11/1001563/covid-pandemic-broken-ai-machine-learning-amazon-retail-fraud-humans-in-the-loop/"]]],[". This is a real problem that affects many companies and is only now starting to get the attention it merits."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"4781243d-0d9e-404b-8572-7e83cd1d7d48":{"role":"reader","value":{"id":"4781243d-0d9e-404b-8572-7e83cd1d7d48","version":8,"type":"sub_header","properties":{"title":[["3 - What Should You Monitor?",[["b"]]]]},"created_time":1646024849952,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"93c284f0-9c06-4a25-bca6-85b296c18ef9":{"role":"reader","value":{"id":"93c284f0-9c06-4a25-bca6-85b296c18ef9","version":2,"type":"text","properties":{"title":[["There are four core types of signals to monitor for machine learning models."]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"94857d4a-4b85-476d-ac00-a6ce4cec194f":{"role":"reader","value":{"id":"94857d4a-4b85-476d-ac00-a6ce4cec194f","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image19.png"]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c2da6103-866d-4923-bf3c-ad7e18fd5289":{"role":"reader","value":{"id":"c2da6103-866d-4923-bf3c-ad7e18fd5289","version":2,"type":"text","properties":{"title":[["These metrics trade off with another in terms of how informative they are and how easy they are to access. "],["Put simply, the harder a metric may be to monitor, the more useful it likely is.",[["b"]]]]},"created_time":1646024849952,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"ce62509a-4152-4406-ae22-e2c695a44d6a":{"role":"reader","value":{"id":"ce62509a-4152-4406-ae22-e2c695a44d6a","version":2,"type":"bulleted_list","properties":{"title":[["The hardest and best metrics to monitor are "],["model performance metrics",[["b"]]],[", though these can be difficult to acquire in real-time (labels are hard to come by)."]]},"created_time":1646024849953,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1a414ed0-ed60-49f3-8884-2bfa3b17d05c":{"role":"reader","value":{"id":"1a414ed0-ed60-49f3-8884-2bfa3b17d05c","version":2,"type":"bulleted_list","properties":{"title":[["Business metrics",[["b"]]],[" can be helpful signals of model degradation in monitoring but can easily be confounded by other impactful considerations."]]},"created_time":1646024849953,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cb5ac27d-b9e3-4532-939d-270d93bea0cf":{"role":"reader","value":{"id":"cb5ac27d-b9e3-4532-939d-270d93bea0cf","version":2,"type":"bulleted_list","properties":{"title":[["Model inputs and predictions",[["b"]]],[" are a simple way to identify high-level drift and are very easy to gather. Still, they can be difficult to assess in terms of actual performance impact, leaving it more of an art than science."]]},"created_time":1646024849953,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"92fcd210-3edb-4a9c-bca3-68e1bc5dcb58":{"role":"reader","value":{"id":"92fcd210-3edb-4a9c-bca3-68e1bc5dcb58","version":2,"type":"bulleted_list","properties":{"title":[["Finally, "],["system performance",[["b"]]],[" (e.g., GPU usage) can be a coarse method of catching serious bugs."]]},"created_time":1646024849953,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d10b75f5-cfcd-4fa6-b325-eebc903b9402":{"role":"reader","value":{"id":"d10b75f5-cfcd-4fa6-b325-eebc903b9402","version":2,"type":"text","properties":{"title":[["In considering which metrics to focus on, prioritize ground-truth metrics (model and business metrics), then approximate performance metrics (business and input/outputs), and finally, system health metrics."]]},"created_time":1646024849953,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1f2290ca-b6ca-48d3-a571-9f623b67fa4c":{"role":"reader","value":{"id":"1f2290ca-b6ca-48d3-a571-9f623b67fa4c","version":8,"type":"sub_header","properties":{"title":[["4 - How Do You Measure Distribution Changes?",[["b"]]]]},"created_time":1646024849953,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"cd6ad45e-1211-4334-96c7-dbed8e46c67e":{"role":"reader","value":{"id":"cd6ad45e-1211-4334-96c7-dbed8e46c67e","version":2,"type":"sub_sub_header","properties":{"title":[["SELECT A REFERENCE WINDOW",[["b"]]]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"76095211-5e3b-4583-a29e-d70b6d4dcef9":{"role":"reader","value":{"id":"76095211-5e3b-4583-a29e-d70b6d4dcef9","version":2,"type":"text","properties":{"title":[["To measure distribution changes in metrics you’re monitoring, start by picking a reference set of production data to compare new data to. There are a few different ways of picking this reference data (e.g., "],["sliding window",[["b"],["_"],["a","https://arxiv.org/abs/1908.04240"]]],[" or fixed window of production data), but the most practical thing to do is "],["to use your training or evaluation data as the reference",[["b"]]],[". Data coming in looking different from what you developed your model using is an important signal to act on."]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"f6451f3e-e420-48cf-b2ba-6e4aad23fd58":{"role":"reader","value":{"id":"f6451f3e-e420-48cf-b2ba-6e4aad23fd58","version":2,"type":"sub_sub_header","properties":{"title":[["SELECT A MEASUREMENT WINDOW",[["b"]]]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"db563262-7c38-4baf-928b-e6bfda6cc44d":{"role":"reader","value":{"id":"db563262-7c38-4baf-928b-e6bfda6cc44d","version":2,"type":"text","properties":{"title":[["After picking a reference window, the next step is to choose a measurement window to compare, measure distance, and evaluate for drift. The challenge is that selecting a measurement window is highly problem-dependent. One solution is "],["to pick one or several window sizes and slide them over the data",[["b"]]],[". To avoid recomputing metrics over and over again, when you slide the window, it’s worth looking into "],["the literature on mergeable (quantile) sketching algorithms",[["b"],["_"],["a","https://www.sketchingbigdata.org/"]]],["."]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e8a60f5c-a18e-4f26-8b87-7bf0147fc4cf":{"role":"reader","value":{"id":"e8a60f5c-a18e-4f26-8b87-7bf0147fc4cf","version":2,"type":"sub_sub_header","properties":{"title":[["COMPARE WINDOWS USING A DISTANCE METRIC",[["b"]]]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b2dfec1a-5e63-4a22-841c-f51ca03423df":{"role":"reader","value":{"id":"b2dfec1a-5e63-4a22-841c-f51ca03423df","version":2,"type":"text","properties":{"title":[["What distance metrics should we use to compare the reference window to the measurement window? Some 1-D metric categories are:"]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e9209cc3-54e0-45d8-bfce-80c7eff38c73":{"role":"reader","value":{"id":"e9209cc3-54e0-45d8-bfce-80c7eff38c73","version":2,"type":"numbered_list","properties":{"title":[["Rule-based distance metrics",[["b"]]],[" (e.g., data quality): Summary statistics, the volume of data points, number of missing values, or more complex tests like overall comparisons are common data quality checks that can be applied. "],["Great Expectations",[["b"],["_"],["a","https://greatexpectations.io/"]]],[" is a valuable library for this. "],["Definitely invest in simple rule-based metrics.",[["b"]]],[" They catch a large number of bugs, as publications from Amazon and Google detail."]]},"created_time":1646024849954,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9":{"role":"reader","value":{"id":"5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9","version":2,"type":"numbered_list","properties":{"title":[["Statistical distance metrics",[["b"]]],[" (e.g., KS statistics, KL divergence, D_1 distance, etc.)"]]},"content":["e4bf83e6-5e54-48ed-a502-f571fef87d74","0196c125-dbd6-45f3-97bd-09f5377387a6","42c4ff95-3b73-4819-ad61-e5d12620d751"],"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"189df4e0-3a8a-4992-a98f-902896b69a81":{"role":"reader","value":{"id":"189df4e0-3a8a-4992-a98f-902896b69a81","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image5.png"]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5619e9c6-7647-4bec-8995-b24a148f347d":{"role":"reader","value":{"id":"5619e9c6-7647-4bec-8995-b24a148f347d","version":2,"type":"text","properties":{"title":[["An open area of research is understanding the impact of differing drift patterns on distance metrics and model performance. Another open area of research is high-dimensional distance metrics. Some options here are:"]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"208418e6-51b2-425c-90da-e3bd4b890488":{"role":"reader","value":{"id":"208418e6-51b2-425c-90da-e3bd4b890488","version":2,"type":"numbered_list","properties":{"title":[["Maximum mean discrepancy",[["b"],["_"],["a","http://alex.smola.org/teaching/iconip2006/iconip_3.pdf"]]]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"60188ef1-07ed-4087-9ae6-073630c0d74d":{"role":"reader","value":{"id":"60188ef1-07ed-4087-9ae6-073630c0d74d","version":2,"type":"numbered_list","properties":{"title":[["Performing multiple 1D comparisons across the data: While suffering from "],["the multiple hypothesis testing problem",[["b"],["_"],["a","https://en.wikipedia.org/wiki/Bonferroni_correction"]]],[", this is a practical approach."]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"695c93f0-c4cc-4727-9de6-870d0d445541":{"role":"reader","value":{"id":"695c93f0-c4cc-4727-9de6-870d0d445541","version":2,"type":"numbered_list","properties":{"title":[["Prioritize some features for 1D comparisons: Another option is to avoid testing all the features and only focus on those that merit comparison; for example, those features you know may have shifted in the data."]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"7a9ef50b-bf23-4218-bf71-222e50c7773a":{"role":"reader","value":{"id":"7a9ef50b-bf23-4218-bf71-222e50c7773a","version":2,"type":"numbered_list","properties":{"title":[["Projections",[["b"],["_"],["a","https://arxiv.org/abs/1810.11953"]]],[": In this approach, large data points are put through a dimensionality reduction process and then subject to a two-sample statistical test. Reducing the dimensionality with a domain-specific approach (e.g., mean pixel value for images, length of sentence) is recommended."]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"21f3fec0-2e54-4f23-a321-c9d0fc3cb40e":{"role":"reader","value":{"id":"21f3fec0-2e54-4f23-a321-c9d0fc3cb40e","version":2,"type":"text","properties":{"title":[["At a high level, this entire distance metric work aims to identify not just a score for any data shift but also understand its impact on the model. While choosing a metric can be complicated with all the possible options, you should focus on understanding your model’s robustness in a post-deployment scenario."]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"73856956-6031-4084-b959-9f0f6250bdb9":{"role":"reader","value":{"id":"73856956-6031-4084-b959-9f0f6250bdb9","version":8,"type":"sub_header","properties":{"title":[["5 - How Do You Tell If A Change Is Bad?",[["b"]]]]},"created_time":1646024849956,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e382b926-d7a9-426d-9711-87bcce1b73f0":{"role":"reader","value":{"id":"e382b926-d7a9-426d-9711-87bcce1b73f0","version":2,"type":"text","properties":{"title":[["There’s no hard and fast rule for finding if a change in the data is bad. An easy option is to set thresholds on the test values. Don’t use a statistical test like the KS test, as they are too sensitive to small shifts. Other "],["options",[["b"],["_"],["a","https://blog.anomalo.com/dynamic-data-testing-f831435dba90?gi=6c18774717d2"]]],[" include setting manual ranges, comparing values over time, or even applying an unsupervised model to detect outliers. In practice, fixed rules and specified ranges of test values are used most in practice."]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"104ccb28-b0e1-44a5-b96f-ce9b3934eb0e":{"role":"reader","value":{"id":"104ccb28-b0e1-44a5-b96f-ce9b3934eb0e","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image2.png"]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6935acb9-415a-4d5f-a6d8-ff2db55cdb6e":{"role":"reader","value":{"id":"6935acb9-415a-4d5f-a6d8-ff2db55cdb6e","version":8,"type":"sub_header","properties":{"title":[["6 - Tools For Monitoring",[["b"]]]]},"created_time":1646024849956,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"b8f681f4-5417-4648-8011-856987dbacf2":{"role":"reader","value":{"id":"b8f681f4-5417-4648-8011-856987dbacf2","version":2,"type":"text","properties":{"title":[["There are three categories of tools useful for monitoring:"]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"5b842801-4858-4296-97ce-c08291d1d20c":{"role":"reader","value":{"id":"5b842801-4858-4296-97ce-c08291d1d20c","version":2,"type":"numbered_list","properties":{"title":[["System monitoring tools",[["b"]]],[" like "],["AWS CloudWatch",[["b"],["_"],["a","https://aws.amazon.com/cloudwatch/"]]],[", "],["Datadog",[["b"],["_"],["a","https://www.datadoghq.com/"]]],[", "],["New Relic",[["b"],["_"],["a","https://newrelic.com/"]]],[", and "],["honeycomb",[["b"],["_"],["a","https://www.honeycomb.io/"]]],[" test traditional performance metrics"]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"c74b3c09-e6c2-4538-8339-b81533a56431":{"role":"reader","value":{"id":"c74b3c09-e6c2-4538-8339-b81533a56431","version":2,"type":"numbered_list","properties":{"title":[["Data quality tools",[["b"]]],[" like "],["Great Expectations",[["b"],["_"],["a","https://greatexpectations.io/"]]],[", "],["Anomalo",[["b"],["_"],["a","https://www.anomalo.com/"]]],[", and "],["Monte Carlo",[["b"],["_"],["a","https://www.montecarlodata.com/"]]],[" test if specific windows of data violate rules or assumptions."]]},"created_time":1646024849956,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e881e5c3-9f08-4e0c-823b-1076d5531d1e":{"role":"reader","value":{"id":"e881e5c3-9f08-4e0c-823b-1076d5531d1e","version":2,"type":"numbered_list","properties":{"title":[["ML monitoring tools",[["b"]]],[" like "],["Arize",[["b"],["_"],["a","https://arize.com/"]]],[", "],["Fiddler",[["b"],["_"],["a","https://www.fiddler.ai/"]]],[", and "],["Arthur",[["b"],["_"],["a","https://www.arthur.ai/"]]],[" can also be useful, as they specifically test models."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"33d5ee45-6f1a-494d-8cf3-5e0cf1457fdc":{"role":"reader","value":{"id":"33d5ee45-6f1a-494d-8cf3-5e0cf1457fdc","version":8,"type":"sub_header","properties":{"title":[["7 - Evaluation Store",[["b"]]]]},"created_time":1646024849957,"last_edited_time":1646024940000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"d442d06c-0de1-488e-9dda-63b064b45f76":{"role":"reader","value":{"id":"d442d06c-0de1-488e-9dda-63b064b45f76","version":2,"type":"text","properties":{"title":[["Monitoring is more central to ML than for traditional software",[["b"]]],["."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"1a07382f-0326-452a-a6e3-cee764438677":{"role":"reader","value":{"id":"1a07382f-0326-452a-a6e3-cee764438677","version":2,"type":"bulleted_list","properties":{"title":[["In traditional SWE, most bugs cause loud failures, and the data that is monitored is most valuable to detect and diagnose problems. If the system is working well, the data from these metrics and monitoring systems may not be useful."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"a8e89d16-f19b-4b51-a6fa-84ab17863857":{"role":"reader","value":{"id":"a8e89d16-f19b-4b51-a6fa-84ab17863857","version":2,"type":"bulleted_list","properties":{"title":[["In machine learning, however, monitoring plays a different role. First off, bugs in ML systems often lead to silent degradations in performance. Furthermore, the data that is monitored in ML is literally the code used to train the next iteration of models."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"aac7cc31-78df-4257-b3c2-8b8493876809":{"role":"reader","value":{"id":"aac7cc31-78df-4257-b3c2-8b8493876809","version":2,"type":"text","properties":{"title":[["Because monitoring is so essential to ML systems, tightly integrating it into the ML system architecture brings major benefits. In particular, "],["better integrating and monitoring practices, or creating an evaluation store, can close the data flywheel loop",[["b"]]],[", a concept we talked about earlier in the class."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"42786fa4-e82f-41f5-bc52-d3480f84f5af":{"role":"reader","value":{"id":"42786fa4-e82f-41f5-bc52-d3480f84f5af","version":2,"type":"image","properties":{"source":[["https://fullstackdeeplearning.com/spring2021/lecture-11-notes-media/image8.png"]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e194ed6e-8255-4793-9018-380b4d376582":{"role":"reader","value":{"id":"e194ed6e-8255-4793-9018-380b4d376582","version":2,"type":"text","properties":{"title":[["As we build models, we create a mapping between data and model. As the data changes and we retrain models, monitoring these changes doesn’t become an endpoint--it becomes a part of the entire model development process. Monitoring, via an evaluation store, should touch all parts of your stack. One challenge that this process helps solve is effectively choosing which data points to collect, store, and label. Evaluation stores can help identify which data to collect more points for based on uncertain performance. As more data is collected and labeled, efficient retraining can be performed using evaluation store guidance."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"8cd79d4b-bc11-490f-b335-d2f4ae8cffcf":{"role":"reader","value":{"id":"8cd79d4b-bc11-490f-b335-d2f4ae8cffcf","version":2,"type":"sub_sub_header","properties":{"title":[["Conclusion",[["b"]]]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"6a9dc031-a76d-4c81-ac24-c0aee884b6c4":{"role":"reader","value":{"id":"6a9dc031-a76d-4c81-ac24-c0aee884b6c4","version":2,"type":"text","properties":{"title":[["In summary, make sure to monitor your models!"]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"63fc89c8-e808-49d6-87b0-a6a5c9443c86":{"role":"reader","value":{"id":"63fc89c8-e808-49d6-87b0-a6a5c9443c86","version":2,"type":"bulleted_list","properties":{"title":[["Something will always go wrong, and you should have a system to catch errors."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"77936720-d694-4754-8d03-c2d7e2ed58d4":{"role":"reader","value":{"id":"77936720-d694-4754-8d03-c2d7e2ed58d4","version":2,"type":"bulleted_list","properties":{"title":[["Start by looking at data quality metrics and system metrics, as they are easiest."]]},"created_time":1646024849957,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"eb4917bd-af5a-4cd0-aaec-f3089c46a235":{"role":"reader","value":{"id":"eb4917bd-af5a-4cd0-aaec-f3089c46a235","version":2,"type":"bulleted_list","properties":{"title":[["In a perfect world, the testing and monitoring should be linked, and they should help you close the data flywheel."]]},"created_time":1646024849958,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"77c57bc2-b5cb-4a88-a6ff-6d2be8feaa93":{"role":"reader","value":{"id":"77c57bc2-b5cb-4a88-a6ff-6d2be8feaa93","version":2,"type":"bulleted_list","properties":{"title":[["There will be a lot of tooling and research that will hopefully come soon!"]]},"created_time":1646024849958,"last_edited_time":1646024820000,"parent_id":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"e4bf83e6-5e54-48ed-a502-f571fef87d74":{"role":"reader","value":{"id":"e4bf83e6-5e54-48ed-a502-f571fef87d74","version":2,"type":"numbered_list","properties":{"title":[["KL Divergence",[["b"],["_"],["a","https://machinelearningmastery.com/divergence-between-probability-distributions/"]]],[": Defined as the expectation of a ratio of logs of two different distributions, this commonly known metric is very sensitive to what happens in the tails of the distribution. It’s not well-suited to data shift testing since it’s easily disturbed, is not interpretable, and struggles with data in different ranges."]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"0196c125-dbd6-45f3-97bd-09f5377387a6":{"role":"reader","value":{"id":"0196c125-dbd6-45f3-97bd-09f5377387a6","version":2,"type":"numbered_list","properties":{"title":[["KS Statistic",[["b"],["_"],["a","https://www.statisticshowto.com/kolmogorov-smirnov-test/"]]],[": This metric is defined as the max distance between CDFs, which is easy to interpret and is thus used widely in practice. Say yes to the KS statistic!"]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}},"42c4ff95-3b73-4819-ad61-e5d12620d751":{"role":"reader","value":{"id":"42c4ff95-3b73-4819-ad61-e5d12620d751","version":2,"type":"numbered_list","properties":{"title":[["D1 Distance",[["b"],["_"],["a","https://mlsys.org/Conferences/2019/doc/2019/167.pdf"]]],[": Defined as the sum of distances between PDFs, this is a metric used at Google. Despite seeming less principled, it’s easily interpretable and has the added benefit of knowing Google uses it (so why not you?)."]]},"created_time":1646024849955,"last_edited_time":1646024820000,"parent_id":"5f14a819-95ae-4bd2-9ce0-5b1ba12e78c9","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","last_edited_by_table":"notion_user","last_edited_by_id":"9e9c4442-4350-473a-b900-c954b0bd7a95","space_id":"bb9288f4-efb5-417f-bf44-1a79fa04feb1"}}},"space":{},"discussion":{},"comment":{},"collection":{},"collection_view":{},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{}},"pageId":"d8ff64bd-6218-4dcb-9b3c-7daed5c4aa3a"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"deployment-monitoring"},"buildId":"YmEd5Nabys24EQBvAhdkU","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>